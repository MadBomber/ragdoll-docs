
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../installation/">
      
      
        <link rel="next" href="../metadata-schemas/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.16">
    
    
      
        <title>LLM Integration - Ragdoll Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#llm-integration" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Ragdoll Documentation" class="md-header__button md-logo" aria-label="Ragdoll Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Ragdoll Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LLM Integration
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Ragdoll Documentation" class="md-nav__button md-logo" aria-label="Ragdoll Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Ragdoll Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Ragdoll Documentation
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../api-client/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Client API Reference
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../api-jobs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Jobs Reference
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../api-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Models Reference
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../api-services/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Services Reference
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Architecture Overview
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../background-processing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Background Processing
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../configuration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Configuration Guide
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../contributing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Contributing
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../database-schema/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Database Schema
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../deployment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Production Deployment Guide
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../development/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Development Setup
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../document-processing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Document Processing
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../embedding-system/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embedding System
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../environment-variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Environment Variables Reference
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../extending/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Extending the System
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../file-uploads/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    File Upload System
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation &amp; Setup
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    LLM Integration
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    LLM Integration
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#multiple-provider-support-and-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Multiple Provider Support and Configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multiple Provider Support and Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#architecture-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture Overview
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration-structure" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration Structure
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#supported-providers" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Providers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Supported Providers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openai" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic" class="md-nav__link">
    <span class="md-ellipsis">
      Anthropic
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#google-vertex-ai" class="md-nav__link">
    <span class="md-ellipsis">
      Google Vertex AI
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#azure-openai" class="md-nav__link">
    <span class="md-ellipsis">
      Azure OpenAI
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ollama-local-models" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama (Local Models)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huggingface" class="md-nav__link">
    <span class="md-ellipsis">
      HuggingFace
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openrouter" class="md-nav__link">
    <span class="md-ellipsis">
      OpenRouter
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#configuration-patterns" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration Patterns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Configuration Patterns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#provider-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Provider Selection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#api-key-management" class="md-nav__link">
    <span class="md-ellipsis">
      API Key Management
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-selection-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      Model Selection Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model Selection Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#task-specific-models" class="md-nav__link">
    <span class="md-ellipsis">
      Task-Specific Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-vs-cost" class="md-nav__link">
    <span class="md-ellipsis">
      Performance vs Cost
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#error-handling" class="md-nav__link">
    <span class="md-ellipsis">
      Error Handling
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Error Handling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#provider-failures" class="md-nav__link">
    <span class="md-ellipsis">
      Provider Failures
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rate-limiting" class="md-nav__link">
    <span class="md-ellipsis">
      Rate Limiting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Best Practices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#provider-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Provider-Specific Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-fine-tuning-approaches" class="md-nav__link">
    <span class="md-ellipsis">
      Model Fine-Tuning Approaches
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prompt-engineering" class="md-nav__link">
    <span class="md-ellipsis">
      Prompt Engineering
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-assessment" class="md-nav__link">
    <span class="md-ellipsis">
      Quality Assessment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#production-deployment-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      Production Deployment Checklist
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../metadata-schemas/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Metadata Schemas
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../monitoring/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Monitoring &amp; Analytics
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../multi-modal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multi-Modal Architecture
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../performance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Performance Tuning
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../quick-start/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quick Start Guide
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../search-analytics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Search &amp; Analytics
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../security/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Security Considerations
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../testing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Testing Guide
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../troubleshooting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Troubleshooting
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#multiple-provider-support-and-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Multiple Provider Support and Configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multiple Provider Support and Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#architecture-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture Overview
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration-structure" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration Structure
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#supported-providers" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Providers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Supported Providers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openai" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic" class="md-nav__link">
    <span class="md-ellipsis">
      Anthropic
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#google-vertex-ai" class="md-nav__link">
    <span class="md-ellipsis">
      Google Vertex AI
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#azure-openai" class="md-nav__link">
    <span class="md-ellipsis">
      Azure OpenAI
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ollama-local-models" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama (Local Models)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huggingface" class="md-nav__link">
    <span class="md-ellipsis">
      HuggingFace
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openrouter" class="md-nav__link">
    <span class="md-ellipsis">
      OpenRouter
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#configuration-patterns" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration Patterns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Configuration Patterns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#provider-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Provider Selection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#api-key-management" class="md-nav__link">
    <span class="md-ellipsis">
      API Key Management
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-selection-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      Model Selection Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model Selection Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#task-specific-models" class="md-nav__link">
    <span class="md-ellipsis">
      Task-Specific Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-vs-cost" class="md-nav__link">
    <span class="md-ellipsis">
      Performance vs Cost
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#error-handling" class="md-nav__link">
    <span class="md-ellipsis">
      Error Handling
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Error Handling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#provider-failures" class="md-nav__link">
    <span class="md-ellipsis">
      Provider Failures
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rate-limiting" class="md-nav__link">
    <span class="md-ellipsis">
      Rate Limiting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Best Practices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#provider-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Provider-Specific Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-fine-tuning-approaches" class="md-nav__link">
    <span class="md-ellipsis">
      Model Fine-Tuning Approaches
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prompt-engineering" class="md-nav__link">
    <span class="md-ellipsis">
      Prompt Engineering
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-assessment" class="md-nav__link">
    <span class="md-ellipsis">
      Quality Assessment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#production-deployment-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      Production Deployment Checklist
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="llm-integration">LLM Integration</h1>
<p>Ragdoll provides comprehensive LLM integration through the RubyLLM library, supporting multiple providers with flexible configuration and robust error handling. The system is designed for production use with automatic fallbacks and cost optimization strategies.</p>
<h2 id="multiple-provider-support-and-configuration">Multiple Provider Support and Configuration</h2>
<p>Ragdoll's LLM integration is built on RubyLLM, providing a unified interface to multiple LLM providers. The configuration system supports environment-based setup, automatic provider detection, and sophisticated fallback strategies.</p>
<h3 id="architecture-overview">Architecture Overview</h3>
<pre><code class="language-mermaid">graph TB
    A[Ragdoll Configuration] --&gt; B[RubyLLM Integration]
    B --&gt; C[Provider Selection]
    C --&gt; D[OpenAI]
    C --&gt; E[Anthropic]
    C --&gt; F[Google Vertex AI]
    C --&gt; G[Azure OpenAI]
    C --&gt; H[Ollama]
    C --&gt; I[HuggingFace]
    C --&gt; J[OpenRouter]

    B --&gt; K[EmbeddingService]
    B --&gt; L[TextGenerationService]

    K --&gt; M[Vector Generation]
    L --&gt; N[Summary Generation]
    L --&gt; O[Keyword Extraction]

    P[Error Handling] --&gt; Q[Provider Fallback]
    P --&gt; R[Retry Logic]
    P --&gt; S[Circuit Breakers]
</code></pre>
<h3 id="configuration-structure">Configuration Structure</h3>
<p>The LLM configuration is centralized in the <code>ruby_llm_config</code> section:</p>
<pre><code class="language-ruby">Ragdoll::Core.configure do |config|
  config.ruby_llm_config = {
    openai: {
      api_key: -&gt; { ENV[&quot;OPENAI_API_KEY&quot;] },
      organization: -&gt; { ENV[&quot;OPENAI_ORGANIZATION&quot;] },
      project: -&gt; { ENV[&quot;OPENAI_PROJECT&quot;] }
    },
    anthropic: {
      api_key: -&gt; { ENV[&quot;ANTHROPIC_API_KEY&quot;] }
    },
    google: {
      api_key: -&gt; { ENV[&quot;GOOGLE_API_KEY&quot;] },
      project_id: -&gt; { ENV[&quot;GOOGLE_PROJECT_ID&quot;] }
    },
    azure: {
      api_key: -&gt; { ENV[&quot;AZURE_OPENAI_API_KEY&quot;] },
      endpoint: -&gt; { ENV[&quot;AZURE_OPENAI_ENDPOINT&quot;] },
      api_version: -&gt; { ENV[&quot;AZURE_OPENAI_API_VERSION&quot;] || &quot;2024-02-01&quot; }
    },
    ollama: {
      endpoint: -&gt; { ENV[&quot;OLLAMA_ENDPOINT&quot;] || &quot;http://localhost:11434/v1&quot; }
    },
    huggingface: {
      api_key: -&gt; { ENV[&quot;HUGGINGFACE_API_KEY&quot;] }
    },
    openrouter: {
      api_key: -&gt; { ENV[&quot;OPENROUTER_API_KEY&quot;] }
    }
  }
end
</code></pre>
<h2 id="supported-providers">Supported Providers</h2>
<p>Ragdoll supports seven major LLM providers through RubyLLM integration. Each provider is configured through environment variables with automatic validation and fallback handling.</p>
<h3 id="openai">OpenAI</h3>
<p><strong>Models Supported:</strong>
- GPT-4 series (gpt-4o, gpt-4-turbo, gpt-4)
- GPT-3.5-turbo series
- Text embedding models (text-embedding-3-small, text-embedding-3-large)
- Image understanding via GPT-4 Vision</p>
<p><strong>Configuration:</strong></p>
<pre><code class="language-ruby">config.ruby_llm_config[:openai] = {
  api_key: -&gt; { ENV[&quot;OPENAI_API_KEY&quot;] },
  organization: -&gt; { ENV[&quot;OPENAI_ORGANIZATION&quot;] },  # Optional
  project: -&gt; { ENV[&quot;OPENAI_PROJECT&quot;] }             # Optional
}

# Model selection
config.models = {
  default: &quot;openai/gpt-4o&quot;,
  summary: &quot;openai/gpt-4o&quot;,
  keywords: &quot;openai/gpt-4o&quot;,
  embedding: {
    text: &quot;text-embedding-3-small&quot;
  }
}
</code></pre>
<p><strong>Rate Limiting &amp; Optimization:</strong>
- Automatic retry with exponential backoff
- Request batching for embeddings
- Token usage optimization
- Cost monitoring through usage tracking</p>
<h3 id="anthropic">Anthropic</h3>
<p><strong>Models Supported:</strong>
- Claude 3 series (claude-3-opus, claude-3-sonnet, claude-3-haiku)
- Claude 2 series for cost optimization
- Long context capabilities (up to 200K tokens)</p>
<p><strong>Configuration:</strong></p>
<pre><code class="language-ruby">config.ruby_llm_config[:anthropic] = {
  api_key: -&gt; { ENV[&quot;ANTHROPIC_API_KEY&quot;] }
}

# Using Anthropic models
config.models[:default] = &quot;anthropic/claude-3-sonnet&quot;
config.models[:summary] = &quot;anthropic/claude-3-haiku&quot;  # Cost optimization
</code></pre>
<p><strong>Best Practices:</strong>
- Use Claude 3 Haiku for simple tasks (cost-effective)
- Use Claude 3 Sonnet for balanced performance
- Use Claude 3 Opus for complex reasoning tasks
- Leverage long context for document analysis</p>
<h3 id="google-vertex-ai">Google Vertex AI</h3>
<p><strong>Models Supported:</strong>
- Gemini Pro and Gemini Pro Vision
- PaLM 2 models
- Embedding models (textembedding-gecko)
- Multi-modal capabilities</p>
<p><strong>Configuration:</strong></p>
<pre><code class="language-ruby">config.ruby_llm_config[:google] = {
  api_key: -&gt; { ENV[&quot;GOOGLE_API_KEY&quot;] },
  project_id: -&gt; { ENV[&quot;GOOGLE_PROJECT_ID&quot;] }
}

# Regional configuration
ENV[&quot;GOOGLE_VERTEX_REGION&quot;] = &quot;us-central1&quot;
</code></pre>
<p><strong>Service Account Setup:</strong>
1. Create service account in Google Cloud Console
2. Download JSON key file
3. Set <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable
4. Enable Vertex AI API for your project</p>
<h3 id="azure-openai">Azure OpenAI</h3>
<p><strong>Enterprise Features:</strong>
- Private endpoints and VNet integration
- Managed identity authentication
- Content filtering and safety
- Compliance certifications (SOC 2, HIPAA)</p>
<p><strong>Configuration:</strong></p>
<pre><code class="language-ruby">config.ruby_llm_config[:azure] = {
  api_key: -&gt; { ENV[&quot;AZURE_OPENAI_API_KEY&quot;] },
  endpoint: -&gt; { ENV[&quot;AZURE_OPENAI_ENDPOINT&quot;] },
  api_version: -&gt; { ENV[&quot;AZURE_OPENAI_API_VERSION&quot;] || &quot;2024-02-01&quot; }
}
</code></pre>
<p><strong>Model Deployment:</strong></p>
<pre><code class="language-bash"># Example environment variables
export AZURE_OPENAI_ENDPOINT=&quot;https://your-resource.openai.azure.com/&quot;
export AZURE_OPENAI_API_KEY=&quot;your-api-key&quot;
export AZURE_OPENAI_API_VERSION=&quot;2024-02-01&quot;
</code></pre>
<h3 id="ollama-local-models">Ollama (Local Models)</h3>
<p><strong>Supported Models:</strong>
- Llama 2 and Code Llama
- Mistral and Mixtral models
- Phi-3 and other local models
- Custom fine-tuned models</p>
<p><strong>Configuration:</strong></p>
<pre><code class="language-ruby">config.ruby_llm_config[:ollama] = {
  endpoint: -&gt; { ENV[&quot;OLLAMA_ENDPOINT&quot;] || &quot;http://localhost:11434/v1&quot; }
}

# No API key required for local deployment
config.embedding_config[:provider] = :ollama
</code></pre>
<p><strong>Performance Optimization:</strong>
- GPU acceleration with CUDA/Metal
- Memory management for large models
- Model quantization for efficiency
- Concurrent request handling</p>
<p><strong>Resource Requirements:</strong>
- Minimum 8GB RAM for 7B models
- 16GB+ RAM for 13B models
- GPU recommended for production use
- SSD storage for model files</p>
<h3 id="huggingface">HuggingFace</h3>
<p><strong>Model Hub Integration:</strong>
- 100,000+ models available
- Custom model deployment
- Inference API integration
- Transformers library compatibility</p>
<p><strong>Configuration:</strong></p>
<pre><code class="language-ruby">config.ruby_llm_config[:huggingface] = {
  api_key: -&gt; { ENV[&quot;HUGGINGFACE_API_KEY&quot;] }
}

# Model selection
config.models[:embedding][:text] = &quot;sentence-transformers/all-MiniLM-L6-v2&quot;
</code></pre>
<p><strong>Performance Tuning:</strong>
- Model caching for faster inference
- Batch processing for embeddings
- Custom inference endpoints
- Auto-scaling with serverless</p>
<h3 id="openrouter">OpenRouter</h3>
<p><strong>Provider Routing:</strong>
- Access to 20+ LLM providers
- Automatic provider selection
- Cost optimization routing
- Real-time pricing updates</p>
<p><strong>Configuration:</strong></p>
<pre><code class="language-ruby">config.ruby_llm_config[:openrouter] = {
  api_key: -&gt; { ENV[&quot;OPENROUTER_API_KEY&quot;] }
}

# Cost-optimized model selection
config.models[:default] = &quot;openrouter/anthropic/claude-3-haiku&quot;
config.models[:summary] = &quot;openrouter/meta-llama/llama-2-7b-chat&quot;
</code></pre>
<p><strong>Fallback Strategies:</strong>
- Primary provider → Secondary provider → Local fallback
- Cost-based provider selection
- Geographic routing for compliance
- Real-time availability checking</p>
<h2 id="configuration-patterns">Configuration Patterns</h2>
<p>Ragdoll provides flexible configuration patterns that support everything from simple single-provider setups to complex multi-provider environments with cost optimization and failover.</p>
<h3 id="provider-selection">Provider Selection</h3>
<p><strong>Model-Specific Provider Configuration:</strong></p>
<pre><code class="language-ruby">Ragdoll::Core.configure do |config|
  # Different providers for different tasks
  config.models = {
    default: &quot;openai/gpt-4o&quot;,           # OpenAI for general tasks
    summary: &quot;anthropic/claude-3-haiku&quot;, # Anthropic for summaries
    keywords: &quot;openai/gpt-3.5-turbo&quot;,   # OpenAI for keywords
    embedding: {
      text: &quot;text-embedding-3-small&quot;,   # OpenAI embeddings
      image: &quot;openai/clip-vit-large&quot;,   # OpenAI image embeddings
      audio: &quot;openai/whisper-1&quot;         # OpenAI audio processing
    }
  }
end
</code></pre>
<p><strong>Automatic Provider Detection:</strong>
The system uses <code>parse_provider_model</code> to automatically detect providers:</p>
<pre><code class="language-ruby"># Format: &quot;provider/model&quot; -&gt; automatic provider detection
config.models[:default] = &quot;openai/gpt-4o&quot;
config.models[:summary] = &quot;anthropic/claude-3-sonnet&quot;

# Format: &quot;model&quot; -&gt; RubyLLM determines provider
config.models[:embedding][:text] = &quot;text-embedding-3-small&quot;

# Configuration parsing
parsed = config.parse_provider_model(&quot;openai/gpt-4o&quot;)
# =&gt; { provider: :openai, model: &quot;gpt-4o&quot; }

parsed = config.parse_provider_model(&quot;gpt-4o&quot;)
# =&gt; { provider: nil, model: &quot;gpt-4o&quot; } # RubyLLM auto-detects
</code></pre>
<p><strong>Fallback Provider Chains:</strong></p>
<pre><code class="language-ruby">class CustomTextGenerationService &lt; Ragdoll::Core::TextGenerationService
  private

  def generate_with_fallback(prompt, models)
    models.each do |model_string|
      begin
        parsed = @configuration.parse_provider_model(model_string)
        return generate_with_model(prompt, parsed[:model])
      rescue StandardError =&gt; e
        Rails.logger.warn &quot;Provider #{parsed[:provider]} failed: #{e.message}&quot;
        next
      end
    end

    # Ultimate fallback to basic processing
    generate_basic_summary(prompt, 300)
  end
end

# Usage with fallback chain
fallback_models = [
  &quot;openai/gpt-4o&quot;,           # Primary
  &quot;anthropic/claude-3-sonnet&quot;, # Secondary
  &quot;ollama/llama2&quot;            # Local fallback
]
</code></pre>
<p><strong>Cost-Based Selection:</strong></p>
<pre><code class="language-ruby"># Cost optimization configuration
config.cost_optimization = {
  enable: true,
  thresholds: {
    summary: { max_cost_per_request: 0.01 },
    keywords: { max_cost_per_request: 0.005 },
    embedding: { max_cost_per_1k_tokens: 0.0001 }
  },
  fallback_models: {
    summary: [&quot;anthropic/claude-3-haiku&quot;, &quot;ollama/llama2&quot;],
    keywords: [&quot;openai/gpt-3.5-turbo&quot;, &quot;ollama/mistral&quot;]
  }
}
</code></pre>
<h3 id="api-key-management">API Key Management</h3>
<p><strong>Environment Variable Setup:</strong></p>
<pre><code class="language-bash"># OpenAI
export OPENAI_API_KEY=&quot;sk-...&quot;
export OPENAI_ORGANIZATION=&quot;org-...&quot;
export OPENAI_PROJECT=&quot;proj_...&quot;

# Anthropic
export ANTHROPIC_API_KEY=&quot;sk-ant-...&quot;

# Google
export GOOGLE_API_KEY=&quot;AIza...&quot;
export GOOGLE_PROJECT_ID=&quot;my-project-id&quot;
export GOOGLE_APPLICATION_CREDENTIALS=&quot;/path/to/service-account.json&quot;

# Azure
export AZURE_OPENAI_API_KEY=&quot;...&quot;
export AZURE_OPENAI_ENDPOINT=&quot;https://my-resource.openai.azure.com/&quot;
export AZURE_OPENAI_API_VERSION=&quot;2024-02-01&quot;

# HuggingFace
export HUGGINGFACE_API_KEY=&quot;hf_...&quot;

# OpenRouter
export OPENROUTER_API_KEY=&quot;sk-or-...&quot;
</code></pre>
<p><strong>Secure Key Storage:</strong></p>
<pre><code class="language-ruby"># Using Rails credentials
config.ruby_llm_config[:openai] = {
  api_key: -&gt; { Rails.application.credentials.openai[:api_key] },
  organization: -&gt; { Rails.application.credentials.openai[:organization] }
}

# Using Vault or similar secret management
config.ruby_llm_config[:openai] = {
  api_key: -&gt; { VaultClient.get_secret(&quot;openai/api_key&quot;) }
}

# Using AWS Secrets Manager
config.ruby_llm_config[:openai] = {
  api_key: -&gt; {
    AWS::SecretsManager::Client.new.get_secret_value(
      secret_id: &quot;prod/ragdoll/openai_api_key&quot;
    ).secret_string
  }
}
</code></pre>
<p><strong>Key Rotation Strategies:</strong></p>
<pre><code class="language-ruby">class APIKeyRotationService
  def self.rotate_keys
    # Implement key rotation logic
    providers = [:openai, :anthropic, :google]

    providers.each do |provider|
      current_key = get_current_key(provider)
      new_key = generate_new_key(provider)

      # Test new key
      if test_api_key(provider, new_key)
        update_key_in_secret_store(provider, new_key)
        schedule_old_key_revocation(provider, current_key)
      end
    end
  end

  private

  def self.test_api_key(provider, key)
    # Test API key with minimal request
    test_config = { provider =&gt; { api_key: key } }
    service = Ragdoll::Core::EmbeddingService.new
    service.generate_embedding(&quot;test&quot;)
    true
  rescue
    false
  end
end
</code></pre>
<p><strong>Multi-Tenant Key Management:</strong></p>
<pre><code class="language-ruby">class MultiTenantConfiguration
  def self.for_tenant(tenant_id)
    Ragdoll::Core.configure do |config|
      tenant_keys = fetch_tenant_keys(tenant_id)

      config.ruby_llm_config = {
        openai: {
          api_key: -&gt; { tenant_keys[:openai_api_key] },
          organization: -&gt; { tenant_keys[:openai_organization] }
        },
        anthropic: {
          api_key: -&gt; { tenant_keys[:anthropic_api_key] }
        }
      }

      # Tenant-specific model preferences
      config.models = tenant_model_preferences(tenant_id)
    end
  end

  private

  def self.fetch_tenant_keys(tenant_id)
    # Fetch from secure tenant key store
    TenantKeyStore.get_keys(tenant_id)
  end
end
</code></pre>
<h2 id="model-selection-strategies">Model Selection Strategies</h2>
<p>Ragdoll implements intelligent model selection based on task requirements, performance characteristics, and cost considerations. The system supports both automatic and manual model selection strategies.</p>
<h3 id="task-specific-models">Task-Specific Models</h3>
<p><strong>Embedding Model Selection:</strong></p>
<pre><code class="language-ruby">config.models[:embedding] = {
  text: &quot;text-embedding-3-small&quot;,      # Fast, cost-effective for most text
  image: &quot;clip-vit-large-patch14&quot;,     # Best for image understanding
  audio: &quot;whisper-embedding-v1&quot;,      # Audio content embeddings
  code: &quot;text-embedding-3-large&quot;,     # Higher accuracy for code
  multilingual: &quot;multilingual-e5-large&quot; # Multi-language support
}

# Dynamic embedding model selection
class SmartEmbeddingService &lt; Ragdoll::Core::EmbeddingService
  def generate_embedding(text, content_type: :text)
    model = select_embedding_model(text, content_type)
    super(text, model: model)
  end

  private

  def select_embedding_model(text, content_type)
    case content_type
    when :code
      detect_programming_language(text) ? &quot;text-embedding-3-large&quot; : &quot;text-embedding-3-small&quot;
    when :multilingual
      detect_language(text) != &quot;en&quot; ? &quot;multilingual-e5-large&quot; : &quot;text-embedding-3-small&quot;
    when :long_document
      text.length &gt; 5000 ? &quot;text-embedding-3-large&quot; : &quot;text-embedding-3-small&quot;
    else
      Ragdoll.config.models[:embedding][content_type]
    end
  end
end
</code></pre>
<p><strong>Summary Generation Models:</strong></p>
<pre><code class="language-ruby"># Task-specific summary model configuration
config.summarization_config = {
  enable: true,
  models: {
    short_content: &quot;openai/gpt-3.5-turbo&quot;,    # &lt; 1000 tokens
    medium_content: &quot;anthropic/claude-3-haiku&quot;, # 1000-5000 tokens
    long_content: &quot;anthropic/claude-3-sonnet&quot;,  # &gt; 5000 tokens
    technical: &quot;openai/gpt-4o&quot;,                # Technical documents
    creative: &quot;anthropic/claude-3-opus&quot;        # Creative content
  },
  max_length: 300,
  quality_threshold: 0.8
}

# Intelligent model selection in TextGenerationService
def select_summary_model(content)
  token_count = estimate_token_count(content)
  content_type = detect_content_type(content)

  case
  when token_count &lt; 1000
    @configuration.summarization_config[:models][:short_content]
  when technical_content?(content)
    @configuration.summarization_config[:models][:technical]
  when creative_content?(content)
    @configuration.summarization_config[:models][:creative]
  else
    @configuration.summarization_config[:models][:medium_content]
  end
end
</code></pre>
<p><strong>Keyword Extraction Models:</strong></p>
<pre><code class="language-ruby"># Specialized keyword extraction configuration
config.keyword_extraction = {
  models: {
    general: &quot;openai/gpt-3.5-turbo&quot;,
    technical: &quot;openai/gpt-4o&quot;,
    academic: &quot;anthropic/claude-3-sonnet&quot;,
    multilingual: &quot;google/gemini-pro&quot;
  },
  max_keywords: 20,
  confidence_threshold: 0.7
}

# Context-aware keyword extraction
class AdvancedKeywordExtraction &lt; Ragdoll::Core::TextGenerationService
  def extract_keywords(text, context: :general)
    model = select_keyword_model(text, context)

    prompt = build_contextual_keyword_prompt(text, context)

    extract_with_model(text, model, prompt)
  end

  private

  def select_keyword_model(text, context)
    case context
    when :technical
      detect_technical_terms(text) ? &quot;openai/gpt-4o&quot; : &quot;openai/gpt-3.5-turbo&quot;
    when :academic
      &quot;anthropic/claude-3-sonnet&quot;
    when :multilingual
      &quot;google/gemini-pro&quot;
    else
      &quot;openai/gpt-3.5-turbo&quot;
    end
  end
end
</code></pre>
<p><strong>Classification Models:</strong></p>
<pre><code class="language-ruby"># Document classification configuration
config.classification = {
  models: {
    content_type: &quot;openai/gpt-3.5-turbo&quot;,
    sentiment: &quot;anthropic/claude-3-haiku&quot;,
    topic: &quot;openai/gpt-4o&quot;,
    language: &quot;google/gemini-pro&quot;
  },
  categories: {
    content_type: [&quot;technical&quot;, &quot;business&quot;, &quot;academic&quot;, &quot;creative&quot;],
    sentiment: [&quot;positive&quot;, &quot;negative&quot;, &quot;neutral&quot;],
    topic: [&quot;technology&quot;, &quot;finance&quot;, &quot;healthcare&quot;, &quot;education&quot;]
  }
}
</code></pre>
<h3 id="performance-vs-cost">Performance vs Cost</h3>
<p><strong>Model Performance Comparison:</strong></p>
<pre><code class="language-ruby"># Performance benchmarking data
MODEL_PERFORMANCE = {
  &quot;openai/gpt-4o&quot; =&gt; {
    accuracy: 0.95,
    speed: &quot;medium&quot;,
    cost_per_1k_tokens: 0.03,
    best_for: [&quot;complex_reasoning&quot;, &quot;technical_analysis&quot;]
  },
  &quot;anthropic/claude-3-sonnet&quot; =&gt; {
    accuracy: 0.92,
    speed: &quot;fast&quot;,
    cost_per_1k_tokens: 0.015,
    best_for: [&quot;document_analysis&quot;, &quot;summarization&quot;]
  },
  &quot;openai/gpt-3.5-turbo&quot; =&gt; {
    accuracy: 0.87,
    speed: &quot;very_fast&quot;,
    cost_per_1k_tokens: 0.002,
    best_for: [&quot;simple_tasks&quot;, &quot;bulk_processing&quot;]
  }
}

class PerformanceCostOptimizer
  def select_optimal_model(task_type, content_length, quality_requirement)
    candidates = MODEL_PERFORMANCE.select do |model, stats|
      stats[:best_for].include?(task_type.to_s) &amp;&amp;
      stats[:accuracy] &gt;= quality_requirement
    end

    # Sort by cost-effectiveness (accuracy/cost ratio)
    candidates.min_by do |model, stats|
      estimated_cost = calculate_cost(content_length, stats[:cost_per_1k_tokens])
      estimated_cost / stats[:accuracy]
    end.first
  end
end
</code></pre>
<p><strong>Cost Optimization Strategies:</strong></p>
<pre><code class="language-ruby"># Adaptive cost optimization
config.cost_optimization = {
  enable: true,
  budget_limits: {
    daily: 100.00,    # Daily budget limit
    monthly: 2000.00  # Monthly budget limit
  },
  strategies: {
    batch_processing: true,     # Batch similar requests
    caching: true,             # Cache similar requests
    model_downgrade: true,     # Use cheaper models when possible
    queue_management: true     # Queue non-urgent requests
  }
}

class CostOptimizationService
  def optimize_request(request_type, content, urgency: :normal)
    current_spend = calculate_daily_spend
    remaining_budget = config.cost_optimization[:budget_limits][:daily] - current_spend

    if remaining_budget &lt; 10.00 &amp;&amp; urgency != :high
      # Use cheaper model or queue request
      return queue_request(request_type, content)
    end

    # Check for cached similar requests
    if cached_result = check_cache(request_type, content)
      return cached_result
    end

    # Select cost-effective model
    model = select_cost_effective_model(request_type, remaining_budget)
    process_request(request_type, content, model)
  end
end
</code></pre>
<p><strong>Quality Thresholds:</strong></p>
<pre><code class="language-ruby"># Quality control configuration
config.quality_control = {
  minimum_thresholds: {
    embedding_similarity: 0.7,
    summary_coherence: 0.8,
    keyword_relevance: 0.75
  },
  validation_methods: {
    semantic_coherence: true,
    factual_accuracy: true,
    language_quality: true
  },
  retry_on_low_quality: true,
  max_retries: 2
}

class QualityValidator
  def validate_summary(summary, original_content)
    scores = {
      coherence: calculate_coherence_score(summary),
      relevance: calculate_relevance_score(summary, original_content),
      completeness: calculate_completeness_score(summary, original_content)
    }

    overall_score = scores.values.sum / scores.length

    if overall_score &lt; config.quality_control[:minimum_thresholds][:summary_coherence]
      raise QualityThresholdError, &quot;Summary quality below threshold: #{overall_score}&quot;
    end

    { valid: true, scores: scores, overall_score: overall_score }
  end
end
</code></pre>
<p><strong>Batch Processing Optimization:</strong></p>
<pre><code class="language-ruby"># Efficient batch processing
class BatchOptimizedEmbeddingService &lt; Ragdoll::Core::EmbeddingService
  MAX_BATCH_SIZE = 100
  OPTIMAL_BATCH_SIZE = 50

  def generate_embeddings_optimized(texts)
    # Group texts by optimal batch size
    batches = texts.each_slice(OPTIMAL_BATCH_SIZE).to_a

    results = []
    batches.each_with_index do |batch, index|
      # Add delay between batches to respect rate limits
      sleep(0.1) if index &gt; 0

      batch_results = generate_embeddings_batch(batch)
      results.concat(batch_results)

      # Progress tracking
      progress = ((index + 1) * 100.0 / batches.length).round(1)
      puts &quot;Batch processing: #{progress}% complete&quot;
    end

    results
  end
end
</code></pre>
<h2 id="error-handling">Error Handling</h2>
<p>Ragdoll implements comprehensive error handling with automatic retries, intelligent fallbacks, and circuit breaker patterns to ensure reliable LLM integration in production environments.</p>
<h3 id="provider-failures">Provider Failures</h3>
<p><strong>Automatic Retry Strategies:</strong></p>
<pre><code class="language-ruby">class RobustLLMService
  MAX_RETRIES = 3
  RETRY_DELAYS = [1, 2, 4] # Exponential backoff in seconds

  def generate_with_retry(content, model, max_retries: MAX_RETRIES)
    attempt = 0

    begin
      attempt += 1
      result = generate_content(content, model)

      # Reset success counter on successful request
      reset_failure_count(model)
      return result

    rescue RateLimitError =&gt; e
      if attempt &lt;= max_retries
        delay = calculate_rate_limit_delay(e)
        Rails.logger.warn &quot;Rate limited, retrying in #{delay}s (attempt #{attempt})&quot;
        sleep(delay)
        retry
      else
        handle_rate_limit_failure(model, e)
      end

    rescue APIError =&gt; e
      if retryable_error?(e) &amp;&amp; attempt &lt;= max_retries
        delay = RETRY_DELAYS[attempt - 1] || 4
        Rails.logger.warn &quot;API error, retrying in #{delay}s (attempt #{attempt}): #{e.message}&quot;
        sleep(delay)
        retry
      else
        handle_api_failure(model, e)
      end

    rescue StandardError =&gt; e
      Rails.logger.error &quot;Unexpected error with #{model}: #{e.message}&quot;
      increment_failure_count(model)
      raise e
    end
  end

  private

  def retryable_error?(error)
    case error
    when NetworkError, TimeoutError, TemporaryServerError
      true
    when AuthenticationError, InvalidModelError
      false
    else
      error.message.include?(&quot;temporary&quot;) || error.message.include?(&quot;retry&quot;)
    end
  end
end
</code></pre>
<p><strong>Provider Fallback:</strong></p>
<pre><code class="language-ruby">class ProviderFallbackService
  def initialize
    @provider_health = Hash.new(0) # Track failure counts
    @circuit_breakers = {} # Circuit breaker states
  end

  def generate_with_fallback(content, task_type)
    providers = get_provider_chain(task_type)

    providers.each do |provider_config|
      next if circuit_breaker_open?(provider_config[:provider])

      begin
        result = attempt_generation(content, provider_config)
        record_success(provider_config[:provider])
        return result

      rescue StandardError =&gt; e
        record_failure(provider_config[:provider], e)
        Rails.logger.warn &quot;Provider #{provider_config[:provider]} failed: #{e.message}&quot;

        # Continue to next provider
        next
      end
    end

    # All providers failed, use basic fallback
    Rails.logger.error &quot;All LLM providers failed, using basic fallback&quot;
    generate_basic_fallback(content, task_type)
  end

  private

  def get_provider_chain(task_type)
    case task_type
    when :summary
      [
        { provider: :openai, model: &quot;gpt-4o&quot;, priority: 1 },
        { provider: :anthropic, model: &quot;claude-3-sonnet&quot;, priority: 2 },
        { provider: :ollama, model: &quot;llama2&quot;, priority: 3 }
      ]
    when :embedding
      [
        { provider: :openai, model: &quot;text-embedding-3-small&quot;, priority: 1 },
        { provider: :huggingface, model: &quot;sentence-transformers/all-MiniLM-L6-v2&quot;, priority: 2 }
      ]
    end.sort_by { |config| config[:priority] }
  end
end
</code></pre>
<p><strong>Error Classification:</strong></p>
<pre><code class="language-ruby">module ErrorClassification
  class LLMError &lt; StandardError; end
  class RateLimitError &lt; LLMError; end
  class AuthenticationError &lt; LLMError; end
  class QuotaExceededError &lt; LLMError; end
  class ModelUnavailableError &lt; LLMError; end
  class InvalidRequestError &lt; LLMError; end
  class NetworkError &lt; LLMError; end
  class TimeoutError &lt; LLMError; end

  def classify_error(error_response)
    case error_response
    when /rate limit/i, /too many requests/i
      RateLimitError.new(error_response)
    when /unauthorized/i, /invalid api key/i
      AuthenticationError.new(error_response)
    when /quota exceeded/i, /billing/i
      QuotaExceededError.new(error_response)
    when /model.*not available/i, /model.*not found/i
      ModelUnavailableError.new(error_response)
    when /timeout/i, /connection/i
      NetworkError.new(error_response)
    else
      LLMError.new(error_response)
    end
  end
end
</code></pre>
<p><strong>Circuit Breaker Patterns:</strong></p>
<pre><code class="language-ruby">class CircuitBreaker
  FAILURE_THRESHOLD = 5
  RECOVERY_TIMEOUT = 300 # 5 minutes
  HALF_OPEN_MAX_CALLS = 3

  def initialize(provider)
    @provider = provider
    @failure_count = 0
    @last_failure_time = nil
    @state = :closed # :closed, :open, :half_open
    @half_open_calls = 0
  end

  def call(&amp;block)
    case @state
    when :closed
      execute_closed(&amp;block)
    when :open
      execute_open(&amp;block)
    when :half_open
      execute_half_open(&amp;block)
    end
  end

  private

  def execute_closed(&amp;block)
    begin
      result = block.call
      reset_failure_count
      result
    rescue StandardError =&gt; e
      record_failure
      if @failure_count &gt;= FAILURE_THRESHOLD
        @state = :open
        @last_failure_time = Time.current
        Rails.logger.error &quot;Circuit breaker opened for #{@provider}&quot;
      end
      raise e
    end
  end

  def execute_open(&amp;block)
    if Time.current - @last_failure_time &gt; RECOVERY_TIMEOUT
      @state = :half_open
      @half_open_calls = 0
      Rails.logger.info &quot;Circuit breaker half-open for #{@provider}&quot;
      execute_half_open(&amp;block)
    else
      raise CircuitBreakerOpenError, &quot;Circuit breaker is open for #{@provider}&quot;
    end
  end

  def execute_half_open(&amp;block)
    begin
      result = block.call
      @half_open_calls += 1

      if @half_open_calls &gt;= HALF_OPEN_MAX_CALLS
        @state = :closed
        reset_failure_count
        Rails.logger.info &quot;Circuit breaker closed for #{@provider}&quot;
      end

      result
    rescue StandardError =&gt; e
      @state = :open
      @last_failure_time = Time.current
      Rails.logger.error &quot;Circuit breaker reopened for #{@provider}&quot;
      raise e
    end
  end
end
</code></pre>
<h3 id="rate-limiting">Rate Limiting</h3>
<p><strong>Request Throttling:</strong></p>
<pre><code class="language-ruby">class RateLimitManager
  def initialize
    @request_timestamps = Hash.new { |h, k| h[k] = [] }
    @rate_limits = {
      openai: { requests_per_minute: 60, tokens_per_minute: 150_000 },
      anthropic: { requests_per_minute: 50, tokens_per_minute: 100_000 },
      google: { requests_per_minute: 100, tokens_per_minute: 200_000 }
    }
  end

  def throttle_request(provider, estimated_tokens)
    cleanup_old_timestamps(provider)

    current_requests = @request_timestamps[provider].length
    current_tokens = calculate_current_token_usage(provider)

    limits = @rate_limits[provider]

    # Check request rate limit
    if current_requests &gt;= limits[:requests_per_minute]
      delay = calculate_request_delay(provider)
      Rails.logger.info &quot;Rate limiting: waiting #{delay}s for #{provider}&quot;
      sleep(delay)
    end

    # Check token rate limit
    if current_tokens + estimated_tokens &gt; limits[:tokens_per_minute]
      delay = calculate_token_delay(provider, estimated_tokens)
      Rails.logger.info &quot;Token rate limiting: waiting #{delay}s for #{provider}&quot;
      sleep(delay)
    end

    # Record this request
    @request_timestamps[provider] &lt;&lt; Time.current
  end

  private

  def cleanup_old_timestamps(provider)
    cutoff = Time.current - 60 # Remove timestamps older than 1 minute
    @request_timestamps[provider].reject! { |timestamp| timestamp &lt; cutoff }
  end
end
</code></pre>
<p><strong>Queue Management:</strong></p>
<pre><code class="language-ruby">class LLMRequestQueue
  def initialize
    @queues = {
      high_priority: [],
      normal_priority: [],
      low_priority: []
    }
    @processing = false
  end

  def enqueue_request(request, priority: :normal_priority)
    @queues[priority] &lt;&lt; {
      request: request,
      timestamp: Time.current,
      retries: 0
    }

    process_queue unless @processing
  end

  def process_queue
    @processing = true

    while (item = get_next_item)
      begin
        result = process_request(item[:request])
        item[:request][:callback].call(result) if item[:request][:callback]

      rescue RateLimitError =&gt; e
        if item[:retries] &lt; 3
          item[:retries] += 1
          delay = extract_retry_delay(e) || 60

          # Re-queue with delay
          Thread.new do
            sleep(delay)
            @queues[:normal_priority] &lt;&lt; item
          end
        else
          handle_failed_request(item, e)
        end

      rescue StandardError =&gt; e
        handle_failed_request(item, e)
      end
    end

    @processing = false
  end

  private

  def get_next_item
    # Process high priority first, then normal, then low
    [:high_priority, :normal_priority, :low_priority].each do |priority|
      return @queues[priority].shift unless @queues[priority].empty?
    end
    nil
  end
end
</code></pre>
<p><strong>Backoff Strategies:</strong></p>
<pre><code class="language-ruby">class AdaptiveBackoffStrategy
  def initialize
    @base_delay = 1.0
    @max_delay = 300.0 # 5 minutes
    @backoff_multiplier = 2.0
    @jitter_factor = 0.1
  end

  def calculate_delay(attempt, error_type = :generic)
    base = case error_type
           when :rate_limit
             @base_delay * 2 # Longer delays for rate limits
           when :server_error
             @base_delay * 1.5
           else
             @base_delay
           end

    # Exponential backoff with jitter
    delay = base * (@backoff_multiplier ** (attempt - 1))
    delay = [@max_delay, delay].min

    # Add jitter to prevent thundering herd
    jitter = delay * @jitter_factor * rand
    delay + jitter
  end

  def calculate_rate_limit_delay(error_response)
    # Try to extract suggested delay from error response
    if match = error_response.match(/retry.*?(\d+).*?second/i)
      match[1].to_i + rand(5) # Add small jitter
    elsif match = error_response.match(/retry.*?(\d+).*?minute/i)
      (match[1].to_i * 60) + rand(30)
    else
      60 + rand(30) # Default 60-90 seconds
    end
  end
end
</code></pre>
<p><strong>Cost Management:</strong></p>
<pre><code class="language-ruby">class CostManager
  def initialize
    @daily_spend = 0.0
    @monthly_spend = 0.0
    @cost_per_provider = Hash.new(0.0)
    @cost_tracking_enabled = true
  end

  def track_request_cost(provider, tokens_used, model)
    return unless @cost_tracking_enabled

    cost = calculate_cost(provider, tokens_used, model)

    @daily_spend += cost
    @monthly_spend += cost
    @cost_per_provider[provider] += cost

    # Check budget limits
    check_budget_limits(cost)

    # Log cost tracking
    Rails.logger.info &quot;LLM cost tracking: #{provider}/#{model} - #{tokens_used} tokens - $#{cost.round(4)}&quot;
  end

  private

  def calculate_cost(provider, tokens, model)
    rate = PRICING_TABLE[provider][model] || 0.002 # Fallback rate
    (tokens / 1000.0) * rate
  end

  def check_budget_limits
    daily_limit = Ragdoll.config.cost_optimization[:budget_limits][:daily]
    monthly_limit = Ragdoll.config.cost_optimization[:budget_limits][:monthly]

    if @daily_spend &gt; daily_limit * 0.9 # 90% of limit
      Rails.logger.warn &quot;Approaching daily budget limit: $#{@daily_spend}/$#{daily_limit}&quot;

      if @daily_spend &gt; daily_limit
        raise BudgetExceededError, &quot;Daily budget limit exceeded: $#{@daily_spend}&quot;
      end
    end
  end
end
</code></pre>
<h2 id="best-practices">Best Practices</h2>
<p>Implementing LLM integration effectively requires careful consideration of provider characteristics, cost optimization, security, and quality assurance. These best practices are derived from production deployments and real-world experience.</p>
<h3 id="provider-specific-optimization">Provider-Specific Optimization</h3>
<p><strong>OpenAI Optimization:</strong></p>
<pre><code class="language-ruby"># OpenAI-specific optimizations
config.openai_optimization = {
  # Use streaming for long responses
  enable_streaming: true,

  # Optimize token usage
  token_optimization: {
    max_tokens: 4000,
    temperature: 0.3,    # Lower for consistent results
    top_p: 0.9,         # Nucleus sampling
    frequency_penalty: 0.1,
    presence_penalty: 0.1
  },

  # Batch embeddings for cost efficiency
  embedding_batch_size: 100,

  # Use cheaper models for simple tasks
  model_selection: {
    simple_tasks: &quot;gpt-3.5-turbo&quot;,
    complex_tasks: &quot;gpt-4o&quot;,
    embeddings: &quot;text-embedding-3-small&quot;
  }
}

class OptimizedOpenAIService
  def generate_summary(content)
    # Use gpt-3.5-turbo for short content, gpt-4o for complex content
    model = content.length &lt; 2000 ? &quot;gpt-3.5-turbo&quot; : &quot;gpt-4o&quot;

    # Optimize prompt for OpenAI
    prompt = build_openai_optimized_prompt(content)

    RubyLLM.chat.with_model(model)
           .with_temperature(0.3)
           .with_max_tokens(300)
           .add_message(role: &quot;user&quot;, content: prompt)
           .complete
  end

  private

  def build_openai_optimized_prompt(content)
    # OpenAI responds well to structured prompts
    &lt;&lt;~PROMPT
      Task: Create a concise summary of the following content.

      Requirements:
      - Maximum 250 words
      - Focus on key points and main themes
      - Use clear, professional language

      Content:
      #{content}

      Summary:
    PROMPT
  end
end
</code></pre>
<p><strong>Anthropic (Claude) Optimization:</strong></p>
<pre><code class="language-ruby"># Claude-specific optimizations
config.anthropic_optimization = {
  # Claude handles long context very well
  max_context_length: 100_000,

  # Optimize for Claude's strengths
  use_cases: {
    document_analysis: &quot;claude-3-sonnet&quot;,    # Excellent for documents
    creative_writing: &quot;claude-3-opus&quot;,      # Best creative capabilities
    cost_effective: &quot;claude-3-haiku&quot;        # Fast and cheap
  },

  # Claude-optimized parameters
  generation_params: {
    temperature: 0.1,     # Claude is naturally creative
    max_tokens: 1000
  }
}

class OptimizedClaudeService
  def analyze_document(content)
    # Claude excels at document analysis
    prompt = build_claude_optimized_prompt(content)

    RubyLLM.chat.with_model(&quot;claude-3-sonnet&quot;)
           .with_temperature(0.1)
           .add_message(role: &quot;user&quot;, content: prompt)
           .complete
  end

  private

  def build_claude_optimized_prompt(content)
    # Claude prefers conversational, detailed prompts
    &lt;&lt;~PROMPT
      I need you to analyze this document and provide insights. Please:

      1. Identify the main themes and topics
      2. Extract key facts and data points
      3. Summarize the document's purpose and conclusions
      4. Note any important technical details

      Here's the document to analyze:

      #{content}

      Please provide a thorough analysis following the structure above.
    PROMPT
  end
end
</code></pre>
<p><strong>Local Model (Ollama) Optimization:</strong></p>
<pre><code class="language-ruby"># Ollama-specific optimizations
config.ollama_optimization = {
  # Optimize for local hardware
  gpu_acceleration: true,
  memory_management: {
    model_cache_size: &quot;8GB&quot;,
    concurrent_requests: 4
  },

  # Model selection for different tasks
  models: {
    general: &quot;llama2:7b&quot;,
    code: &quot;codellama:7b&quot;,
    embedding: &quot;nomic-embed-text&quot;
  },

  # Performance tuning
  generation_params: {
    temperature: 0.7,
    num_ctx: 4096,      # Context window
    num_predict: 512,   # Max prediction tokens
    repeat_penalty: 1.1
  }
}

class OptimizedOllamaService
  def initialize
    @model_cache = {}
    @request_queue = Queue.new
    setup_worker_threads
  end

  def generate_text(prompt, model: &quot;llama2:7b&quot;)
    # Warm up model if not cached
    warm_up_model(model) unless @model_cache[model]

    # Use optimized prompt for local models
    optimized_prompt = optimize_for_local_model(prompt)

    RubyLLM.chat.with_model(model)
           .with_temperature(0.7)
           .add_message(role: &quot;user&quot;, content: optimized_prompt)
           .complete
  end

  private

  def optimize_for_local_model(prompt)
    # Local models benefit from more structured prompts
    &quot;### Instruction:\n#{prompt}\n\n### Response:&quot;
  end
end
</code></pre>
<h3 id="model-fine-tuning-approaches">Model Fine-Tuning Approaches</h3>
<p><strong>Domain-Specific Fine-Tuning:</strong></p>
<pre><code class="language-ruby">class FineTuningManager
  def self.prepare_training_data(domain:, documents:)
    training_examples = []

    documents.each do |doc|
      # Create training examples from document content
      examples = case domain
                 when :legal
                   extract_legal_examples(doc)
                 when :medical
                   extract_medical_examples(doc)
                 when :technical
                   extract_technical_examples(doc)
                 end

      training_examples.concat(examples)
    end

    # Format for OpenAI fine-tuning
    format_for_openai_training(training_examples)
  end

  def self.create_fine_tuned_model(training_file:, base_model: &quot;gpt-3.5-turbo&quot;)
    # Submit fine-tuning job to OpenAI
    client = OpenAI::Client.new

    response = client.fine_tuning.create(
      training_file: training_file,
      model: base_model,
      hyperparameters: {
        n_epochs: 3,
        batch_size: 16,
        learning_rate: 0.0001
      }
    )

    response[&quot;id&quot;]
  end
end
</code></pre>
<h3 id="prompt-engineering">Prompt Engineering</h3>
<p><strong>Effective Prompt Patterns:</strong></p>
<pre><code class="language-ruby">module PromptEngineering
  # Chain-of-thought prompting
  def self.build_cot_prompt(question, context: nil)
    prompt = &quot;Let's approach this step-by-step:\n\n&quot;
    prompt += &quot;Context: #{context}\n\n&quot; if context
    prompt += &quot;Question: #{question}\n\n&quot;
    prompt += &quot;Let's think through this carefully:\n&quot;
    prompt += &quot;1) First, I'll identify the key elements...\n&quot;
    prompt += &quot;2) Then, I'll analyze the relationships...\n&quot;
    prompt += &quot;3) Finally, I'll draw conclusions...\n\n&quot;
    prompt += &quot;Step-by-step analysis:&quot;
    prompt
  end

  # Few-shot prompting
  def self.build_few_shot_prompt(task, examples, input)
    prompt = &quot;Here are some examples of #{task}:\n\n&quot;

    examples.each_with_index do |example, index|
      prompt += &quot;Example #{index + 1}:\n&quot;
      prompt += &quot;Input: #{example[:input]}\n&quot;
      prompt += &quot;Output: #{example[:output]}\n\n&quot;
    end

    prompt += &quot;Now, please #{task} for the following:\n&quot;
    prompt += &quot;Input: #{input}\n&quot;
    prompt += &quot;Output:&quot;
    prompt
  end

  # Role-based prompting
  def self.build_role_prompt(role, task, content)
    &lt;&lt;~PROMPT
      You are a #{role} with extensive experience in your field.

      Your task: #{task}

      Please approach this with your professional expertise and provide:
      - Clear, accurate analysis
      - Relevant professional insights
      - Actionable recommendations

      Content to analyze:
      #{content}

      Your professional analysis:
    PROMPT
  end
end

# Usage examples
class SmartPromptService
  include PromptEngineering

  def generate_technical_summary(content)
    prompt = build_role_prompt(
      &quot;senior technical writer&quot;,
      &quot;create a concise technical summary&quot;,
      content
    )

    generate_with_prompt(prompt)
  end

  def extract_key_insights(content)
    examples = [
      {
        input: &quot;Quarterly revenue increased by 15% due to strong product sales...&quot;,
        output: &quot;Revenue growth: +15%, Driver: Strong product sales, Time: Quarterly&quot;
      },
      {
        input: &quot;Customer satisfaction scores improved following the new support system...&quot;,
        output: &quot;Customer satisfaction: Improved, Cause: New support system, Impact: Positive&quot;
      }
    ]

    prompt = build_few_shot_prompt(&quot;extract key insights&quot;, examples, content)
    generate_with_prompt(prompt)
  end
end
</code></pre>
<h3 id="quality-assessment">Quality Assessment</h3>
<p><strong>Automated Quality Metrics:</strong></p>
<pre><code class="language-ruby">class QualityAssessmentService
  def assess_summary_quality(summary, original_content)
    metrics = {}

    # Semantic similarity
    metrics[:semantic_similarity] = calculate_semantic_similarity(summary, original_content)

    # Information coverage
    metrics[:coverage] = calculate_information_coverage(summary, original_content)

    # Coherence score
    metrics[:coherence] = calculate_coherence_score(summary)

    # Factual accuracy
    metrics[:factual_accuracy] = verify_factual_accuracy(summary, original_content)

    # Length appropriateness
    metrics[:length_score] = assess_length_appropriateness(summary, original_content)

    # Overall quality score
    metrics[:overall_score] = calculate_overall_score(metrics)

    metrics
  end

  def assess_keyword_quality(keywords, content)
    {
      relevance: calculate_keyword_relevance(keywords, content),
      coverage: calculate_keyword_coverage(keywords, content),
      specificity: calculate_keyword_specificity(keywords),
      uniqueness: calculate_keyword_uniqueness(keywords)
    }
  end

  private

  def calculate_semantic_similarity(text1, text2)
    embedding1 = embedding_service.generate_embedding(text1)
    embedding2 = embedding_service.generate_embedding(text2)

    cosine_similarity(embedding1, embedding2)
  end

  def calculate_information_coverage(summary, original)
    # Extract key concepts from both texts
    original_concepts = extract_key_concepts(original)
    summary_concepts = extract_key_concepts(summary)

    # Calculate coverage ratio
    covered_concepts = original_concepts &amp; summary_concepts
    covered_concepts.length.to_f / original_concepts.length
  end
end

# Quality monitoring and alerting
class QualityMonitor
  QUALITY_THRESHOLDS = {
    semantic_similarity: 0.75,
    information_coverage: 0.80,
    coherence: 0.85,
    factual_accuracy: 0.90
  }

  def monitor_quality(result, type)
    quality_scores = assess_quality(result, type)

    # Check for quality issues
    issues = []
    QUALITY_THRESHOLDS.each do |metric, threshold|
      if quality_scores[metric] &lt; threshold
        issues &lt;&lt; &quot;#{metric}: #{quality_scores[metric]} (threshold: #{threshold})&quot;
      end
    end

    # Alert if quality issues found
    if issues.any?
      alert_quality_issues(type, issues, quality_scores)
    end

    quality_scores
  end

  private

  def alert_quality_issues(type, issues, scores)
    Rails.logger.warn &quot;Quality issues detected for #{type}:&quot;
    issues.each { |issue| Rails.logger.warn &quot;  - #{issue}&quot; }

    # Send notification if configured
    if Rails.env.production?
      QualityAlertMailer.quality_degradation(
        type: type,
        issues: issues,
        scores: scores
      ).deliver_now
    end
  end
end
</code></pre>
<h3 id="production-deployment-checklist">Production Deployment Checklist</h3>
<pre><code class="language-ruby"># Production readiness checklist
class ProductionReadinessChecker
  def self.check_readiness
    checks = {
      api_keys_configured: check_api_keys,
      rate_limits_configured: check_rate_limits,
      error_handling_enabled: check_error_handling,
      monitoring_setup: check_monitoring,
      cost_controls_active: check_cost_controls,
      quality_thresholds_set: check_quality_thresholds,
      backup_providers_configured: check_backup_providers
    }

    failed_checks = checks.select { |check, passed| !passed }

    if failed_checks.any?
      raise ProductionReadinessError, &quot;Failed checks: #{failed_checks.keys.join(', ')}&quot;
    end

    true
  end

  private

  def self.check_api_keys
    required_keys = %w[OPENAI_API_KEY ANTHROPIC_API_KEY]
    required_keys.all? { |key| ENV[key].present? }
  end

  def self.check_rate_limits
    Ragdoll.config.respond_to?(:rate_limits) &amp;&amp;
    Ragdoll.config.rate_limits.present?
  end

  def self.check_cost_controls
    Ragdoll.config.cost_optimization[:enable] &amp;&amp;
    Ragdoll.config.cost_optimization[:budget_limits].present?
  end
end
</code></pre>
<hr />
<p><em>This document is part of the Ragdoll documentation suite. For immediate help, see the <a href="../quick-start/">Quick Start Guide</a> or <a href="../api-client/">API Reference</a>.</em></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.50899def.min.js"></script>
      
    
  </body>
</html>