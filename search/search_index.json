{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":"\ud83c\udfaa Ragdoll Documentation <p>Advanced multi-modal document intelligence platform built on ActiveRecord with PostgreSQL + pgvector</p>        \ud83d\ude80 Quick Start             \ud83d\udce6 Installation             \ud83c\udfd7\ufe0f Architecture      \ud83c\udfaf Multi-Modal First <p>Designed from the ground up to handle text, image, and audio content as first-class citizens through a sophisticated polymorphic architecture.</p> \ud83c\udfd7\ufe0f Production Ready <p>Enterprise-grade features including 7 LLM providers, PostgreSQL + pgvector, background processing, and comprehensive error handling.</p> \u26a1 Performance Optimized <p>Hardware-accelerated vector operations, intelligent indexing, and connection pooling for scalable deployments.</p> \ud83d\udcca Advanced Analytics <p>Sophisticated ranking algorithms, usage tracking, and performance monitoring with smart search result optimization.</p> \ud83d\udd27 Extensible <p>Clear patterns for adding new content types, document processors, embedding providers, and search algorithms.</p> \ud83d\udee1\ufe0f Secure <p>Production security best practices, API key management, file validation, and comprehensive audit logging.</p>"},{"location":"#whats-new","title":"\ud83c\udd95 What's New","text":""},{"location":"#search-tracking-system-v019","title":"Search Tracking System (v0.1.9)","text":"<p>Ragdoll now includes comprehensive search tracking and analytics capabilities:</p> <ul> <li>Automatic Search Recording: All searches are automatically tracked with query embeddings, execution times, and result metrics</li> <li>Search Similarity Analysis: Find similar searches using vector similarity on query embeddings  </li> <li>Click-Through Tracking: Monitor user engagement with search results</li> <li>Performance Analytics: Track slow queries, execution times, and search patterns</li> <li>Session &amp; User Tracking: Associate searches with sessions and users for behavior analysis</li> <li>Automatic Cleanup: Orphaned and old unused searches are automatically cleaned up</li> </ul> <p>Learn more about Search Tracking \u2192</p>"},{"location":"#documentation-overview","title":"\ud83d\udcda Documentation Overview","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Quick Start Guide - Get up and running with Ragdoll in minutes</li> <li>Installation &amp; Setup - Complete installation and environment setup</li> <li>Configuration Guide - Comprehensive configuration system documentation</li> </ul>"},{"location":"#core-architecture","title":"Core Architecture","text":"<ul> <li>Architecture Overview - System design and component relationships</li> <li>Multi-Modal Support - Text, image, and audio content handling</li> <li>Database Schema - Polymorphic multi-modal database design</li> <li>Background Processing - ActiveJob integration and async operations</li> </ul>"},{"location":"#features-capabilities","title":"Features &amp; Capabilities","text":"<ul> <li>Document Processing - File parsing, metadata extraction, and content analysis</li> <li>Search &amp; Analytics - Advanced semantic search with usage analytics</li> <li>Embedding System - Vector generation and similarity search</li> <li>File Upload System - Shrine-based production file handling</li> </ul>"},{"location":"#api-documentation","title":"API Documentation","text":"<ul> <li>Client API Reference - High-level client interface methods</li> <li>Models Reference - ActiveRecord models and relationships</li> <li>Services Reference - Business logic and processing services</li> <li>Jobs Reference - Background job system</li> </ul>"},{"location":"#deployment-operations","title":"Deployment &amp; Operations","text":"<ul> <li>Production Deployment - Production setup with PostgreSQL + pgvector</li> <li>Performance Tuning - Optimization strategies and monitoring</li> <li>Monitoring &amp; Analytics - Usage tracking and system health</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"#advanced-topics","title":"Advanced Topics","text":"<ul> <li>LLM Integration - Multiple provider support and configuration</li> <li>Metadata Schemas - Structured content analysis and validation</li> <li>Extending the System - Adding new content types and processors</li> <li>Security Considerations - Production security best practices</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Development Setup - Setting up development environment</li> <li>Testing Guide - Running tests and coverage analysis</li> <li>Contributing - Guidelines for contributing to the project</li> </ul>"},{"location":"#what-makes-ragdoll-special","title":"\ud83d\ude80 What Makes Ragdoll Special","text":"<p>Ragdoll is not just a \"simple RAG library\" - it's a production-ready document intelligence platform with enterprise-grade features:</p>"},{"location":"#multi-modal-first","title":"\ud83c\udfaf Multi-Modal First","text":"<p>Unlike most RAG systems that retrofit multi-modal support, Ragdoll was designed from the ground up to handle text, image, and audio content as first-class citizens through a sophisticated polymorphic architecture.</p>"},{"location":"#sophisticated-architecture","title":"\ud83c\udfd7\ufe0f Sophisticated Architecture","text":"<ul> <li>Dual Metadata Design: Separates LLM-generated content analysis from system file properties</li> <li>Polymorphic Database Schema: Unified search across all content types</li> <li>Background Processing: Complete ActiveJob integration for scalable operations</li> <li>Production File Handling: Shrine-based upload system with validation</li> </ul>"},{"location":"#advanced-analytics","title":"\ud83d\udcca Advanced Analytics","text":"<ul> <li>Usage Tracking: Sophisticated ranking algorithms based on frequency and recency</li> <li>Performance Monitoring: Built-in analytics for search patterns and system health</li> <li>Smart Ranking: Combines similarity scores with usage analytics for better results</li> </ul>"},{"location":"#enterprise-features","title":"\ud83d\udd27 Enterprise Features","text":"<ul> <li>7 LLM Providers: OpenAI, Anthropic, Google, Azure, Ollama, HuggingFace, OpenRouter</li> <li>Production Database Support: PostgreSQL + pgvector</li> <li>Comprehensive Error Handling: Custom exception hierarchy with detailed logging</li> <li>Health Monitoring: System diagnostics and status reporting</li> </ul>"},{"location":"#performance-optimized","title":"\u26a1 Performance Optimized","text":"<ul> <li>pgvector Integration: Hardware-accelerated vector operations</li> <li>Intelligent Indexing: Optimized database indexes for fast search</li> <li>Background Processing: Non-blocking document processing</li> <li>Connection Pooling: Scalable database connections</li> </ul>"},{"location":"#documentation-philosophy","title":"\ud83d\udcd6 Documentation Philosophy","text":"<p>This documentation is implementation-driven - every feature documented here is fully implemented and tested. We believe in accurate documentation that matches the actual capabilities of the system.</p>"},{"location":"#what-youll-find-here","title":"What You'll Find Here:","text":"<ul> <li>\u2705 Accurate Examples: All code examples are tested and working</li> <li>\u2705 Production-Ready Guidance: Real-world deployment and optimization advice</li> <li>\u2705 Complete Feature Coverage: Documentation for all implemented features</li> <li>\u2705 Advanced Use Cases: Enterprise scenarios and complex integrations</li> </ul>"},{"location":"#what-you-wont-find","title":"What You Won't Find:","text":"<ul> <li>\u274c Vapor Features: We don't document features that don't exist</li> <li>\u274c Oversimplified Examples: Our examples reflect real-world complexity</li> <li>\u274c Marketing Fluff: Technical accuracy over marketing copy</li> </ul>"},{"location":"#getting-help","title":"\ud83e\udd1d Getting Help","text":""},{"location":"#documentation-issues","title":"Documentation Issues","text":"<p>If you find any discrepancies between the documentation and actual implementation, please file an issue. We maintain strict accuracy standards.</p>"},{"location":"#feature-requests","title":"Feature Requests","text":"<p>Ragdoll has many undocumented capabilities. Before requesting a feature, check if it already exists by reviewing the complete documentation.</p>"},{"location":"#support-channels","title":"Support Channels","text":"<ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>Documentation: Comprehensive guides and references</li> <li>Code Examples: Working examples for all major features</li> </ul>"},{"location":"#quick-navigation","title":"\ud83c\udfaf Quick Navigation","text":"<p>New to Ragdoll? Start with:</p> <ol> <li>Quick Start Guide - Basic usage in 5 minutes</li> <li>Architecture Overview - Understand the system design</li> <li>Multi-Modal Support - See what makes us different</li> </ol> <p>Ready for Production? Focus on:</p> <ol> <li>Production Deployment - PostgreSQL setup</li> <li>Configuration Guide - Enterprise configuration</li> <li>Performance Tuning - Optimization strategies</li> </ol> <p>Integrating with Existing Systems? Review:</p> <ol> <li>API Reference - Client interface methods</li> <li>LLM Integration - Provider configuration</li> <li>Security Considerations - Production security</li> </ol> <p>This documentation is intended to reflect the actual implementation of Ragdoll v0.1.10 and should be updated with each release to maintain accuracy.</p>"},{"location":"api-reference/api-client/","title":"Client API Reference","text":"<p>The Ragdoll Client provides a high-level interface for all document intelligence operations. This comprehensive API handles document management, search operations, RAG enhancement, and system monitoring through a clean, intuitive interface.</p>"},{"location":"api-reference/api-client/#overview","title":"Overview","text":"<p>Schema Note: Following recent schema optimization, the Ragdoll database now uses a normalized schema where <code>embedding_model</code> information is stored in content-specific tables rather than duplicated in individual embeddings. This provides better data organization while maintaining all API functionality.</p> <p>The Client class serves as the primary orchestration layer, providing:</p> <ul> <li>Document Lifecycle Management: Add, update, delete, and monitor documents</li> <li>Multi-Modal Content Support: Handle text, image, and audio content seamlessly</li> <li>Advanced Search Operations: Semantic, full-text, and hybrid search capabilities</li> <li>RAG Enhancement: Context retrieval and prompt enhancement for LLM applications</li> <li>System Analytics: Health monitoring, usage statistics, and performance metrics</li> <li>Background Processing: Asynchronous job management and status tracking</li> </ul>"},{"location":"api-reference/api-client/#client-initialization","title":"Client Initialization","text":""},{"location":"api-reference/api-client/#basic-initialization","title":"Basic Initialization","text":"<pre><code># Setup configuration first\nRagdoll::Core.configure do |config|\n  config.llm_providers[:openai][:api_key] = ENV['OPENAI_API_KEY']\n  config.models[:embedding][:text] = 'openai/text-embedding-3-small'\n  config.models[:text_generation][:default] = 'openai/gpt-4o-mini'\n  config.database = {\n    adapter: 'postgresql',\n    database: 'ragdoll_development',\n    username: 'ragdoll',\n    password: ENV['RAGDOLL_DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true\n  }\nend\n\n# Initialize client (uses global configuration)\nclient = Ragdoll::Core::Client.new\n</code></pre>"},{"location":"api-reference/api-client/#document-management","title":"Document Management","text":""},{"location":"api-reference/api-client/#adding-documents","title":"Adding Documents","text":""},{"location":"api-reference/api-client/#single-document-addition","title":"Single Document Addition","text":"<pre><code># Add document from file path\nresult = client.add_document(path: 'research_paper.pdf')\n# Returns:\n# {\n#   success: true,\n#   document_id: \"123\",\n#   title: \"research_paper\",\n#   document_type: \"pdf\",\n#   content_length: 15000,\n#   embeddings_queued: true,\n#   message: \"Document 'research_paper' added successfully with ID 123\"\n# }\n</code></pre>"},{"location":"api-reference/api-client/#text-content-addition","title":"Text Content Addition","text":"<pre><code># Add raw text content\ndoc_id = client.add_text(\n  content: \"This is the text content to be processed...\",\n  title: \"Text Document\",\n  source: 'user_input',\n  language: 'en'\n)\n\n# Add text with custom chunking\ndoc_id = client.add_text(\n  content: long_text_content,\n  title: \"Long Text Document\",\n  chunk_size: 800,\n  chunk_overlap: 150\n)\n</code></pre>"},{"location":"api-reference/api-client/#directory-processing","title":"Directory Processing","text":"<pre><code># Process entire directory\nresults = client.add_directory(\n  path: '/path/to/documents',\n  recursive: true\n)\n\n# Returns array of results:\n# [\n#   { file: \"/path/to/doc1.pdf\", document_id: \"123\", status: \"success\" },\n#   { file: \"/path/to/doc2.txt\", document_id: \"124\", status: \"success\" },\n#   { file: \"/path/to/bad.doc\", error: \"Unsupported format\", status: \"error\" }\n# ]\n</code></pre>"},{"location":"api-reference/api-client/#document-retrieval","title":"Document Retrieval","text":""},{"location":"api-reference/api-client/#single-document-retrieval","title":"Single Document Retrieval","text":"<pre><code># Get document by ID\ndocument = client.get_document(id: \"123\")\n# Returns document hash with basic information\n\n# Get document status\nstatus = client.document_status(id: \"123\")\n# Returns:\n# {\n#   id: \"123\",\n#   title: \"research_paper\",\n#   status: \"processed\",\n#   embeddings_count: 24,\n#   embeddings_ready: true,\n#   content_preview: \"This research paper discusses...\",\n#   message: \"Document processed successfully with 24 embeddings\"\n# }\n</code></pre>"},{"location":"api-reference/api-client/#document-listing","title":"Document Listing","text":"<pre><code># List documents with basic options\ndocuments = client.list_documents\n\n# List with options\ndocuments = client.list_documents(\n  status: 'processed',\n  document_type: 'pdf',\n  limit: 20\n)\n</code></pre>"},{"location":"api-reference/api-client/#document-updates","title":"Document Updates","text":"<pre><code># Update document metadata\nresult = client.update_document(\n  id: \"123\",\n  title: \"Updated Title\",\n  metadata: {\n    category: 'technical',\n    priority: 'high',\n    last_reviewed: Date.current\n  }\n)\n\n# Reprocess document with new settings\nresult = client.reprocess_document(\n  id: \"123\",\n  chunk_size: 1200,\n  regenerate_embeddings: true,\n  update_summary: true,\n  extract_keywords: true\n)\n</code></pre>"},{"location":"api-reference/api-client/#document-deletion","title":"Document Deletion","text":"<pre><code># Delete single document\nresult = client.delete_document(id: \"123\")\n# Returns:\n# {\n#   success: true,\n#   message: \"Document 123 and associated content deleted successfully\",\n#   deleted_embeddings: 24,\n#   deleted_content_items: 4\n# }\n\n# Delete multiple documents\nresult = client.delete_documents(ids: [\"123\", \"124\", \"125\"])\n\n# Delete with criteria\nresult = client.delete_documents(\n  status: 'error',\n  created_before: 1.month.ago,\n  confirm: true  # Safety check for bulk deletion\n)\n</code></pre>"},{"location":"api-reference/api-client/#search-operations","title":"Search Operations","text":""},{"location":"api-reference/api-client/#basic-search","title":"Basic Search","text":"<pre><code># Simple semantic search\nresults = client.search(query: \"machine learning algorithms\")\n\n# Search with options\nresults = client.search(\n  query: \"neural network architectures\",\n  limit: 25,\n  threshold: 0.8\n)\n\n# Returns:\n# {\n#   query: \"neural network architectures\",\n#   results: [\n#     {\n#       embedding_id: \"456\",\n#       document_id: \"123\",\n#       document_title: \"Deep Learning Fundamentals\",\n#       document_location: \"/path/to/document.pdf\",\n#       content: \"Neural networks are computational models...\",\n#       similarity: 0.92,\n#       chunk_index: 3,\n#       usage_count: 5\n#     }\n#   ],\n#   total_results: 15\n# }\n\n# Hybrid search (semantic + full-text)\nresults = client.hybrid_search(\n  query: \"neural networks\",\n  semantic_weight: 0.7,\n  text_weight: 0.3\n)\n</code></pre>"},{"location":"api-reference/api-client/#search-analytics","title":"Search Analytics","text":"<pre><code># Simple search analytics available\nanalytics = client.search_analytics(days: 30)\n# Returns basic usage statistics\n</code></pre>"},{"location":"api-reference/api-client/#rag-enhancement","title":"RAG Enhancement","text":""},{"location":"api-reference/api-client/#context-retrieval","title":"Context Retrieval","text":"<pre><code># Get relevant context for a query\ncontext = client.get_context(\n  query: \"How do neural networks learn?\",\n  limit: 5\n)\n\n# Returns:\n# {\n#   context_chunks: [\n#     {\n#       content: \"Neural networks learn through backpropagation...\",\n#       source: \"/path/to/textbook.pdf\",\n#       similarity: 0.91,\n#       chunk_index: 3\n#     }\n#   ],\n#   combined_context: \"Neural networks learn through backpropagation...\",\n#   total_chunks: 5\n# }\n</code></pre>"},{"location":"api-reference/api-client/#prompt-enhancement","title":"Prompt Enhancement","text":"<pre><code># Enhance prompt with relevant context\nenhanced = client.enhance_prompt(\n  prompt: \"Explain how neural networks learn\",\n  context_limit: 5\n)\n\n# Returns:\n# {\n#   enhanced_prompt: \"You are an AI assistant. Use the following context...\",\n#   original_prompt: \"Explain how neural networks learn\",\n#   context_sources: [\"/path/to/textbook.pdf\", \"/path/to/paper.pdf\"],\n#   context_count: 3\n# }\n</code></pre>"},{"location":"api-reference/api-client/#document-status-and-monitoring","title":"Document Status and Monitoring","text":""},{"location":"api-reference/api-client/#processing-status","title":"Processing Status","text":"<pre><code># Check document processing status\nstatus = client.document_status(id: \"123\")\n# Returns:\n# {\n#   document_id: \"123\",\n#   status: \"processing\",  # pending, processing, processed, error\n#   progress: 65,          # Percentage complete\n#   message: \"Generating embeddings (15/24 chunks complete)\",\n#   jobs_queued: 2,\n#   jobs_completed: 6,\n#   estimated_completion: \"2024-01-15T10:30:00Z\",\n#   processing_steps: {\n#     text_extraction: \"completed\",\n#     embedding_generation: \"in_progress\",\n#     keyword_extraction: \"queued\",\n#     summary_generation: \"queued\"\n#   },\n#   error_details: nil\n# }\n\n# Batch status check\nstatuses = client.batch_document_status(ids: [\"123\", \"124\", \"125\"])\n</code></pre>"},{"location":"api-reference/api-client/#background-job-management","title":"Background Job Management","text":"<pre><code># Monitor background jobs\njob_status = client.job_status\n# Returns:\n# {\n#   active_jobs: 12,\n#   queued_jobs: 5,\n#   failed_jobs: 1,\n#   queue_status: {\n#     embeddings: { size: 3, latency: 2.5 },\n#     processing: { size: 2, latency: 1.1 },\n#     analysis: { size: 0, latency: 0.0 }\n#   },\n#   worker_status: {\n#     busy_workers: 4,\n#     idle_workers: 2,\n#     total_workers: 6\n#   }\n# }\n\n# Retry failed jobs\nresult = client.retry_failed_jobs(\n  job_types: ['GenerateEmbeddingsJob'],\n  older_than: 1.hour.ago\n)\n</code></pre>"},{"location":"api-reference/api-client/#system-operations","title":"System Operations","text":""},{"location":"api-reference/api-client/#health-monitoring","title":"Health Monitoring","text":"<pre><code># Simple health check\nhealthy = client.healthy?\n# Returns true/false\n\n# Basic system statistics\nstats = client.stats\n# Returns document statistics hash\n</code></pre>"},{"location":"api-reference/api-client/#error-handling","title":"Error Handling","text":""},{"location":"api-reference/api-client/#standard-error-response-format","title":"Standard Error Response Format","text":"<pre><code># All methods return structured error information on failure\nbegin\n  result = client.add_document(path: 'nonexistent.pdf')\nrescue Ragdoll::Core::DocumentNotFoundError =&gt; e\n  puts e.message  # \"File not found: nonexistent.pdf\"\n  puts e.error_code  # \"DOCUMENT_NOT_FOUND\"\n  puts e.details  # { path: 'nonexistent.pdf', checked_locations: [...] }\nend\n\n# Check for errors in response\nresult = client.search(query: \"test\")\nif result[:success]\n  # Process results\n  puts result[:results]\nelse\n  # Handle error\n  puts result[:error]\n  puts result[:error_code]\n  puts result[:details]\nend\n</code></pre>"},{"location":"api-reference/api-client/#common-error-types","title":"Common Error Types","text":"<pre><code># Document processing errors\nRagdoll::Core::DocumentNotFoundError\nRagdoll::Core::UnsupportedDocumentTypeError\nRagdoll::Core::DocumentProcessingError\n\n# Search errors\nRagdoll::Core::InvalidQueryError\nRagdoll::Core::SearchTimeoutError\nRagdoll::Core::EmbeddingGenerationError\n\n# Configuration errors\nRagdoll::Core::ConfigurationError\nRagdoll::Core::InvalidProviderError\nRagdoll::Core::MissingCredentialsError\n\n# System errors\nRagdoll::Core::DatabaseConnectionError\nRagdoll::Core::BackgroundJobError\nRagdoll::Core::SystemHealthError\n</code></pre>"},{"location":"api-reference/api-client/#best-practices","title":"Best Practices","text":""},{"location":"api-reference/api-client/#1-error-handling","title":"1. Error Handling","text":"<ul> <li>Always check for errors in responses</li> <li>Implement retry logic for transient failures</li> <li>Log errors with sufficient context for debugging</li> <li>Use appropriate error handling strategies for different error types</li> </ul>"},{"location":"api-reference/api-client/#2-performance-optimization","title":"2. Performance Optimization","text":"<ul> <li>Use batch operations for multiple documents</li> <li>Implement appropriate caching for frequently accessed data</li> <li>Monitor search performance and adjust thresholds accordingly</li> <li>Use background processing for large document collections</li> </ul>"},{"location":"api-reference/api-client/#3-search-strategy","title":"3. Search Strategy","text":"<ul> <li>Start with default settings and tune based on results quality</li> <li>Use faceted search to help users navigate large result sets</li> <li>Implement search suggestions to improve user experience</li> <li>Monitor search analytics to understand usage patterns</li> </ul>"},{"location":"api-reference/api-client/#4-rag-implementation","title":"4. RAG Implementation","text":"<ul> <li>Choose appropriate context limits based on your LLM's capabilities</li> <li>Use prompt templates for consistent formatting</li> <li>Include source attribution for transparency</li> <li>Monitor RAG response quality and adjust context retrieval accordingly</li> </ul> <p>The Client API provides a comprehensive interface for building sophisticated document intelligence applications with Ragdoll, offering both simplicity for basic use cases and advanced features for complex requirements.</p>"},{"location":"api-reference/api-jobs/","title":"Jobs Reference","text":"<p>Ragdoll uses ActiveJob for background processing of document analysis, content extraction, and embedding generation. The job system provides reliable, asynchronous processing with comprehensive error handling and monitoring capabilities.</p>"},{"location":"api-reference/api-jobs/#background-job-system","title":"Background Job System","text":"<p>The background job system orchestrates multi-step document processing workflows, ensuring efficient handling of large documents and compute-intensive AI operations. All jobs inherit from <code>ActiveJob::Base</code> and integrate seamlessly with Rails job queue adapters including Sidekiq, Resque, and Delayed Job.</p> <p>Key Features: - Asynchronous document processing pipeline - Automatic job chaining and dependency management - Comprehensive error handling with status tracking - Built-in retry mechanisms with exponential backoff - Integration with document status lifecycle management</p>"},{"location":"api-reference/api-jobs/#core-jobs","title":"Core Jobs","text":"<p>Ragdoll includes four primary job classes that handle the complete document processing pipeline:</p>"},{"location":"api-reference/api-jobs/#extracttext","title":"ExtractText","text":"<p>The <code>ExtractText</code> job initiates the document processing pipeline by extracting text content from uploaded files.</p> <pre><code>module Ragdoll\n  module Core\n    module Jobs\n      class ExtractText &lt; ActiveJob::Base\n        queue_as :default\n\n        def perform(document_id)\n          document = Ragdoll::Document.find(document_id)\n          return unless document.file_attached?\n          return if document.content.present?\n\n          document.update!(status: \"processing\")\n\n          extracted_content = document.extract_text_from_file\n\n          if extracted_content.present?\n            document.update!(\n              content: extracted_content,\n              status: \"processed\"\n            )\n\n            # Queue follow-up jobs\n            Ragdoll::GenerateSummaryJob.perform_later(document_id)\n            GenerateKeywordsJob.perform_later(document_id)\n            Ragdoll::GenerateEmbeddingsJob.perform_later(document_id)\n          else\n            document.update!(status: \"error\")\n          end\n        rescue ActiveRecord::RecordNotFound\n          # Document was deleted, nothing to do\n        rescue StandardError =&gt; e\n          document&amp;.update!(status: \"error\")\n          raise e\n        end\n      end\n    end\n  end\nend\n</code></pre> <p>Key Responsibilities: - File Processing: Extracts text from PDF, DOCX, HTML, and other supported formats - Status Management: Updates document status through processing lifecycle - Job Orchestration: Triggers downstream jobs upon successful completion - Error Recovery: Handles file access errors and processing failures</p> <p>Usage Examples: <pre><code># Queue text extraction for a document\nRagdoll::ExtractTextJob.perform_later(document.id)\n\n# Process immediately (synchronous)\nRagdoll::ExtractTextJob.perform_now(document.id)\n\n# Schedule for later processing\nRagdoll::ExtractTextJob.set(wait: 1.hour).perform_later(document.id)\n</code></pre></p>"},{"location":"api-reference/api-jobs/#generateembeddings","title":"GenerateEmbeddings","text":"<p>The <code>GenerateEmbeddings</code> job creates vector embeddings for all content associated with a document.</p> <pre><code>module Ragdoll\n  module Core\n    module Jobs\n      class GenerateEmbeddings &lt; ActiveJob::Base\n        queue_as :default\n\n        def perform(document_id, chunk_size: nil, chunk_overlap: nil)\n          document = Ragdoll::Document.find(document_id)\n          return unless document.content.present?\n          return if document.all_embeddings.exists?\n\n          # Process all content records using their own generate_embeddings! methods\n          document.contents.each(&amp;:generate_embeddings!)\n\n          # Update document status to processed\n          document.update!(status: \"processed\")\n        rescue ActiveRecord::RecordNotFound\n          # Document was deleted, nothing to do\n        rescue StandardError =&gt; e\n          if defined?(Rails)\n            Rails.logger.error \"Failed to generate embeddings for document #{document_id}: #{e.message}\"\n          end\n          raise e\n        end\n      end\n    end\n  end\nend\n</code></pre> <p>Key Features: - Multi-Modal Support: Handles text, image, and audio content embeddings - Chunking Integration: Respects content-specific chunking strategies - Batch Processing: Processes multiple content chunks efficiently - Provider Abstraction: Works with OpenAI, Hugging Face, and other embedding providers</p> <p>Configuration Options: <pre><code># Custom chunk sizing\nRagdoll::GenerateEmbeddingsJob.perform_later(\n  document.id,\n  chunk_size: 1500,\n  chunk_overlap: 300\n)\n\n# Monitor embedding generation\ndocument.contents.each do |content|\n  puts \"Embeddings for #{content.type}: #{content.embeddings.count}\"\nend\n</code></pre></p>"},{"location":"api-reference/api-jobs/#extractkeywords","title":"ExtractKeywords","text":"<p>The <code>ExtractKeywords</code> job uses LLM-powered analysis to extract relevant keywords from document content.</p> <pre><code>module Ragdoll\n  module Core\n    module Jobs\n      class ExtractKeywords &lt; ActiveJob::Base\n        queue_as :default\n\n        def perform(document_id)\n          document = Ragdoll::Document.find(document_id)\n          return unless document.content.present?\n          return if document.keywords.present?\n\n          text_service = TextGenerationService.new\n          keywords_array = text_service.extract_keywords(document.content)\n\n          if keywords_array.present?\n            keywords_string = keywords_array.join(\", \")\n            document.update!(keywords: keywords_string)\n          end\n        rescue ActiveRecord::RecordNotFound\n          # Document was deleted, nothing to do\n        rescue StandardError =&gt; e\n          Rails.logger.error \"Failed to generate keywords for document #{document_id}: #{e.message}\" if defined?(Rails)\n          raise e\n        end\n      end\n    end\n  end\nend\n</code></pre> <p>LLM Integration: - Uses configured LLM provider (OpenAI, Anthropic, etc.) - Applies intelligent keyword extraction prompts - Validates and formats keyword output - Stores keywords as searchable metadata</p> <p>Quality Controls: <pre><code># Keywords are stored as comma-separated strings\ndocument.keywords # =&gt; \"machine learning, artificial intelligence, neural networks\"\n\n# Access as array for programmatic use\ndocument.keywords_array # =&gt; [\"machine learning\", \"artificial intelligence\", \"neural networks\"]\n\n# Add/remove keywords programmatically\ndocument.add_keyword(\"deep learning\")\ndocument.remove_keyword(\"outdated term\")\n</code></pre></p>"},{"location":"api-reference/api-jobs/#generatesummary","title":"GenerateSummary","text":"<p>The <code>GenerateSummary</code> job creates concise summaries of document content using LLM providers.</p> <pre><code>module Ragdoll\n  module Core\n    module Jobs\n      class GenerateSummary &lt; ActiveJob::Base\n        queue_as :default\n\n        def perform(document_id)\n          document = Ragdoll::Document.find(document_id)\n          return unless document.content.present?\n          return if document.summary.present?\n\n          text_service = TextGenerationService.new\n          summary = text_service.generate_summary(document.content)\n\n          document.update!(summary: summary) if summary.present?\n        rescue ActiveRecord::RecordNotFound\n          # Document was deleted, nothing to do\n        rescue StandardError =&gt; e\n          Rails.logger.error \"Failed to generate summary for document #{document_id}: #{e.message}\" if defined?(Rails)\n          raise e\n        end\n      end\n    end\n  end\nend\n</code></pre> <p>Summarization Features: - Configurable Length: Respects <code>summary_max_length</code> configuration - Content Analysis: Identifies key themes and concepts - Multi-Provider Support: Works with GPT, Claude, Gemini, and other LLMs - Quality Validation: Ensures summary quality and relevance</p> <p>Configuration Examples: <pre><code># Configure summarization in Ragdoll config\nRagdoll::Core.configure do |config|\n  config.summarization_config[:enable] = true\n  config.summarization_config[:max_length] = 300\n  config.summarization_config[:min_content_length] = 500\nend\n\n# Access generated summaries\ndocument.summary # =&gt; \"This document discusses machine learning applications...\"\ndocument.has_summary? # =&gt; true\n</code></pre></p>"},{"location":"api-reference/api-jobs/#job-configuration","title":"Job Configuration","text":"<p>Ragdoll jobs are built on ActiveJob, providing flexible configuration options for different deployment scenarios.</p>"},{"location":"api-reference/api-jobs/#queue-adapters","title":"Queue Adapters","text":"<p>Support for all major ActiveJob queue adapters:</p> <pre><code># config/application.rb or initializer\nconfig.active_job.queue_adapter = :sidekiq    # Production recommended\n# config.active_job.queue_adapter = :resque   # Alternative production option\n# config.active_job.queue_adapter = :inline   # Development/testing only\n# config.active_job.queue_adapter = :async    # Development with background processing\n</code></pre> <p>Sidekiq Configuration (Recommended for Production): <pre><code># config/sidekiq.yml\n:concurrency: 10\n:queues:\n  - default\n  - ragdoll_processing\n  - ragdoll_embeddings\n\n# Gemfile\ngem 'sidekiq'\ngem 'redis' # Required for Sidekiq\n\n# config/routes.rb (for web UI)\nrequire 'sidekiq/web'\nmount Sidekiq::Web =&gt; '/sidekiq'\n</code></pre></p> <p>Queue-Specific Configuration: <pre><code># Custom job queues for different processing types\nclass ExtractText &lt; ActiveJob::Base\n  queue_as :document_processing\nend\n\nclass GenerateEmbeddings &lt; ActiveJob::Base\n  queue_as :ai_processing  # Separate queue for AI operations\nend\n\n# Environment-specific queue names\nclass ExtractKeywords &lt; ActiveJob::Base\n  queue_as Rails.env.production? ? :production_ai : :development_ai\nend\n</code></pre></p>"},{"location":"api-reference/api-jobs/#retry-policies-and-backoff","title":"Retry Policies and Backoff","text":"<p>Comprehensive retry configuration with exponential backoff:</p> <pre><code>module Ragdoll\n  module Core\n    module Jobs\n      class BaseJob &lt; ActiveJob::Base\n        # Default retry configuration for all Ragdoll jobs\n        retry_on StandardError, wait: :exponentially_longer, attempts: 5\n        retry_on Timeout::Error, wait: 30.seconds, attempts: 3\n        retry_on ActiveRecord::ConnectionNotEstablished, wait: 5.seconds, attempts: 10\n\n        # Specific handling for different error types\n        retry_on Net::TimeoutError, wait: :exponentially_longer, attempts: 3\n        discard_on ActiveRecord::RecordNotFound\n        discard_on ArgumentError\n\n        # Custom retry logic\n        retry_on CustomAPIError do |job, exception|\n          if exception.response_code == 429 # Rate limited\n            job.retry_job(wait: 1.minute)\n          elsif exception.response_code &gt;= 500\n            job.retry_job(wait: :exponentially_longer)\n          else\n            job.discard_job\n          end\n        end\n      end\n    end\n  end\nend\n\n# Job-specific retry policies\nclass GenerateEmbeddings &lt; BaseJob\n  # AI API calls may need different retry behavior\n  retry_on OpenAI::APIError, wait: 2.minutes, attempts: 3\n  retry_on Anthropic::RateLimitError, wait: 5.minutes, attempts: 2\nend\n</code></pre>"},{"location":"api-reference/api-jobs/#timeout-settings","title":"Timeout Settings","text":"<p>Configurable timeouts for different job types:</p> <pre><code># Per-job timeout configuration\nclass ExtractText &lt; ActiveJob::Base\n  # Large PDFs may take time to process\n  queue_with_priority 10\n  timeout 15.minutes\nend\n\nclass GenerateEmbeddings &lt; ActiveJob::Base\n  # AI operations can be slow\n  timeout 30.minutes\nend\n\n# Global timeout configuration\n# config/application.rb\nconfig.active_job.default_timeout = 10.minutes\n\n# Sidekiq-specific timeout\n# config/sidekiq.yml\n:timeout: 1800  # 30 minutes\n</code></pre>"},{"location":"api-reference/api-jobs/#priority-levels","title":"Priority Levels","text":"<p>Job prioritization for optimal resource allocation:</p> <pre><code># High priority for user-facing operations\nclass ExtractText &lt; ActiveJob::Base\n  queue_with_priority 100  # Higher number = higher priority\nend\n\n# Lower priority for background optimization\nclass GenerateEmbeddings &lt; ActiveJob::Base\n  queue_with_priority 50\nend\n\n# Dynamic priority based on content\nclass ProcessDocument\n  def self.enqueue_with_priority(document)\n    priority = case document.document_type\n               when 'urgent' then 100\n               when 'normal' then 50\n               else 10\n               end\n\n    ExtractText.set(priority: priority).perform_later(document.id)\n  end\nend\n\n# Sidekiq priority queues\n# config/sidekiq.yml\n:queues:\n  - [urgent, 10]      # High priority, weight 10\n  - [normal, 5]       # Normal priority, weight 5\n  - [background, 1]   # Low priority, weight 1\n</code></pre>"},{"location":"api-reference/api-jobs/#queue-management","title":"Queue Management","text":"<p>Efficient queue management ensures optimal resource utilization and processing performance across different workload types.</p>"},{"location":"api-reference/api-jobs/#queue-naming-conventions","title":"Queue Naming Conventions","text":"<p>Standardized queue naming for clear operational management:</p> <pre><code># Ragdoll queue naming conventions\nmodule Ragdoll\n  module Core\n    module Jobs\n      # Primary processing queues\n      class ExtractText &lt; ActiveJob::Base\n        queue_as 'ragdoll_extraction'\n      end\n\n      class GenerateEmbeddings &lt; ActiveJob::Base\n        queue_as 'ragdoll_embeddings'\n      end\n\n      class GenerateSummary &lt; ActiveJob::Base\n        queue_as 'ragdoll_ai_processing'\n      end\n\n      class ExtractKeywords &lt; ActiveJob::Base\n        queue_as 'ragdoll_ai_processing'\n      end\n    end\n  end\nend\n\n# Environment-specific queue names\nclass BaseJob &lt; ActiveJob::Base\n  queue_as -&gt;(job) {\n    env_prefix = Rails.env.production? ? 'prod' : Rails.env\n    \"#{env_prefix}_#{job.class.name.demodulize.underscore}\"\n  }\nend\n\n# Queue naming by workload characteristics\nqueue_names = {\n  cpu_intensive: 'ragdoll_cpu_heavy',     # AI processing, text analysis\n  io_intensive: 'ragdoll_io_heavy',       # File processing, network calls\n  memory_intensive: 'ragdoll_memory_heavy', # Large document processing\n  quick_tasks: 'ragdoll_quick',           # Fast operations\n  batch_processing: 'ragdoll_batch'       # Bulk operations\n}\n</code></pre>"},{"location":"api-reference/api-jobs/#worker-configuration","title":"Worker Configuration","text":"<p>Optimized worker configuration for different processing types:</p> <pre><code># Sidekiq worker configuration\n# config/sidekiq.yml\n:concurrency: 20\n:timeout: 3600\n:verbose: false\n\n:queues:\n  - [ragdoll_quick, 10]        # Fast jobs, high concurrency\n  - [ragdoll_extraction, 5]    # File processing\n  - [ragdoll_embeddings, 3]    # AI embeddings (resource intensive)\n  - [ragdoll_ai_processing, 2] # LLM operations (rate limited)\n  - [ragdoll_batch, 1]         # Batch operations (low priority)\n\n# Environment-specific configurations\nproduction:\n  :concurrency: 50\n  :timeout: 7200\n  :queues:\n    - [ragdoll_quick, 20]\n    - [ragdoll_extraction, 15]\n    - [ragdoll_embeddings, 10]\n    - [ragdoll_ai_processing, 5]\n\ndevelopment:\n  :concurrency: 5\n  :timeout: 300\n  :queues:\n    - [ragdoll_quick, 3]\n    - [ragdoll_extraction, 2]\n    - [ragdoll_embeddings, 1]\n</code></pre> <p>Per-Worker Resource Limits: <pre><code># Worker-specific configuration\nclass AIProcessingWorker\n  include Sidekiq::Worker\n\n  sidekiq_options queue: 'ragdoll_ai_processing',\n                  concurrency: 2,        # Limit concurrent AI calls\n                  memory_limit: '2GB',   # Memory constraint\n                  timeout: 1800         # 30 minute timeout\nend\n\n# Resource-aware job assignment\nclass ResourceManager\n  def self.optimal_queue_for_job(job_type, content_size)\n    case job_type\n    when :embedding_generation\n      content_size &gt; 10_000 ? 'ragdoll_embeddings_large' : 'ragdoll_embeddings'\n    when :text_extraction\n      content_size &gt; 50_000 ? 'ragdoll_extraction_large' : 'ragdoll_extraction'\n    else\n      'ragdoll_default'\n    end\n  end\nend\n</code></pre></p>"},{"location":"api-reference/api-jobs/#scaling-strategies","title":"Scaling Strategies","text":"<p>Horizontal and vertical scaling approaches for different workloads:</p> <pre><code># Auto-scaling configuration\nclass AutoScaler\n  def self.scale_workers_based_on_queue_depth\n    queue_depths = Sidekiq::Queue.all.map { |q| [q.name, q.size] }.to_h\n\n    scaling_rules = {\n      'ragdoll_embeddings' =&gt; { threshold: 50, max_workers: 10 },\n      'ragdoll_extraction' =&gt; { threshold: 100, max_workers: 20 },\n      'ragdoll_ai_processing' =&gt; { threshold: 20, max_workers: 5 }\n    }\n\n    scaling_rules.each do |queue_name, config|\n      current_depth = queue_depths[queue_name] || 0\n\n      if current_depth &gt; config[:threshold]\n        scale_up_workers(queue_name, config[:max_workers])\n      elsif current_depth &lt; config[:threshold] / 4\n        scale_down_workers(queue_name)\n      end\n    end\n  end\n\n  private\n\n  def self.scale_up_workers(queue_name, max_workers)\n    # Kubernetes/Docker scaling logic\n    current_workers = get_current_worker_count(queue_name)\n    return if current_workers &gt;= max_workers\n\n    target_workers = [current_workers + 2, max_workers].min\n    execute_scaling_command(queue_name, target_workers)\n  end\nend\n\n# Kubernetes horizontal pod autoscaler configuration\n# k8s/ragdoll-workers-hpa.yml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: ragdoll-workers\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: ragdoll-sidekiq-workers\n  minReplicas: 2\n  maxReplicas: 20\n  metrics:\n  - type: Pods\n    pods:\n      metric:\n        name: sidekiq_queue_depth\n      target:\n        type: AverageValue\n        averageValue: \"10\"\n</code></pre>"},{"location":"api-reference/api-jobs/#load-balancing","title":"Load Balancing","text":"<p>Intelligent load distribution across workers and queues:</p> <pre><code># Queue load balancing\nclass LoadBalancer\n  def self.distribute_job(job_class, *args)\n    # Find least loaded queue for job type\n    available_queues = queues_for_job_type(job_class)\n    optimal_queue = find_least_loaded_queue(available_queues)\n\n    job_class.set(queue: optimal_queue).perform_later(*args)\n  end\n\n  def self.queues_for_job_type(job_class)\n    case job_class.name\n    when /Extract/\n      %w[ragdoll_extraction_1 ragdoll_extraction_2 ragdoll_extraction_3]\n    when /Generate.*Embedding/\n      %w[ragdoll_embeddings_1 ragdoll_embeddings_2]\n    when /AI|Summary|Keywords/\n      %w[ragdoll_ai_1 ragdoll_ai_2]\n    else\n      %w[ragdoll_default]\n    end\n  end\n\n  def self.find_least_loaded_queue(queue_names)\n    queue_loads = queue_names.map do |name|\n      queue = Sidekiq::Queue.new(name)\n      [name, queue.size + active_jobs_count(name)]\n    end\n\n    queue_loads.min_by { |_, load| load }.first\n  end\n\n  private\n\n  def self.active_jobs_count(queue_name)\n    Sidekiq::Workers.new.count { |_, _, work| work['queue'] == queue_name }\n  end\nend\n\n# Geographic load balancing for distributed deployments\nclass GeographicLoadBalancer\n  REGIONS = {\n    'us-east-1' =&gt; %w[ragdoll_us_east_1 ragdoll_us_east_2],\n    'us-west-2' =&gt; %w[ragdoll_us_west_1 ragdoll_us_west_2],\n    'eu-west-1' =&gt; %w[ragdoll_eu_west_1 ragdoll_eu_west_2]\n  }\n\n  def self.route_job_by_region(job_class, region, *args)\n    regional_queues = REGIONS[region] || REGIONS['us-east-1']\n    optimal_queue = LoadBalancer.find_least_loaded_queue(regional_queues)\n\n    job_class.set(queue: optimal_queue).perform_later(*args)\n  end\nend\n\n# Usage examples\n# Route to least loaded extraction queue\nLoadBalancer.distribute_job(Ragdoll::ExtractTextJob, document.id)\n\n# Route by geographic region\nGeographicLoadBalancer.route_job_by_region(\n  Ragdoll::GenerateEmbeddingsJob,\n  'us-west-2',\n  document.id\n)\n</code></pre>"},{"location":"api-reference/api-jobs/#error-handling","title":"Error Handling","text":"<p>Robust error handling ensures reliable document processing and provides comprehensive failure recovery mechanisms.</p>"},{"location":"api-reference/api-jobs/#retry-strategies","title":"Retry Strategies","text":"<p>Multi-layered retry logic with intelligent backoff and error classification:</p> <pre><code>module Ragdoll\n  module Core\n    module Jobs\n      class BaseJob &lt; ActiveJob::Base\n        # Comprehensive retry configuration\n        retry_on StandardError, wait: :exponentially_longer, attempts: 5 do |job, exception|\n          # Log retry attempt\n          Rails.logger.warn \"Retrying #{job.class.name} (attempt #{job.executions}): #{exception.message}\" if defined?(Rails)\n\n          # Update document status if applicable\n          if job.respond_to?(:document_id) &amp;&amp; job.arguments.first\n            document = Ragdoll::Document.find_by(id: job.arguments.first)\n            document&amp;.update(status: 'processing') # Keep as processing during retries\n          end\n        end\n\n        # Specific retry policies for different error types\n        retry_on Net::TimeoutError, wait: 30.seconds, attempts: 3\n        retry_on ActiveRecord::ConnectionNotEstablished, wait: 5.seconds, attempts: 10\n        retry_on Errno::ECONNREFUSED, wait: 1.minute, attempts: 5\n\n        # AI API specific retries\n        retry_on OpenAI::RateLimitError, wait: 2.minutes, attempts: 3\n        retry_on OpenAI::APIConnectionError, wait: :exponentially_longer, attempts: 5\n\n        # Immediate failures - don't retry\n        discard_on ActiveRecord::RecordNotFound\n        discard_on ArgumentError\n        discard_on JSON::ParserError\n\n        # Custom retry logic for specific scenarios\n        retry_on CustomProcessingError do |job, exception|\n          case exception.error_code\n          when 'file_corrupted'\n            job.discard_job # Don't retry corrupted files\n          when 'temporary_unavailable'\n            job.retry_job(wait: 5.minutes)\n          when 'rate_limited'\n            job.retry_job(wait: exception.retry_after || 60.seconds)\n          else\n            job.retry_job(wait: :exponentially_longer)\n          end\n        end\n\n        # Final failure handling\n        discard_on StandardError do |job, exception|\n          handle_final_failure(job, exception)\n        end\n\n        private\n\n        def self.handle_final_failure(job, exception)\n          # Mark document as failed if applicable\n          if job.respond_to?(:document_id) &amp;&amp; job.arguments.first\n            document = Ragdoll::Document.find_by(id: job.arguments.first)\n            if document\n              document.update!(\n                status: 'error',\n                metadata: document.metadata.merge(\n                  'error' =&gt; {\n                    'message' =&gt; exception.message,\n                    'backtrace' =&gt; exception.backtrace&amp;.first(5),\n                    'job_class' =&gt; job.class.name,\n                    'failed_at' =&gt; Time.current.iso8601,\n                    'attempts' =&gt; job.executions\n                  }\n                )\n              )\n            end\n          end\n\n          # Send failure notification\n          ErrorNotifier.notify_job_failure(job, exception)\n        end\n      end\n    end\n  end\nend\n\n# Job-specific retry strategies\nclass ExtractText &lt; BaseJob\n  # File processing may have different retry needs\n  retry_on Errno::ENOENT, attempts: 1 # File not found - don't retry\n  retry_on PDF::Reader::MalformedPDFError, attempts: 2, wait: 10.seconds\n  retry_on Docx::DocumentCorrupted, attempts: 1 # Don't retry corrupted docs\nend\n\nclass GenerateEmbeddings &lt; BaseJob\n  # AI operations need careful retry handling\n  retry_on OpenAI::APIError do |job, exception|\n    if exception.response&amp;.dig('error', 'code') == 'context_length_exceeded'\n      # Try with smaller chunks\n      job.arguments[1] = (job.arguments[1] || 1000) * 0.8 # Reduce chunk size\n      job.retry_job(wait: 1.minute) if job.executions &lt; 3\n    else\n      job.retry_job(wait: :exponentially_longer)\n    end\n  end\nend\n</code></pre>"},{"location":"api-reference/api-jobs/#dead-letter-queues","title":"Dead Letter Queues","text":"<p>Manage permanently failed jobs with comprehensive tracking:</p> <pre><code># Dead letter queue implementation\nmodule Ragdoll\n  module Core\n    class DeadLetterQueue\n      def self.store_failed_job(job, exception)\n        failed_job_record = {\n          id: SecureRandom.uuid,\n          job_class: job.class.name,\n          job_arguments: job.arguments,\n          queue_name: job.queue_name,\n          failed_at: Time.current,\n          exception_class: exception.class.name,\n          exception_message: exception.message,\n          exception_backtrace: exception.backtrace,\n          job_executions: job.executions,\n          enqueued_at: job.enqueued_at,\n          created_at: Time.current\n        }\n\n        # Store in database table or external storage\n        store_failed_job_record(failed_job_record)\n\n        # Optional: Store in Redis for quick access\n        Redis.current.lpush('ragdoll:dead_letter_queue', failed_job_record.to_json)\n        Redis.current.expire('ragdoll:dead_letter_queue', 30.days)\n      end\n\n      def self.replay_failed_job(failed_job_id)\n        failed_job = get_failed_job_record(failed_job_id)\n        return false unless failed_job\n\n        job_class = failed_job[:job_class].constantize\n        job_class.perform_later(*failed_job[:job_arguments])\n\n        # Mark as replayed\n        mark_job_replayed(failed_job_id)\n        true\n      rescue =&gt; e\n        Rails.logger.error \"Failed to replay job #{failed_job_id}: #{e.message}\" if defined?(Rails)\n        false\n      end\n\n      def self.failed_jobs_summary\n        # Get failed jobs statistics\n        {\n          total_failed: count_failed_jobs,\n          by_job_class: failed_jobs_by_class,\n          by_error_type: failed_jobs_by_error,\n          recent_failures: recent_failed_jobs(24.hours),\n          retry_candidates: jobs_eligible_for_retry\n        }\n      end\n\n      private\n\n      def self.store_failed_job_record(record)\n        # Implementation depends on storage choice\n        # Could be database table, file system, or external service\n        Rails.logger.error \"FAILED JOB: #{record.to_json}\" if defined?(Rails)\n      end\n    end\n  end\nend\n\n# Sidekiq dead job handling\nclass SidekiqDeadJobHandler\n  def self.process_dead_jobs\n    dead_set = Sidekiq::DeadSet.new\n\n    dead_set.each do |job|\n      if job['class'].start_with?('Ragdoll::')\n        # Extract Ragdoll-specific failed jobs\n        failed_job_data = {\n          job_class: job['class'],\n          arguments: job['args'],\n          error_message: job['error_message'],\n          failed_at: Time.at(job['failed_at']),\n          retry_count: job['retry_count']\n        }\n\n        Ragdoll::Core::DeadLetterQueue.store_failed_job_data(failed_job_data)\n      end\n    end\n  end\n\n  def self.clear_old_dead_jobs(older_than: 30.days)\n    dead_set = Sidekiq::DeadSet.new\n    cutoff_time = older_than.ago\n\n    dead_set.select { |job| Time.at(job['failed_at']) &lt; cutoff_time }.each(&amp;:delete)\n  end\nend\n</code></pre>"},{"location":"api-reference/api-jobs/#error-notification","title":"Error Notification","text":"<p>Comprehensive error notification system:</p> <pre><code>module Ragdoll\n  module Core\n    class ErrorNotifier\n      def self.notify_job_failure(job, exception)\n        notification_data = {\n          job_class: job.class.name,\n          job_id: job.job_id,\n          arguments: sanitize_arguments(job.arguments),\n          exception: {\n            class: exception.class.name,\n            message: exception.message,\n            backtrace: exception.backtrace&amp;.first(10)\n          },\n          context: {\n            queue: job.queue_name,\n            executions: job.executions,\n            enqueued_at: job.enqueued_at,\n            failed_at: Time.current\n          },\n          environment: Rails.env,\n          severity: determine_severity(job, exception)\n        }\n\n        # Send to multiple notification channels\n        send_to_configured_channels(notification_data)\n      end\n\n      def self.notify_queue_backlog(queue_name, depth)\n        return unless depth &gt; backlog_threshold(queue_name)\n\n        notification = {\n          type: 'queue_backlog',\n          queue: queue_name,\n          depth: depth,\n          threshold: backlog_threshold(queue_name),\n          severity: depth &gt; (backlog_threshold(queue_name) * 2) ? 'critical' : 'warning'\n        }\n\n        send_to_configured_channels(notification)\n      end\n\n      private\n\n      def self.send_to_configured_channels(data)\n        channels = Ragdoll.config&amp;.notification_channels || [:log]\n\n        channels.each do |channel|\n          case channel\n          when :log\n            log_error(data)\n          when :email\n            send_email_notification(data)\n          when :slack\n            send_slack_notification(data)\n          when :webhook\n            send_webhook_notification(data)\n          when :sentry\n            send_sentry_notification(data)\n          end\n        end\n      end\n\n      def self.log_error(data)\n        logger = defined?(Rails) ? Rails.logger : Logger.new(STDOUT)\n        logger.error \"[RAGDOLL JOB FAILURE] #{data[:job_class]}: #{data[:exception][:message]}\"\n      end\n\n      def self.send_slack_notification(data)\n        return unless ENV['SLACK_WEBHOOK_URL']\n\n        payload = {\n          text: \"Ragdoll Job Failure: #{data[:job_class]}\",\n          attachments: [{\n            color: data[:severity] == 'critical' ? 'danger' : 'warning',\n            fields: [\n              { title: 'Job Class', value: data[:job_class], short: true },\n              { title: 'Queue', value: data[:context][:queue], short: true },\n              { title: 'Error', value: data[:exception][:message], short: false },\n              { title: 'Attempts', value: data[:context][:executions], short: true }\n            ],\n            timestamp: Time.current.to_i\n          }]\n        }\n\n        # Send webhook (implementation depends on HTTP library)\n        send_webhook(ENV['SLACK_WEBHOOK_URL'], payload)\n      end\n\n      def self.determine_severity(job, exception)\n        case exception\n        when ActiveRecord::RecordNotFound, ArgumentError\n          'low'      # Expected failures\n        when Net::TimeoutError, Errno::ECONNREFUSED\n          'medium'   # Infrastructure issues\n        when OpenAI::RateLimitError\n          'medium'   # External service limits\n        else\n          job.executions &gt;= 3 ? 'high' : 'medium'\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"api-reference/api-jobs/#debugging-approaches","title":"Debugging Approaches","text":"<p>Comprehensive debugging tools and techniques:</p> <pre><code># Job debugging utilities\nmodule Ragdoll\n  module Core\n    module JobDebugging\n      def self.debug_failed_document(document_id)\n        document = Ragdoll::Document.find(document_id)\n\n        debug_info = {\n          document: {\n            id: document.id,\n            status: document.status,\n            document_type: document.document_type,\n            location: document.location,\n            content_present: document.content.present?,\n            content_length: document.content&amp;.length,\n            metadata: document.metadata\n          },\n          processing_history: get_processing_history(document_id),\n          related_jobs: get_related_jobs(document_id),\n          system_state: get_system_state_at_failure(document_id)\n        }\n\n        puts JSON.pretty_generate(debug_info)\n        debug_info\n      end\n\n      def self.trace_job_execution(job_class, *args)\n        # Enable detailed logging for specific job execution\n        original_log_level = Rails.logger.level if defined?(Rails)\n\n        begin\n          Rails.logger.level = Logger::DEBUG if defined?(Rails)\n\n          # Add execution tracing\n          job_instance = job_class.new(*args)\n\n          puts \"=== Starting #{job_class.name} execution ===\"\n          puts \"Arguments: #{args.inspect}\"\n          puts \"Queue: #{job_instance.queue_name}\"\n\n          start_time = Time.current\n          result = job_instance.perform_now\n          end_time = Time.current\n\n          puts \"=== Completed in #{((end_time - start_time) * 1000).round(2)}ms ===\"\n          puts \"Result: #{result.inspect}\"\n\n          result\n        rescue =&gt; e\n          puts \"=== FAILED with #{e.class.name}: #{e.message} ===\"\n          puts \"Backtrace:\"\n          puts e.backtrace.first(10).map { |line| \"  #{line}\" }\n          raise e\n        ensure\n          Rails.logger.level = original_log_level if defined?(Rails) &amp;&amp; original_log_level\n        end\n      end\n\n      def self.analyze_queue_performance(queue_name, period: 24.hours)\n        # Analyze job performance in specific queue\n        stats = {\n          queue_name: queue_name,\n          period: period,\n          current_depth: Sidekiq::Queue.new(queue_name).size,\n          processed_jobs: get_processed_jobs_count(queue_name, period),\n          failed_jobs: get_failed_jobs_count(queue_name, period),\n          avg_processing_time: calculate_avg_processing_time(queue_name, period),\n          slowest_jobs: get_slowest_jobs(queue_name, period, limit: 5)\n        }\n\n        puts \"=== Queue Performance Analysis: #{queue_name} ===\"\n        puts JSON.pretty_generate(stats)\n        stats\n      end\n\n      private\n\n      def self.get_processing_history(document_id)\n        # Get timeline of document processing attempts\n        # This would typically come from application logs or database\n        [\n          # Mock data structure\n          {\n            timestamp: 1.hour.ago,\n            event: 'ExtractText queued',\n            details: { queue: 'ragdoll_extraction' }\n          },\n          {\n            timestamp: 55.minutes.ago,\n            event: 'ExtractText started',\n            details: { worker_id: 'worker-1' }\n          },\n          {\n            timestamp: 50.minutes.ago,\n            event: 'ExtractText failed',\n            details: { error: 'PDF parsing error', attempt: 1 }\n          }\n        ]\n      end\n    end\n  end\nend\n\n# Usage examples\n# Debug a specific failed document\nRagdoll::Core::JobDebugging.debug_failed_document(123)\n\n# Trace a job execution with detailed logging\nRagdoll::Core::JobDebugging.trace_job_execution(\n  Ragdoll::ExtractTextJob,\n  document.id\n)\n\n# Analyze queue performance\nRagdoll::Core::JobDebugging.analyze_queue_performance(\n  'ragdoll_embeddings',\n  period: 7.days\n)\n</code></pre>"},{"location":"api-reference/api-jobs/#monitoring","title":"Monitoring","text":"<p>Comprehensive monitoring and observability for job performance, queue health, and system reliability.</p>"},{"location":"api-reference/api-jobs/#job-status-tracking","title":"Job Status Tracking","text":"<p>Real-time tracking of job execution status through document lifecycle:</p> <pre><code>module Ragdoll\n  module Core\n    class JobStatusTracker\n      def self.document_processing_status(document_id)\n        document = Ragdoll::Document.find(document_id)\n\n        {\n          document_id: document.id,\n          current_status: document.status,\n          processing_stages: {\n            text_extraction: check_text_extraction_status(document),\n            embedding_generation: check_embedding_status(document),\n            summary_generation: check_summary_status(document),\n            keyword_extraction: check_keyword_status(document)\n          },\n          queue_positions: get_queue_positions(document_id),\n          estimated_completion: estimate_completion_time(document),\n          processing_metrics: get_processing_metrics(document)\n        }\n      end\n\n      def self.system_processing_overview\n        {\n          total_documents: Ragdoll::Document.count,\n          status_distribution: Ragdoll::Document.group(:status).count,\n          processing_pipeline: {\n            pending_extraction: Ragdoll::Document.where(status: 'pending').count,\n            currently_processing: Ragdoll::Document.where(status: 'processing').count,\n            completed_today: Ragdoll::Document\n              .where(status: 'processed')\n              .where('updated_at &gt; ?', Date.current)\n              .count,\n            failed_processing: Ragdoll::Document.where(status: 'error').count\n          },\n          queue_depths: get_all_queue_depths,\n          active_workers: get_active_worker_count\n        }\n      end\n\n      private\n\n      def self.check_text_extraction_status(document)\n        {\n          completed: document.content.present?,\n          content_length: document.content&amp;.length || 0,\n          extraction_method: determine_extraction_method(document)\n        }\n      end\n\n      def self.check_embedding_status(document)\n        embeddings_count = document.total_embedding_count\n        {\n          completed: embeddings_count &gt; 0,\n          embeddings_count: embeddings_count,\n          by_content_type: document.embeddings_by_type\n        }\n      end\n\n      def self.get_queue_positions(document_id)\n        # Check position in various queues\n        positions = {}\n\n        %w[ragdoll_extraction ragdoll_embeddings ragdoll_ai_processing].each do |queue_name|\n          queue = Sidekiq::Queue.new(queue_name)\n          position = queue.find_job { |job| job.args.first == document_id }\n          positions[queue_name] = position ? queue.to_a.index(position) + 1 : nil\n        end\n\n        positions\n      end\n    end\n  end\nend\n\n# Real-time status updates using ActionCable (if in Rails app)\nclass JobStatusChannel &lt; ApplicationCable::Channel\n  def subscribed\n    stream_from \"job_status_#{params[:document_id]}\"\n  end\n\n  def self.broadcast_status_update(document_id, status_data)\n    ActionCable.server.broadcast(\n      \"job_status_#{document_id}\",\n      status_data\n    )\n  end\nend\n\n# Update status from jobs\n# In job classes:\nafter_perform do |job|\n  if job.arguments.first # document_id\n    status_data = JobStatusTracker.document_processing_status(job.arguments.first)\n    JobStatusChannel.broadcast_status_update(job.arguments.first, status_data)\n  end\nend\n</code></pre>"},{"location":"api-reference/api-jobs/#performance-metrics","title":"Performance Metrics","text":"<p>Detailed performance tracking and analysis:</p> <pre><code>class JobPerformanceMetrics\n  def self.collect_performance_data\n    {\n      timestamp: Time.current.iso8601,\n      processing_times: calculate_processing_times,\n      throughput_metrics: calculate_throughput,\n      resource_utilization: measure_resource_usage,\n      error_rates: calculate_error_rates,\n      queue_performance: analyze_queue_performance\n    }\n  end\n\n  def self.calculate_processing_times\n    recent_docs = Ragdoll::Document\n      .where('updated_at &gt; ?', 24.hours.ago)\n      .where(status: 'processed')\n      .includes(:contents)\n\n    processing_times = recent_docs.map do |doc|\n      total_time = (doc.updated_at - doc.created_at).to_i\n      {\n        document_id: doc.id,\n        document_type: doc.document_type,\n        content_size: doc.content&amp;.length || 0,\n        processing_time_seconds: total_time,\n        embedding_count: doc.total_embedding_count\n      }\n    end\n\n    {\n      individual_times: processing_times,\n      averages: {\n        overall: processing_times.map { |t| t[:processing_time_seconds] }.sum / processing_times.length,\n        by_document_type: processing_times.group_by { |t| t[:document_type] }\n                                         .transform_values { |times| \n                                           times.map { |t| t[:processing_time_seconds] }.sum / times.length \n                                         },\n        by_content_size: calculate_time_by_content_size(processing_times)\n      }\n    }\n  end\n\n  def self.calculate_throughput\n    time_periods = {\n      last_hour: 1.hour.ago,\n      last_24_hours: 24.hours.ago,\n      last_week: 1.week.ago\n    }\n\n    throughput_data = {}\n\n    time_periods.each do |period, start_time|\n      processed_count = Ragdoll::Document\n        .where('updated_at &gt; ?', start_time)\n        .where(status: 'processed')\n        .count\n\n      time_span_hours = (Time.current - start_time) / 1.hour\n\n      throughput_data[period] = {\n        documents_processed: processed_count,\n        documents_per_hour: processed_count / time_span_hours,\n        embeddings_generated: calculate_embeddings_in_period(start_time),\n        summaries_created: Ragdoll::Document\n          .where('updated_at &gt; ?', start_time)\n          .where.not(summary: [nil, ''])\n          .count\n      }\n    end\n\n    throughput_data\n  end\n\n  def self.measure_resource_usage\n    {\n      memory: {\n        current_mb: `ps -o rss= -p #{Process.pid}`.to_i / 1024,\n        gc_stats: GC.stat\n      },\n      database: {\n        active_connections: ActiveRecord::Base.connection_pool.stat[:checked_out],\n        pool_size: ActiveRecord::Base.connection_pool.stat[:size],\n        connection_usage_percent: (\n          ActiveRecord::Base.connection_pool.stat[:checked_out].to_f / \n          ActiveRecord::Base.connection_pool.stat[:size] * 100\n        ).round(2)\n      },\n      redis: redis_memory_usage,\n      sidekiq: {\n        busy_workers: Sidekiq::Workers.new.size,\n        total_processed: Sidekiq::Stats.new.processed,\n        total_failed: Sidekiq::Stats.new.failed\n      }\n    }\n  end\n\n  def self.calculate_error_rates\n    time_periods = [1.hour.ago, 24.hours.ago, 1.week.ago]\n\n    error_rates = {}\n\n    time_periods.each do |start_time|\n      period_name = case start_time\n                    when 1.hour.ago then :last_hour\n                    when 24.hours.ago then :last_24_hours\n                    else :last_week\n                    end\n\n      total_attempts = Ragdoll::Document.where('created_at &gt; ?', start_time).count\n      failed_attempts = Ragdoll::Document\n        .where('created_at &gt; ?', start_time)\n        .where(status: 'error')\n        .count\n\n      error_rates[period_name] = {\n        total_attempts: total_attempts,\n        failed_attempts: failed_attempts,\n        error_rate_percent: total_attempts &gt; 0 ? (failed_attempts.to_f / total_attempts * 100).round(2) : 0,\n        error_types: get_error_types_in_period(start_time)\n      }\n    end\n\n    error_rates\n  end\n\n  private\n\n  def self.redis_memory_usage\n    return {} unless defined?(Redis)\n\n    redis_info = Redis.current.info\n    {\n      used_memory_mb: redis_info['used_memory'].to_i / (1024 * 1024),\n      connected_clients: redis_info['connected_clients'].to_i,\n      total_commands_processed: redis_info['total_commands_processed'].to_i\n    }\n  rescue\n    { error: 'Redis connection unavailable' }\n  end\nend\n\n# Automated performance reporting\nclass PerformanceReportJob &lt; ActiveJob::Base\n  queue_as :default\n\n  def perform\n    metrics = JobPerformanceMetrics.collect_performance_data\n\n    # Store metrics for historical analysis\n    store_performance_metrics(metrics)\n\n    # Send alerts if performance degrades\n    check_performance_alerts(metrics)\n\n    # Generate daily/weekly reports\n    generate_performance_report(metrics) if should_generate_report?\n  end\n\n  private\n\n  def check_performance_alerts(metrics)\n    alerts = []\n\n    # Check processing time alerts\n    avg_processing_time = metrics[:processing_times][:averages][:overall]\n    if avg_processing_time &gt; 300 # 5 minutes\n      alerts &lt;&lt; {\n        type: :slow_processing,\n        severity: avg_processing_time &gt; 600 ? :critical : :warning,\n        message: \"Average processing time: #{avg_processing_time}s\"\n      }\n    end\n\n    # Check error rate alerts\n    error_rate = metrics[:error_rates][:last_hour][:error_rate_percent]\n    if error_rate &gt; 5\n      alerts &lt;&lt; {\n        type: :high_error_rate,\n        severity: error_rate &gt; 15 ? :critical : :warning,\n        message: \"Error rate in last hour: #{error_rate}%\"\n      }\n    end\n\n    # Send alerts if any found\n    alerts.each { |alert| send_performance_alert(alert) }\n  end\nend\n</code></pre>"},{"location":"api-reference/api-jobs/#queue-health-monitoring","title":"Queue Health Monitoring","text":"<p>Comprehensive queue monitoring and health checks:</p> <pre><code>class QueueHealthMonitor\n  HEALTH_THRESHOLDS = {\n    queue_depth: {\n      warning: 50,\n      critical: 200\n    },\n    processing_rate: {\n      warning: 10,  # jobs per minute\n      critical: 5\n    },\n    worker_utilization: {\n      warning: 80,  # percentage\n      critical: 95\n    }\n  }\n\n  def self.check_all_queues\n    queue_names = %w[\n      ragdoll_extraction\n      ragdoll_embeddings\n      ragdoll_ai_processing\n      ragdoll_quick\n    ]\n\n    health_report = {\n      timestamp: Time.current.iso8601,\n      overall_health: 'healthy',\n      queues: {},\n      alerts: []\n    }\n\n    queue_names.each do |queue_name|\n      queue_health = analyze_queue_health(queue_name)\n      health_report[:queues][queue_name] = queue_health\n\n      # Collect alerts\n      if queue_health[:alerts].any?\n        health_report[:alerts].concat(queue_health[:alerts])\n      end\n\n      # Update overall health\n      if queue_health[:status] == 'critical'\n        health_report[:overall_health] = 'critical'\n      elsif queue_health[:status] == 'warning' &amp;&amp; health_report[:overall_health] == 'healthy'\n        health_report[:overall_health] = 'warning'\n      end\n    end\n\n    health_report\n  end\n\n  def self.analyze_queue_health(queue_name)\n    queue = Sidekiq::Queue.new(queue_name)\n\n    health_data = {\n      name: queue_name,\n      depth: queue.size,\n      latency: queue.latency,\n      processing_rate: calculate_processing_rate(queue_name),\n      worker_utilization: calculate_worker_utilization(queue_name),\n      oldest_job_age: calculate_oldest_job_age(queue),\n      status: 'healthy',\n      alerts: []\n    }\n\n    # Check depth threshold\n    if health_data[:depth] &gt;= HEALTH_THRESHOLDS[:queue_depth][:critical]\n      health_data[:status] = 'critical'\n      health_data[:alerts] &lt;&lt; {\n        type: :queue_depth,\n        severity: :critical,\n        message: \"Queue depth #{health_data[:depth]} exceeds critical threshold\"\n      }\n    elsif health_data[:depth] &gt;= HEALTH_THRESHOLDS[:queue_depth][:warning]\n      health_data[:status] = 'warning'\n      health_data[:alerts] &lt;&lt; {\n        type: :queue_depth,\n        severity: :warning,\n        message: \"Queue depth #{health_data[:depth]} exceeds warning threshold\"\n      }\n    end\n\n    # Check processing rate\n    if health_data[:processing_rate] &lt;= HEALTH_THRESHOLDS[:processing_rate][:critical]\n      health_data[:status] = 'critical'\n      health_data[:alerts] &lt;&lt; {\n        type: :slow_processing,\n        severity: :critical,\n        message: \"Processing rate #{health_data[:processing_rate]} jobs/min is critically low\"\n      }\n    elsif health_data[:processing_rate] &lt;= HEALTH_THRESHOLDS[:processing_rate][:warning]\n      health_data[:status] = 'warning' if health_data[:status] == 'healthy'\n      health_data[:alerts] &lt;&lt; {\n        type: :slow_processing,\n        severity: :warning,\n        message: \"Processing rate #{health_data[:processing_rate]} jobs/min is below normal\"\n      }\n    end\n\n    health_data\n  end\n\n  def self.queue_trending_analysis(period: 24.hours)\n    # Analyze queue depth trends over time\n    queue_names = %w[ragdoll_extraction ragdoll_embeddings ragdoll_ai_processing]\n\n    trending_data = {}\n\n    queue_names.each do |queue_name|\n      # This would typically come from stored metrics\n      # For now, simulate trend analysis\n      trending_data[queue_name] = {\n        current_depth: Sidekiq::Queue.new(queue_name).size,\n        trend_direction: calculate_trend_direction(queue_name, period),\n        peak_depth_24h: get_peak_depth(queue_name, period),\n        average_depth_24h: get_average_depth(queue_name, period),\n        prediction: predict_queue_behavior(queue_name)\n      }\n    end\n\n    trending_data\n  end\n\n  private\n\n  def self.calculate_processing_rate(queue_name)\n    # Calculate jobs processed per minute for this queue\n    # This would come from Sidekiq stats or custom metrics\n    processed_count = get_processed_jobs_last_hour(queue_name)\n    processed_count / 60.0 # per minute\n  end\n\n  def self.calculate_worker_utilization(queue_name)\n    # Calculate percentage of workers busy with this queue\n    total_workers = Sidekiq::ProcessSet.new.sum(&amp;:busy)\n    queue_workers = count_workers_processing_queue(queue_name)\n\n    return 0 if total_workers == 0\n    (queue_workers.to_f / total_workers * 100).round(2)\n  end\nend\n\n# Health monitoring job\nclass QueueHealthCheckJob &lt; ActiveJob::Base\n  queue_as :default\n\n  def perform\n    health_report = QueueHealthMonitor.check_all_queues\n\n    # Store health metrics\n    store_health_report(health_report)\n\n    # Send alerts for critical issues\n    critical_alerts = health_report[:alerts].select { |a| a[:severity] == :critical }\n    critical_alerts.each { |alert| send_critical_alert(alert) }\n\n    # Log health status\n    Rails.logger.info \"Queue Health: #{health_report[:overall_health]}\" if defined?(Rails)\n\n    health_report\n  end\nend\n</code></pre>"},{"location":"api-reference/api-jobs/#alerting-strategies","title":"Alerting Strategies","text":"<p>Intelligent alerting with escalation and noise reduction:</p> <pre><code>class JobAlertingSystem\n  ALERT_RULES = {\n    queue_backlog: {\n      thresholds: { warning: 50, critical: 200 },\n      escalation_time: 15.minutes,\n      channels: [:log, :slack, :email]\n    },\n    high_error_rate: {\n      thresholds: { warning: 5, critical: 15 }, # percentage\n      escalation_time: 5.minutes,\n      channels: [:log, :slack, :webhook]\n    },\n    slow_processing: {\n      thresholds: { warning: 300, critical: 600 }, # seconds\n      escalation_time: 30.minutes,\n      channels: [:log, :email]\n    },\n    worker_shortage: {\n      thresholds: { warning: 90, critical: 100 }, # utilization percentage\n      escalation_time: 10.minutes,\n      channels: [:log, :slack, :pager]\n    }\n  }\n\n  def self.evaluate_alerts\n    current_metrics = collect_current_metrics\n    active_alerts = []\n\n    ALERT_RULES.each do |alert_type, config|\n      alert_value = extract_metric_value(current_metrics, alert_type)\n\n      severity = determine_severity(alert_value, config[:thresholds])\n\n      if severity\n        alert = create_alert(alert_type, severity, alert_value, config)\n        active_alerts &lt;&lt; alert\n\n        # Check if this is a new alert or escalation\n        if should_send_alert?(alert)\n          send_alert(alert, config[:channels])\n        end\n      end\n    end\n\n    # Update alert history\n    update_alert_history(active_alerts)\n\n    active_alerts\n  end\n\n  def self.create_alert(alert_type, severity, current_value, config)\n    {\n      id: generate_alert_id(alert_type, severity),\n      type: alert_type,\n      severity: severity,\n      current_value: current_value,\n      threshold: config[:thresholds][severity],\n      message: generate_alert_message(alert_type, severity, current_value),\n      timestamp: Time.current,\n      escalation_time: config[:escalation_time],\n      channels: config[:channels]\n    }\n  end\n\n  def self.should_send_alert?(alert)\n    # Prevent alert spam by checking recent alert history\n    recent_similar = get_recent_alerts(alert[:type], alert[:severity], 30.minutes)\n\n    # Send if no recent similar alerts\n    return true if recent_similar.empty?\n\n    # Send if value has significantly worsened\n    last_alert = recent_similar.last\n    return true if alert[:current_value] &gt; (last_alert[:current_value] * 1.5)\n\n    # Send if escalation time has passed\n    time_since_last = Time.current - last_alert[:timestamp]\n    return true if time_since_last &gt;= alert[:escalation_time]\n\n    false\n  end\n\n  def self.generate_alert_message(alert_type, severity, current_value)\n    case alert_type\n    when :queue_backlog\n      \"Queue backlog #{severity}: #{current_value} jobs waiting\"\n    when :high_error_rate\n      \"High error rate #{severity}: #{current_value}% of jobs failing\"\n    when :slow_processing\n      \"Slow processing #{severity}: Average #{current_value}s per job\"\n    when :worker_shortage\n      \"Worker shortage #{severity}: #{current_value}% utilization\"\n    else\n      \"System alert #{severity}: #{alert_type} = #{current_value}\"\n    end\n  end\n\n  def self.send_alert(alert, channels)\n    channels.each do |channel|\n      case channel\n      when :log\n        log_alert(alert)\n      when :slack\n        send_slack_alert(alert)\n      when :email\n        send_email_alert(alert)\n      when :webhook\n        send_webhook_alert(alert)\n      when :pager\n        send_pager_alert(alert) if alert[:severity] == :critical\n      end\n    end\n  end\n\n  private\n\n  def self.collect_current_metrics\n    {\n      queue_depths: get_all_queue_depths,\n      error_rates: calculate_current_error_rates,\n      processing_times: get_recent_processing_times,\n      worker_utilization: calculate_worker_utilization\n    }\n  end\nend\n\n# Scheduled alert evaluation\nclass AlertEvaluationJob &lt; ActiveJob::Base\n  queue_as :default\n\n  def perform\n    alerts = JobAlertingSystem.evaluate_alerts\n\n    # Log alert summary\n    if alerts.any?\n      critical_count = alerts.count { |a| a[:severity] == :critical }\n      warning_count = alerts.count { |a| a[:severity] == :warning }\n\n      Rails.logger.info \"Active alerts: #{critical_count} critical, #{warning_count} warnings\" if defined?(Rails)\n    end\n\n    alerts\n  end\nend\n\n# Schedule alert evaluation every 5 minutes\n# In Rails initializer:\n# AlertEvaluationJob.set(cron: '*/5 * * * *').perform_later\n</code></pre>"},{"location":"api-reference/api-jobs/#custom-jobs","title":"Custom Jobs","text":"<p>Create custom background jobs that integrate seamlessly with Ragdoll's processing pipeline and follow established patterns.</p>"},{"location":"api-reference/api-jobs/#job-class-patterns","title":"Job Class Patterns","text":"<p>Standardized patterns for creating custom jobs:</p> <pre><code>module Ragdoll\n  module Core\n    module Jobs\n      # Base class for all custom Ragdoll jobs\n      class CustomBaseJob &lt; ActiveJob::Base\n        # Inherit standard retry and error handling\n        include JobErrorHandling\n\n        queue_as :custom_processing\n\n        # Standard retry configuration\n        retry_on StandardError, wait: :exponentially_longer, attempts: 3\n        discard_on ActiveRecord::RecordNotFound\n\n        private\n\n        # Helper method for document status updates\n        def update_document_status(document_id, status, metadata = {})\n          document = Ragdoll::Document.find_by(id: document_id)\n          return unless document\n\n          update_data = { status: status }\n          if metadata.any?\n            update_data[:metadata] = document.metadata.merge(metadata)\n          end\n\n          document.update!(update_data)\n        end\n\n        # Helper for logging job progress\n        def log_progress(message, document_id: nil)\n          log_data = {\n            job_class: self.class.name,\n            message: message,\n            job_id: job_id,\n            timestamp: Time.current.iso8601\n          }\n          log_data[:document_id] = document_id if document_id\n\n          Rails.logger.info log_data.to_json if defined?(Rails)\n        end\n      end\n    end\n  end\nend\n\n# Example custom job: Advanced content analysis\nclass AdvancedContentAnalysis &lt; Ragdoll::CustomBaseJob\n  def perform(document_id, analysis_options = {})\n    log_progress(\"Starting advanced content analysis\", document_id: document_id)\n\n    document = Ragdoll::Document.find(document_id)\n    return unless document.content.present?\n\n    # Update status to show processing\n    update_document_status(document_id, 'processing', {\n      'analysis_stage' =&gt; 'advanced_analysis',\n      'analysis_options' =&gt; analysis_options\n    })\n\n    analysis_results = {\n      sentiment_analysis: perform_sentiment_analysis(document.content),\n      topic_modeling: perform_topic_modeling(document.content),\n      entity_extraction: extract_named_entities(document.content),\n      readability_score: calculate_readability(document.content)\n    }\n\n    # Store analysis results in metadata\n    update_document_status(document_id, 'processed', {\n      'advanced_analysis' =&gt; analysis_results,\n      'analysis_completed_at' =&gt; Time.current.iso8601\n    })\n\n    log_progress(\"Advanced content analysis completed\", document_id: document_id)\n\n    # Queue follow-up jobs if needed\n    if analysis_results[:sentiment_analysis][:score] &lt; -0.5\n      ContentModerationJob.perform_later(document_id)\n    end\n\n    analysis_results\n  end\n\n  private\n\n  def perform_sentiment_analysis(content)\n    # Integration with sentiment analysis service\n    # This could use various APIs or libraries\n    {\n      score: rand(-1.0..1.0), # Mock score\n      label: %w[positive negative neutral].sample,\n      confidence: rand(0.5..1.0)\n    }\n  end\n\n  def perform_topic_modeling(content)\n    # Topic modeling implementation\n    {\n      primary_topics: ['technology', 'business', 'science'],\n      topic_distribution: { 'technology' =&gt; 0.6, 'business' =&gt; 0.3, 'science' =&gt; 0.1 }\n    }\n  end\nend\n\n# Example: Batch processing job\nclass BatchDocumentProcessor &lt; Ragdoll::CustomBaseJob\n  queue_as :batch_processing\n\n  def perform(document_ids, processing_options = {})\n    log_progress(\"Starting batch processing of #{document_ids.length} documents\")\n\n    results = {\n      total_documents: document_ids.length,\n      successful: 0,\n      failed: 0,\n      processing_times: [],\n      errors: []\n    }\n\n    document_ids.each_with_index do |document_id, index|\n      start_time = Time.current\n\n      begin\n        process_single_document(document_id, processing_options)\n        results[:successful] += 1\n\n        processing_time = Time.current - start_time\n        results[:processing_times] &lt;&lt; processing_time\n\n        # Progress reporting every 10 documents\n        if (index + 1) % 10 == 0\n          log_progress(\"Processed #{index + 1}/#{document_ids.length} documents\")\n        end\n\n      rescue =&gt; e\n        results[:failed] += 1\n        results[:errors] &lt;&lt; {\n          document_id: document_id,\n          error: e.message,\n          backtrace: e.backtrace&amp;.first(3)\n        }\n\n        Rails.logger.error \"Batch processing failed for document #{document_id}: #{e.message}\" if defined?(Rails)\n      end\n    end\n\n    log_progress(\"Batch processing completed: #{results[:successful]} successful, #{results[:failed]} failed\")\n    results\n  end\n\n  private\n\n  def process_single_document(document_id, options)\n    # Queue individual processing jobs\n    ExtractText.perform_now(document_id) if options[:extract_text]\n    GenerateEmbeddings.perform_now(document_id) if options[:generate_embeddings]\n    GenerateSummary.perform_now(document_id) if options[:generate_summary]\n  end\nend\n</code></pre>"},{"location":"api-reference/api-jobs/#integration-with-core-services","title":"Integration with Core Services","text":"<p>Seamless integration with Ragdoll services:</p> <pre><code># Custom job using core services\nclass CustomEmbeddingEnhancement &lt; Ragdoll::CustomBaseJob\n  def perform(document_id, enhancement_type)\n    document = Ragdoll::Document.find(document_id)\n\n    # Use core embedding service\n    embedding_service = Ragdoll::EmbeddingService.new\n\n    case enhancement_type\n    when 'multi_provider'\n      enhance_with_multiple_providers(document, embedding_service)\n    when 'contextual_chunks'\n      create_contextual_embeddings(document, embedding_service)\n    when 'semantic_clustering'\n      cluster_document_embeddings(document, embedding_service)\n    end\n  end\n\n  private\n\n  def enhance_with_multiple_providers(document, embedding_service)\n    # Generate embeddings using different providers for comparison\n    providers = ['openai', 'huggingface', 'cohere']\n\n    providers.each do |provider|\n      begin\n        # Configure service for specific provider\n        embedding_service.configure_provider(provider)\n\n        # Generate embeddings\n        embeddings = embedding_service.generate_embeddings_for_content(\n          document.content,\n          model: get_model_for_provider(provider)\n        )\n\n        # Store with provider-specific metadata\n        store_provider_embeddings(document.id, provider, embeddings)\n\n      rescue =&gt; e\n        Rails.logger.warn \"Failed to generate embeddings with #{provider}: #{e.message}\" if defined?(Rails)\n      end\n    end\n  end\n\n  def create_contextual_embeddings(document, embedding_service)\n    # Use text chunker for intelligent content splitting\n    text_chunker = Ragdoll::TextChunker.new\n\n    chunks = text_chunker.chunk_text(\n      document.content,\n      max_tokens: 512,\n      overlap: 128,\n      preserve_sentences: true\n    )\n\n    # Generate embeddings with surrounding context\n    chunks.each_with_index do |chunk, index|\n      # Add context from adjacent chunks\n      contextual_content = build_contextual_content(chunks, index)\n\n      embedding = embedding_service.generate_embedding(\n        contextual_content,\n        metadata: {\n          chunk_index: index,\n          has_context: true,\n          context_size: contextual_content.length - chunk.length\n        }\n      )\n\n      # Store contextual embedding\n      store_contextual_embedding(document.id, chunk, embedding, index)\n    end\n  end\nend\n\n# Custom job using search engine\nclass DocumentSimilarityAnalysis &lt; Ragdoll::CustomBaseJob\n  def perform(document_id, similarity_threshold = 0.8)\n    document = Ragdoll::Document.find(document_id)\n\n    # Use core search engine\n    embedding_service = Ragdoll::EmbeddingService.new\n    search_engine = Ragdoll::SearchEngine.new(embedding_service)\n\n    # Find similar documents\n    similar_documents = search_engine.search_documents(\n      document.content[0..1000], # Use first 1000 chars as query\n      limit: 20,\n      threshold: similarity_threshold,\n      filters: {\n        document_type: document.document_type\n      }\n    )\n\n    # Analyze similarity patterns\n    similarity_analysis = {\n      total_similar: similar_documents.length,\n      avg_similarity: calculate_average_similarity(similar_documents),\n      similarity_clusters: group_by_similarity_ranges(similar_documents),\n      related_keywords: extract_common_keywords(similar_documents)\n    }\n\n    # Update document metadata with similarity analysis\n    update_document_status(document_id, document.status, {\n      'similarity_analysis' =&gt; similarity_analysis,\n      'similar_document_ids' =&gt; similar_documents.map { |d| d[:document_id] }\n    })\n\n    similarity_analysis\n  end\nend\n</code></pre>"},{"location":"api-reference/api-jobs/#testing-approaches","title":"Testing Approaches","text":"<p>Comprehensive testing strategies for custom jobs:</p> <pre><code># Test helper for job testing\nmodule JobTestHelpers\n  def perform_job_now(job_class, *args)\n    # Perform job synchronously for testing\n    job_class.perform_now(*args)\n  end\n\n  def assert_job_enqueued(job_class, *args)\n    assert_enqueued_with(job: job_class, args: args)\n  end\n\n  def create_test_document(attributes = {})\n    default_attributes = {\n      title: 'Test Document',\n      location: '/tmp/test.txt',\n      document_type: 'text',\n      content: 'This is test content for processing.',\n      status: 'pending',\n      file_modified_at: Time.current\n    }\n\n    Ragdoll::Core::Ragdoll::Document.create!(default_attributes.merge(attributes))\n  end\nend\n\n# Example test class\nclass AdvancedContentAnalysisTest &lt; ActiveJob::TestCase\n  include JobTestHelpers\n\n  setup do\n    @document = create_test_document(\n      content: 'This is a comprehensive test document with substantial content for analysis.'\n    )\n  end\n\n  test 'processes document successfully' do\n    # Test synchronous execution\n    result = perform_job_now(AdvancedContentAnalysis, @document.id)\n\n    assert_not_nil result\n    assert_includes result.keys, :sentiment_analysis\n    assert_includes result.keys, :topic_modeling\n\n    # Verify document status updated\n    @document.reload\n    assert_equal 'processed', @document.status\n    assert_not_nil @document.metadata['advanced_analysis']\n  end\n\n  test 'handles missing document gracefully' do\n    # Test with non-existent document ID\n    assert_nothing_raised do\n      perform_job_now(AdvancedContentAnalysis, 999999)\n    end\n  end\n\n  test 'queues follow-up jobs for negative sentiment' do\n    # Mock sentiment analysis to return negative score\n    AdvancedContentAnalysis.any_instance.stubs(:perform_sentiment_analysis)\n                           .returns({ score: -0.8, label: 'negative', confidence: 0.9 })\n\n    assert_enqueued_with(job: ContentModerationJob) do\n      perform_job_now(AdvancedContentAnalysis, @document.id)\n    end\n  end\n\n  test 'handles service errors gracefully' do\n    # Simulate service failure\n    AdvancedContentAnalysis.any_instance.stubs(:perform_sentiment_analysis)\n                           .raises(StandardError, 'Service unavailable')\n\n    assert_raises(StandardError) do\n      perform_job_now(AdvancedContentAnalysis, @document.id)\n    end\n\n    # Verify document marked as error\n    @document.reload\n    assert_equal 'error', @document.status\n  end\n\n  test 'processes with custom options' do\n    options = { include_entities: true, deep_analysis: false }\n\n    result = perform_job_now(AdvancedContentAnalysis, @document.id, options)\n\n    @document.reload\n    stored_options = @document.metadata['analysis_options']\n    assert_equal options, stored_options\n  end\nend\n\n# Integration testing with real services\nclass CustomJobIntegrationTest &lt; ActiveJob::TestCase\n  include JobTestHelpers\n\n  setup do\n    # Setup real service connections for integration testing\n    @original_config = Ragdoll.config.dup\n\n    Ragdoll::Core.configure do |config|\n      config.ruby_llm_config[:openai][:api_key] = ENV['TEST_OPENAI_API_KEY']\n      config.embedding_config[:text][:model] = 'text-embedding-3-small'\n    end\n  end\n\n  teardown do\n    # Restore original configuration\n    Ragdoll.config = @original_config\n  end\n\n  test 'integration with real embedding service' do\n    skip 'Requires API keys' unless ENV['TEST_OPENAI_API_KEY']\n\n    document = create_test_document(\n      content: 'Machine learning is transforming artificial intelligence applications.'\n    )\n\n    # This will make real API calls\n    result = perform_job_now(CustomEmbeddingEnhancement, document.id, 'multi_provider')\n\n    # Verify embeddings were created\n    assert document.embeddings.count &gt; 0\n\n    # Verify embedding quality\n    embedding = document.embeddings.first\n    assert embedding.embedding_vector.present?\n    assert_equal 1536, embedding.embedding_vector.length # OpenAI embedding size\n  end\nend\n</code></pre>"},{"location":"api-reference/api-jobs/#performance-considerations","title":"Performance Considerations","text":"<p>Optimization strategies for custom jobs:</p> <pre><code># Performance-optimized job patterns\nclass PerformantCustomJob &lt; Ragdoll::CustomBaseJob\n  # Configuration for performance optimization\n  queue_as :high_performance\n  timeout 30.minutes\n\n  def perform(document_ids, options = {})\n    # Batch processing for efficiency\n    documents = Ragdoll::Document.includes(:contents, :embeddings)\n                               .where(id: document_ids)\n\n    # Pre-load services to avoid repeated initialization\n    services = initialize_services\n\n    # Process in batches to manage memory\n    documents.find_in_batches(batch_size: 50) do |batch|\n      process_batch(batch, services, options)\n\n      # Periodic garbage collection for long-running jobs\n      GC.start if should_run_gc?\n    end\n  end\n\n  private\n\n  def initialize_services\n    {\n      embedding_service: Ragdoll::EmbeddingService.new,\n      text_chunker: Ragdoll::TextChunker.new,\n      search_engine: Ragdoll::SearchEngine.new(embedding_service)\n    }\n  end\n\n  def process_batch(documents, services, options)\n    # Batch database operations\n    updates = []\n\n    documents.each do |document|\n      result = process_single_document(document, services, options)\n\n      if result\n        updates &lt;&lt; {\n          id: document.id,\n          metadata: document.metadata.merge(result)\n        }\n      end\n    end\n\n    # Bulk update to reduce database round trips\n    bulk_update_documents(updates) if updates.any?\n  end\n\n  def bulk_update_documents(updates)\n    # Use raw SQL for efficient bulk updates\n    updates.each_slice(100) do |batch|\n      update_sql = build_bulk_update_sql(batch)\n      ActiveRecord::Base.connection.execute(update_sql)\n    end\n  end\n\n  def should_run_gc?\n    # Run GC every 100 processed documents or when memory usage is high\n    @processed_count ||= 0\n    @processed_count += 1\n\n    (@processed_count % 100 == 0) || (get_memory_usage &gt; 500) # 500MB threshold\n  end\n\n  def get_memory_usage\n    `ps -o rss= -p #{Process.pid}`.to_i / 1024 # MB\n  end\nend\n\n# Memory-efficient streaming job\nclass StreamingProcessorJob &lt; Ragdoll::CustomBaseJob\n  def perform(large_document_id)\n    document = Ragdoll::Document.find(large_document_id)\n\n    # Process document in chunks to avoid loading entire content into memory\n    process_in_chunks(document) do |chunk, chunk_index|\n      # Process each chunk individually\n      result = process_chunk(chunk, chunk_index)\n\n      # Yield control to allow other jobs to run\n      sleep(0.01) if chunk_index % 10 == 0\n\n      result\n    end\n  end\n\n  private\n\n  def process_in_chunks(document)\n    chunk_size = 1000 # characters\n    content_length = document.content.length\n\n    (0...content_length).step(chunk_size).each_with_index do |start, index|\n      chunk = document.content[start, chunk_size]\n      yield chunk, index\n    end\n  end\nend\n\n# Concurrent processing job (be careful with database connections)\nclass ConcurrentProcessingJob &lt; Ragdoll::CustomBaseJob\n  def perform(document_ids, max_concurrency: 5)\n    require 'concurrent-ruby'\n\n    # Use thread pool for concurrent processing\n    pool = Concurrent::FixedThreadPool.new(max_concurrency)\n    futures = []\n\n    document_ids.each do |document_id|\n      future = Concurrent::Future.execute(executor: pool) do\n        # Each thread needs its own database connection\n        ActiveRecord::Base.connection_pool.with_connection do\n          process_single_document(document_id)\n        end\n      end\n\n      futures &lt;&lt; future\n    end\n\n    # Wait for all to complete\n    results = futures.map(&amp;:value)\n\n    # Shutdown thread pool\n    pool.shutdown\n    pool.wait_for_termination(30) # 30 second timeout\n\n    results\n  rescue =&gt; e\n    pool&amp;.shutdown\n    raise e\n  end\nend\n</code></pre>"},{"location":"api-reference/api-jobs/#summary","title":"Summary","text":"<p>Ragdoll's background job system provides a robust, scalable foundation for asynchronous document processing. Built on ActiveJob, the system offers:</p> <p>Core Jobs: - <code>ExtractText</code>: File processing and content extraction - <code>GenerateEmbeddings</code>: Vector embedding generation for all content types - <code>ExtractKeywords</code>: AI-powered keyword extraction using LLMs - <code>GenerateSummary</code>: Document summarization with configurable length</p> <p>Key Features: - Flexible Configuration: Support for all major queue adapters (Sidekiq, Resque, etc.) - Intelligent Retry Logic: Exponential backoff with error-specific retry policies - Comprehensive Monitoring: Job status tracking, performance metrics, and queue health - Error Handling: Dead letter queues, failure notifications, and recovery tools - Custom Job Support: Extensible framework for domain-specific processing</p> <p>Production Ready: - Horizontal scaling with load balancing - Resource-aware job distribution  - Automated alerting and escalation - Performance optimization patterns - Comprehensive testing approaches</p> <p>The job system seamlessly integrates with Ragdoll's document processing pipeline, ensuring reliable and efficient handling of AI operations at scale.</p> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"api-reference/api-models/","title":"Models Reference","text":"<p>Ragdoll uses a sophisticated ActiveRecord model architecture with Single Table Inheritance (STI) for multi-modal content storage and polymorphic associations for flexible embeddings.</p>"},{"location":"api-reference/api-models/#activerecord-models-and-relationships","title":"ActiveRecord Models and Relationships","text":"<p>The model architecture provides:</p> <ul> <li>Single Table Inheritance (STI): Content models (<code>TextContent</code>, <code>ImageContent</code>, <code>AudioContent</code>) share a single table</li> <li>Polymorphic Associations: Embeddings can belong to any content type through polymorphic relationships</li> <li>PostgreSQL Optimizations: Native JSON columns, full-text search indexes, and pgvector integration</li> <li>Rich Metadata Support: Flexible metadata storage with validation and type-specific schemas</li> <li>Usage Analytics: Built-in tracking for search optimization and performance monitoring</li> <li>Comprehensive Validations: Data integrity through extensive validation rules and callbacks</li> </ul>"},{"location":"api-reference/api-models/#core-models","title":"Core Models","text":""},{"location":"api-reference/api-models/#document-model","title":"Document Model","text":"<p>Class: <code>Ragdoll::Document</code></p> <p>Table: <code>ragdoll_documents</code></p> <p>Primary Attributes: <pre><code># Core document identification\nid                :bigint           # Primary key\nlocation          :string           # Source location (file path, URL, identifier)\ntitle             :string           # Human-readable document title\ndocument_type     :string           # Document format type (text, image, audio, pdf, etc.)\nstatus            :string           # Processing status (pending, processing, processed, error)\nfile_modified_at  :datetime         # Source file modification timestamp\n\n# Metadata storage\nmetadata          :json             # LLM-generated structured metadata\nfile_metadata     :json             # File properties and processing metadata\n\n# Timestamps\ncreated_at        :datetime\nupdated_at        :datetime\n</code></pre></p> <p>Multi-Modal Content Associations: <pre><code># STI-based content relationships\nhas_many :contents, class_name: \"Ragdoll::Content\", dependent: :destroy\nhas_many :text_contents, class_name: \"Ragdoll::TextContent\"\nhas_many :image_contents, class_name: \"Ragdoll::ImageContent\"\nhas_many :audio_contents, class_name: \"Ragdoll::AudioContent\"\n\n# Embedding relationships through content\nhas_many :text_embeddings, through: :text_contents, source: :embeddings\nhas_many :image_embeddings, through: :image_contents, source: :embeddings\nhas_many :audio_embeddings, through: :audio_contents, source: :embeddings\n</code></pre></p> <p>Key Instance Methods: <pre><code># Content access\ndocument.content                    # Returns combined content from all content types\ndocument.content = \"new content\"    # Creates appropriate content model\n\n# Multi-modal detection\ndocument.multi_modal?               # True if document has multiple content types\ndocument.content_types              # Array of content types: ['text', 'image', 'audio']\ndocument.primary_content_type       # Primary content type for the document\n\n# Statistics\ndocument.total_word_count           # Sum of words across all text content\ndocument.total_character_count      # Sum of characters across all text content\ndocument.total_embedding_count      # Total embeddings across all content types\ndocument.embeddings_by_type         # Hash: { text: 10, image: 5, audio: 2 }\n\n# Processing\ndocument.processed?                 # True if status == 'processed'\ndocument.process_content!           # Generate embeddings and metadata\ndocument.generate_metadata!         # Generate LLM-based metadata\n</code></pre></p> <p>Search and Query Methods: <pre><code># PostgreSQL full-text search\nDocument.search_content(\"machine learning\")\n\n# Faceted search with metadata filters\nDocument.faceted_search(\n  query: \"AI research\",\n  keywords: [\"neural networks\"],\n  classification: \"academic_paper\",\n  tags: [\"machine-learning\"]\n)\n\n# Hybrid search combining semantic and text search\nDocument.hybrid_search(\n  \"deep learning applications\",\n  query_embedding: embedding_vector,\n  semantic_weight: 0.7,\n  text_weight: 0.3\n)\n</code></pre></p>"},{"location":"api-reference/api-models/#content-models-sti-architecture","title":"Content Models (STI Architecture)","text":"<p>Base Class: <code>Ragdoll::Content</code></p> <p>Table: <code>ragdoll_contents</code> (shared by all content types)</p> <p>STI Classes: - <code>Ragdoll::TextContent</code> - <code>Ragdoll::ImageContent</code>  - <code>Ragdoll::AudioContent</code></p> <p>Shared Attributes: <pre><code>id              :bigint           # Primary key\ntype            :string           # STI discriminator (TextContent, ImageContent, etc.)\ndocument_id     :bigint           # Foreign key to document\nembedding_model :string           # Model used for embeddings\ncontent         :text             # Content text (text, description, transcript)\ndata            :text             # Raw file data or metadata\nmetadata        :json             # Content-specific metadata\nduration        :float            # Audio duration (audio content only)\nsample_rate     :integer          # Audio sample rate (audio content only)\ncreated_at      :datetime\nupdated_at      :datetime\n</code></pre></p> <p>Polymorphic Relationships: <pre><code># Each content model belongs to a document\nbelongs_to :document, class_name: \"Ragdoll::Document\"\n\n# Each content model can have many embeddings\nhas_many :embeddings, class_name: \"Ragdoll::Embedding\", as: :embeddable\n</code></pre></p>"},{"location":"api-reference/api-models/#textcontent-model","title":"TextContent Model","text":"<p>Specific Validations: <pre><code>validates :content, presence: true  # Text content is required\n</code></pre></p> <p>Text-Specific Methods: <pre><code># Content analysis\ntext_content.word_count             # Number of words in content\ntext_content.character_count        # Number of characters in content\ntext_content.line_count             # Number of lines (from metadata)\n\n# Chunking configuration\ntext_content.chunk_size             # Tokens per chunk (default: 1000)\ntext_content.chunk_size = 1500      # Set custom chunk size\ntext_content.overlap                # Token overlap (default: 200)\ntext_content.overlap = 300          # Set custom overlap\n\n# Content processing\ntext_content.chunks                 # Array of content chunks with positions\ntext_content.generate_embeddings!   # Generate embeddings for all chunks\n</code></pre></p> <p>Text Processing Example: <pre><code>text_content = document.text_contents.create!(\n  content: \"Large document text...\",\n  embedding_model: \"text-embedding-3-large\",\n  metadata: {\n    encoding: \"UTF-8\",\n    line_count: 150,\n    chunk_size: 1000,\n    overlap: 200\n  }\n)\n\n# Generate embeddings automatically\ntext_content.generate_embeddings!\n\n# Access generated chunks and embeddings\ntext_content.chunks.each do |chunk|\n  puts \"Chunk #{chunk[:chunk_index]}: #{chunk[:content][0..50]}...\"\nend\n\ntext_content.embeddings.each do |embedding|\n  puts \"Embedding #{embedding.chunk_index}: #{embedding.embedding_vector.length} dimensions\"\nend\n</code></pre></p>"},{"location":"api-reference/api-models/#imagecontent-model","title":"ImageContent Model","text":"<p>Image-Specific Attributes: <pre><code># content field stores AI-generated description\n# data field stores image binary data or file reference\n# metadata stores image properties\n</code></pre></p> <p>Image-Specific Methods: <pre><code># Image properties (from metadata)\nimage_content.width                 # Image width in pixels\nimage_content.height                # Image height in pixels\nimage_content.file_size             # File size in bytes\nimage_content.format                # Image format (jpg, png, etc.)\n\n# AI-generated content\nimage_content.description           # AI-generated description (stored in content field)\nimage_content.objects_detected      # Detected objects (from metadata)\nimage_content.scene_type            # Scene classification (from metadata)\n</code></pre></p>"},{"location":"api-reference/api-models/#audiocontent-model-planned","title":"AudioContent Model (Planned)","text":"<p>Audio-Specific Attributes: <pre><code>duration        :float            # Audio duration in seconds\nsample_rate     :integer          # Sample rate in Hz\n# content field stores transcript\n# data field stores audio binary data\n# metadata stores audio properties and timestamps\n</code></pre></p> <p>Audio-Specific Methods (Planned): <pre><code># Audio properties\naudio_content.duration_formatted    # \"5:42\" format\naudio_content.bitrate               # Audio bitrate (from metadata)\naudio_content.channels              # Number of audio channels\n\n# Transcript and timestamps\naudio_content.transcript            # Full transcript (stored in content field)\naudio_content.timestamps            # Word-level timestamps (from metadata)\naudio_content.speakers             # Speaker identification (from metadata)\n</code></pre></p>"},{"location":"api-reference/api-models/#embedding-model","title":"Embedding Model","text":"<p>Class: <code>Ragdoll::Embedding</code></p> <p>Table: <code>ragdoll_embeddings</code></p> <p>Attributes: <pre><code>id                :bigint           # Primary key\nembeddable_type   :string           # Polymorphic type (Content class name)\nembeddable_id     :bigint           # Polymorphic ID (Content record ID)\nchunk_index       :integer          # Order within content\ncontent           :text             # Original text that was embedded\nembedding_vector  :vector(1536)     # pgvector column (configurable dimensions)\nusage_count       :integer          # Number of times used in searches\nreturned_at       :datetime         # Last usage timestamp\ncreated_at        :datetime\nupdated_at        :datetime\n</code></pre></p> <p>Polymorphic Association: <pre><code># Belongs to any content type through polymorphic association\nbelongs_to :embeddable, polymorphic: true\n\n# Can belong to TextContent, ImageContent, or AudioContent\nembedding.embeddable                # Returns the associated content object\nembedding.embeddable_type           # \"Ragdoll::TextContent\"\nembedding.embeddable_id             # Content record ID\n</code></pre></p> <p>Vector Search Methods: <pre><code># pgvector similarity search\nEmbedding.search_similar(\n  query_embedding,\n  limit: 20,\n  threshold: 0.7,\n  filters: {\n    embeddable_type: \"Ragdoll::TextContent\",\n    document_type: \"pdf\",\n    embedding_model: \"text-embedding-3-large\"\n  }\n)\n\n# Usage analytics\nembedding.mark_as_used!             # Increment usage_count and update returned_at\nembedding.usage_score               # Calculated usage score for ranking\nembedding.embedding_dimensions      # Number of vector dimensions\n</code></pre></p> <p>Search Result Format: <pre><code>[\n  {\n    embedding_id: \"123\",\n    embeddable_id: \"456\",\n    embeddable_type: \"Ragdoll::TextContent\",\n    document_id: \"789\",\n    document_title: \"AI Research Paper\",\n    document_location: \"/path/to/document.pdf\",\n    content: \"Machine learning algorithms...\",\n    similarity: 0.85,\n    distance: 0.15,\n    chunk_index: 5,\n    embedding_dimensions: 1536,\n    embedding_model: \"text-embedding-3-large\",\n    usage_count: 12,\n    returned_at: \"2025-01-15T10:30:00Z\",\n    combined_score: 0.92\n  }\n]\n</code></pre></p>"},{"location":"api-reference/api-models/#model-relationships","title":"Model Relationships","text":"<p>Ragdoll uses a sophisticated relationship structure optimized for multi-modal content:</p>"},{"location":"api-reference/api-models/#primary-relationships","title":"Primary Relationships","text":"<pre><code>erDiagram\n    Document ||--o{ TextContent : \"has many\"\n    Document ||--o{ ImageContent : \"has many\"\n    Document ||--o{ AudioContent : \"has many\"\n    TextContent ||--o{ Embedding : \"has many (polymorphic)\"\n    ImageContent ||--o{ Embedding : \"has many (polymorphic)\"\n    AudioContent ||--o{ Embedding : \"has many (polymorphic)\"\n\n    Document {\n        bigint id PK\n        string location\n        string title\n        string document_type\n        string status\n        json metadata\n        json file_metadata\n        datetime file_modified_at\n    }\n\n    TextContent {\n        bigint id PK\n        string type \"'TextContent'\"\n        bigint document_id FK\n        string embedding_model\n        text content\n        text data\n        json metadata\n    }\n\n    ImageContent {\n        bigint id PK\n        string type \"'ImageContent'\"\n        bigint document_id FK\n        string embedding_model\n        text content \"AI description\"\n        text data \"Image data\"\n        json metadata\n    }\n\n    AudioContent {\n        bigint id PK\n        string type \"'AudioContent'\"\n        bigint document_id FK\n        string embedding_model\n        text content \"Transcript\"\n        text data \"Audio data\"\n        json metadata\n        float duration\n        integer sample_rate\n    }\n\n    Embedding {\n        bigint id PK\n        string embeddable_type FK\n        bigint embeddable_id FK\n        integer chunk_index\n        text content\n        vector embedding_vector\n        integer usage_count\n        datetime returned_at\n    }</code></pre>"},{"location":"api-reference/api-models/#association-details","title":"Association Details","text":"<p>Document Associations: <pre><code>class Document &lt; ActiveRecord::Base\n  # Content associations (STI)\n  has_many :contents, class_name: \"Content\", dependent: :destroy\n  has_many :text_contents, -&gt; { where(type: \"TextContent\") }\n  has_many :image_contents, -&gt; { where(type: \"ImageContent\") }\n  has_many :audio_contents, -&gt; { where(type: \"AudioContent\") }\n\n  # Embedding associations through content\n  has_many :text_embeddings, through: :text_contents, source: :embeddings\n  has_many :image_embeddings, through: :image_contents, source: :embeddings\n  has_many :audio_embeddings, through: :audio_contents, source: :embeddings\n\n  # Access all embeddings across content types\n  def all_embeddings(content_type: nil)\n    if content_type\n      case content_type.to_s\n      when 'text' then text_embeddings\n      when 'image' then image_embeddings\n      when 'audio' then audio_embeddings\n      end\n    else\n      Embedding.where(\n        embeddable_type: 'Ragdoll::Content',\n        embeddable_id: contents.pluck(:id)\n      )\n    end\n  end\nend\n</code></pre></p> <p>Content Associations (STI Base): <pre><code>class Content &lt; ActiveRecord::Base\n  # Parent document relationship\n  belongs_to :document, class_name: \"Document\", foreign_key: \"document_id\"\n\n  # Polymorphic embedding relationship\n  has_many :embeddings, as: :embeddable, dependent: :destroy\n\n  # STI subclasses: TextContent, ImageContent, AudioContent\nend\n</code></pre></p> <p>Embedding Associations (Polymorphic): <pre><code>class Embedding &lt; ActiveRecord::Base\n  # Polymorphic association - can belong to any content type\n  belongs_to :embeddable, polymorphic: true\n\n  # Access parent document through content\n  def document\n    embeddable&amp;.document\n  end\n\n  # Scopes for different content types\n  scope :text_embeddings, -&gt; { where(embeddable_type: \"Ragdoll::TextContent\") }\n  scope :image_embeddings, -&gt; { where(embeddable_type: \"Ragdoll::ImageContent\") }\n  scope :audio_embeddings, -&gt; { where(embeddable_type: \"Ragdoll::AudioContent\") }\nend\n</code></pre></p>"},{"location":"api-reference/api-models/#database-constraints-and-foreign-keys","title":"Database Constraints and Foreign Keys","text":"<p>Foreign Key Constraints: <pre><code>-- Document to Content relationship\nALTER TABLE ragdoll_contents \nADD CONSTRAINT fk_contents_document \nFOREIGN KEY (document_id) REFERENCES ragdoll_documents(id) \nON DELETE CASCADE;\n\n-- Polymorphic embedding relationships (enforced by application)\n-- Note: PostgreSQL doesn't support polymorphic foreign key constraints\n-- These are enforced through ActiveRecord validations and callbacks\n</code></pre></p> <p>Unique Constraints: <pre><code>-- Ensure unique document locations\nALTER TABLE ragdoll_documents \nADD CONSTRAINT unique_document_location UNIQUE (location);\n\n-- Ensure unique chunk indexes per content\nALTER TABLE ragdoll_embeddings \nADD CONSTRAINT unique_chunk_per_content \nUNIQUE (embeddable_type, embeddable_id, chunk_index);\n</code></pre></p> <p>Check Constraints: <pre><code>-- Ensure valid document types\nALTER TABLE ragdoll_documents \nADD CONSTRAINT valid_document_type \nCHECK (document_type IN ('text', 'image', 'audio', 'pdf', 'docx', 'html', 'markdown', 'mixed'));\n\n-- Ensure valid processing status\nALTER TABLE ragdoll_documents \nADD CONSTRAINT valid_status \nCHECK (status IN ('pending', 'processing', 'processed', 'error'));\n\n-- Ensure valid content types for STI\nALTER TABLE ragdoll_contents \nADD CONSTRAINT valid_content_type \nCHECK (type IN ('Ragdoll::TextContent', \n                'Ragdoll::ImageContent', \n                'Ragdoll::AudioContent'));\n</code></pre></p>"},{"location":"api-reference/api-models/#index-strategy","title":"Index Strategy","text":"<p>Performance Indexes: <pre><code>-- Document indexes\nCREATE INDEX idx_documents_status ON ragdoll_documents(status);\nCREATE INDEX idx_documents_type ON ragdoll_documents(document_type);\nCREATE INDEX idx_documents_created_at ON ragdoll_documents(created_at);\n\n-- Content indexes (STI table)\nCREATE INDEX idx_contents_type ON ragdoll_contents(type);\nCREATE INDEX idx_contents_document_id ON ragdoll_contents(document_id);\nCREATE INDEX idx_contents_embedding_model ON ragdoll_contents(embedding_model);\n\n-- Embedding indexes\nCREATE INDEX idx_embeddings_embeddable ON ragdoll_embeddings(embeddable_type, embeddable_id);\nCREATE INDEX idx_embeddings_usage_count ON ragdoll_embeddings(usage_count);\nCREATE INDEX idx_embeddings_returned_at ON ragdoll_embeddings(returned_at);\n\n-- pgvector similarity search index\nCREATE INDEX idx_embeddings_vector_cosine ON ragdoll_embeddings \nUSING ivfflat (embedding_vector vector_cosine_ops) WITH (lists = 100);\n</code></pre></p> <p>Full-Text Search Indexes: <pre><code>-- Document full-text search\nCREATE INDEX idx_documents_fulltext ON ragdoll_documents \nUSING gin(to_tsvector('english', \n  title || ' ' || \n  COALESCE(metadata-&gt;&gt;'summary', '') || ' ' || \n  COALESCE(metadata-&gt;&gt;'keywords', '') || ' ' || \n  COALESCE(metadata-&gt;&gt;'description', '')\n));\n\n-- Content full-text search\nCREATE INDEX idx_contents_fulltext ON ragdoll_contents \nUSING gin(to_tsvector('english', COALESCE(content, '')));\n</code></pre></p>"},{"location":"api-reference/api-models/#instance-methods","title":"Instance Methods","text":""},{"location":"api-reference/api-models/#document-methods","title":"Document Methods","text":""},{"location":"api-reference/api-models/#content-retrieval-methods","title":"Content Retrieval Methods","text":"<pre><code># Dynamic content access based on primary content type\ndocument.content                    \n# Returns combined content from all content types\n# For text: concatenated text from all text_contents\n# For image: concatenated descriptions from all image_contents\n# For audio: concatenated transcripts from all audio_contents\n\n# Content type detection\ndocument.content_types              # =&gt; ['text', 'image']\ndocument.primary_content_type       # =&gt; 'text'\ndocument.multi_modal?               # =&gt; true (if multiple content types)\n\n# Content statistics\ndocument.total_word_count           # Sum across all text content\ndocument.total_character_count      # Sum across all text content\ndocument.total_embedding_count      # Sum across all content types\ndocument.embeddings_by_type         # =&gt; { text: 15, image: 3, audio: 0 }\n\n# Content access by type\ndocument.text_contents.each { |tc| puts tc.content }\ndocument.image_contents.each { |ic| puts ic.content }  # AI descriptions\ndocument.audio_contents.each { |ac| puts ac.content }  # Transcripts\n</code></pre>"},{"location":"api-reference/api-models/#metadata-accessors","title":"Metadata Accessors","text":"<pre><code># LLM-generated metadata (stored in metadata JSON column)\ndocument.metadata                   # Full metadata hash\ndocument.description                # metadata['description']\ndocument.description = \"New desc\"   # Updates metadata hash\ndocument.classification             # metadata['classification']\ndocument.classification = \"technical\"\ndocument.tags                       # metadata['tags'] (array)\ndocument.tags = ['ai', 'research']\n\n# Metadata utility methods\ndocument.has_summary?               # Check if summary exists\ndocument.has_keywords?              # Check if keywords exist\ndocument.keywords_array             # Parse keywords into array\ndocument.add_keyword('machine-learning')\ndocument.remove_keyword('outdated')\n\n# File metadata (stored in file_metadata JSON column)\ndocument.file_metadata              # File processing metadata\ndocument.total_file_size            # Sum of all content file sizes\ndocument.primary_file_type          # Document's primary file type\n</code></pre>"},{"location":"api-reference/api-models/#processing-status-methods","title":"Processing Status Methods","text":"<pre><code># Status checking\ndocument.processed?                 # status == 'processed'\ndocument.status                     # 'pending', 'processing', 'processed', 'error'\n\n# Content processing\ndocument.process_content!           # Full processing pipeline:\n                                    # 1. Generate embeddings for all content\n                                    # 2. Generate LLM metadata\n                                    # 3. Update status to 'processed'\n\ndocument.generate_embeddings_for_all_content!\n                                    # Generate embeddings only\n\ndocument.generate_metadata!         # Generate LLM metadata only\n\n# Processing validation\ndocument.has_files?                 # Check if content has associated files\ndocument.has_pending_content?       # Check for content awaiting processing\n</code></pre>"},{"location":"api-reference/api-models/#file-handling-methods","title":"File Handling Methods","text":"<pre><code># File association (through content models)\ndocument.has_files?                 # Any content has file data\ndocument.total_file_size            # Sum of all file sizes\ndocument.primary_file_type          # Main file type\n\n# File metadata access\ndocument.file_modified_at           # Source file modification time\ndocument.location                   # Source file path or URL\n\n# Content creation from files\ndocument.content = \"new text\"       # Creates TextContent automatically\n# For images/audio, use specific content models:\ndocument.image_contents.create!(data: image_data, embedding_model: 'clip')\n</code></pre>"},{"location":"api-reference/api-models/#content-methods","title":"Content Methods","text":""},{"location":"api-reference/api-models/#embedding-generation","title":"Embedding Generation","text":"<pre><code># Base Content methods (inherited by all content types)\ncontent.generate_embeddings!        # Generate embeddings for this content\ncontent.should_generate_embeddings? # Check if embeddings needed\ncontent.content_for_embedding       # Text to use for embedding (overrideable)\n\n# TextContent specific\ntext_content.generate_embeddings!   # Chunks text and generates embeddings\ntext_content.chunks                 # Array of content chunks with metadata\ntext_content.chunk_size             # Tokens per chunk\ntext_content.overlap                # Token overlap between chunks\n\n# Embedding management\ncontent.embeddings.count            # Number of embeddings\ncontent.embedding_count             # Alias for count\ncontent.embeddings.destroy_all      # Remove all embeddings\n</code></pre>"},{"location":"api-reference/api-models/#content-validation","title":"Content Validation","text":"<pre><code># Base validations (all content types)\ncontent.valid?                      # ActiveRecord validation\ncontent.errors.full_messages        # Validation error messages\n\n# Content-specific validations\ntext_content.content.present?       # TextContent requires content\nimage_content.data.present?         # ImageContent requires data\n\n# Custom validation methods\ncontent.validate_embedding_model    # Ensure model is supported\ncontent.validate_content_size       # Check content size limits\n</code></pre>"},{"location":"api-reference/api-models/#processing-callbacks","title":"Processing Callbacks","text":"<pre><code># Automatic processing callbacks\n# after_create: Generate embeddings if content is ready\n# after_update: Regenerate embeddings if content changed\n# before_destroy: Clean up associated embeddings\n\n# Manual callback triggering\ncontent.run_callbacks(:create)      # Trigger create callbacks\ncontent.run_callbacks(:update)      # Trigger update callbacks\n\n# Callback status checking\ncontent.embeddings_generated?       # Check if embeddings exist\ncontent.metadata['embeddings_generated_at']  # Generation timestamp\n</code></pre>"},{"location":"api-reference/api-models/#embedding-methods","title":"Embedding Methods","text":""},{"location":"api-reference/api-models/#similarity-search","title":"Similarity Search","text":"<pre><code># Instance-level similarity (compare with other embeddings)\nembedding.similarity_to(other_embedding)     # Cosine similarity score\nembedding.distance_to(other_embedding)       # Distance (1 - similarity)\n\n# Class-level similarity search\nEmbedding.search_similar(\n  query_embedding,\n  limit: 20,\n  threshold: 0.7,\n  filters: {\n    embeddable_type: 'Ragdoll::TextContent',\n    document_type: 'pdf'\n  }\n)\n\n# Specialized search methods\nembedding.find_similar(limit: 10)           # Find similar embeddings\nembedding.find_related_in_document(limit: 5) # Similar chunks in same document\n</code></pre>"},{"location":"api-reference/api-models/#usage-tracking","title":"Usage Tracking","text":"<pre><code># Usage analytics\nembedding.mark_as_used!             # Increment usage_count, update returned_at\nembedding.usage_count               # Number of times used in searches\nembedding.returned_at               # Last usage timestamp\nembedding.last_used_days_ago        # Days since last use\n\n# Usage scoring\nembedding.usage_score               # Calculated usage score for ranking\nembedding.frequency_score           # Frequency-based component\nembedding.recency_score             # Recency-based component\n\n# Usage statistics\nembedding.is_popular?               # usage_count &gt; threshold\nembedding.is_recent?                # used within recent timeframe\nembedding.is_trending?              # increasing usage pattern\n</code></pre>"},{"location":"api-reference/api-models/#analytics-methods","title":"Analytics Methods","text":"<pre><code># Embedding metadata\nembedding.embedding_dimensions      # Vector dimensionality\nembedding.embedding_model           # Model used (via content relationship)\nembedding.chunk_index               # Position within content\n\n# Content access\nembedding.embeddable                # Associated content object\nembedding.document                  # Parent document (through content)\nembedding.content_preview(length: 100)  # Truncated content preview\n\n# Search result formatting\nembedding.to_search_result(similarity: 0.85)\n# Returns formatted hash for search APIs\n\n# Performance metrics\nembedding.vector_magnitude          # Vector magnitude (for normalization)\nembedding.vector_norm               # L2 norm of the vector\nembedding.vector_sparsity           # Percentage of zero values\n</code></pre>"},{"location":"api-reference/api-models/#class-methods","title":"Class Methods","text":""},{"location":"api-reference/api-models/#document-class-methods","title":"Document Class Methods","text":""},{"location":"api-reference/api-models/#scopes-and-query-methods","title":"Scopes and Query Methods","text":"<pre><code># Status-based scopes\nDocument.processed                  # WHERE status = 'processed'\nDocument.pending                    # WHERE status = 'pending'\nDocument.processing                 # WHERE status = 'processing'\nDocument.with_errors                # WHERE status = 'error'\n\n# Content-based scopes\nDocument.by_type('pdf')             # WHERE document_type = 'pdf'\nDocument.multi_modal                # Documents with multiple content types\nDocument.text_only                  # Documents with only text content\nDocument.with_content               # Documents that have content models\nDocument.without_content            # Documents missing content models\n\n# Time-based scopes\nDocument.recent                     # ORDER BY created_at DESC\nDocument.created_since(1.week.ago)  # WHERE created_at &gt; ?\nDocument.modified_since(1.day.ago)  # WHERE file_modified_at &gt; ?\n\n# Advanced queries\nDocument.with_embeddings_count      # Includes embedding count\nDocument.by_content_length(min: 1000)  # Filter by content length\nDocument.by_file_size(max: 10.megabytes)  # Filter by file size\n</code></pre>"},{"location":"api-reference/api-models/#search-and-filtering","title":"Search and Filtering","text":"<pre><code># PostgreSQL full-text search\nDocument.search_content(\n  \"machine learning algorithms\",\n  limit: 20\n)\n\n# Faceted search with metadata filters\nDocument.faceted_search(\n  query: \"neural networks\",\n  keywords: [\"deep learning\", \"AI\"],\n  classification: \"research_paper\",\n  tags: [\"computer-science\"],\n  limit: 50\n)\n\n# Hybrid search (semantic + full-text)\nDocument.hybrid_search(\n  \"artificial intelligence applications\",\n  query_embedding: embedding_vector,\n  semantic_weight: 0.7,\n  text_weight: 0.3,\n  limit: 25\n)\n\n# Metadata-based filtering\nDocument.with_classification('technical_manual')\nDocument.with_keywords(['api', 'documentation'])\nDocument.with_tags(['development', 'guide'])\nDocument.by_metadata_field('complexity', 'advanced')\n</code></pre>"},{"location":"api-reference/api-models/#statistics-and-analytics","title":"Statistics and Analytics","text":"<pre><code># Comprehensive statistics\nDocument.stats\n# Returns:\n# {\n#   total_documents: 1250,\n#   by_status: { processed: 1100, pending: 50, processing: 75, error: 25 },\n#   by_type: { pdf: 600, docx: 300, text: 200, image: 100, mixed: 50 },\n#   multi_modal_documents: 75,\n#   total_text_contents: 1000,\n#   total_image_contents: 125,\n#   total_audio_contents: 25,\n#   total_embeddings: { text: 15000, image: 500, audio: 100 },\n#   storage_type: \"activerecord_polymorphic\"\n# }\n\n# Usage analytics\nDocument.popular(limit: 10)         # Most searched documents\nDocument.trending(timeframe: 1.week) # Recently popular documents\nDocument.usage_summary(period: 1.month)  # Usage statistics\n\n# Content analysis\nDocument.average_word_count          # Average words per document\nDocument.total_storage_size          # Total storage used\nDocument.embedding_coverage          # Percentage with embeddings\n\n# Performance metrics\nDocument.processing_time_stats       # Processing time statistics\nDocument.error_rate(period: 1.day)   # Error rate percentage\nDocument.throughput_stats            # Documents processed per hour\n</code></pre>"},{"location":"api-reference/api-models/#batch-operations","title":"Batch Operations","text":"<pre><code># Batch processing\nDocument.process_pending!            # Process all pending documents\nDocument.regenerate_embeddings!(model: 'text-embedding-3-large')\nDocument.bulk_update_metadata(classification: 'archived')\n\n# Batch import\nDocument.import_from_directory(\n  '/path/to/documents',\n  file_patterns: ['*.pdf', '*.docx'],\n  recursive: true,\n  batch_size: 100\n)\n\n# Batch cleanup\nDocument.cleanup_orphaned_content!   # Remove content without documents\nDocument.remove_old_embeddings!(older_than: 6.months)\nDocument.vacuum_unused_storage!      # Cleanup unused file storage\n</code></pre>"},{"location":"api-reference/api-models/#content-class-methods","title":"Content Class Methods","text":""},{"location":"api-reference/api-models/#content-type-specific-queries","title":"Content-Type Specific Queries","text":"<pre><code># Base Content class methods\nContent.by_type('TextContent')       # Filter by STI type\nContent.with_embeddings              # Content that has embeddings\nContent.without_embeddings           # Content missing embeddings\nContent.by_embedding_model('text-embedding-3-large')\n\n# TextContent specific\nTextContent.by_word_count(min: 500, max: 5000)\nTextContent.by_character_count(min: 2000)\nTextContent.with_long_content        # Content over threshold\nTextContent.recently_processed       # Recently generated embeddings\n\n# ImageContent specific\nImageContent.by_dimensions(min_width: 800, min_height: 600)\nImageContent.by_file_size(max: 5.megabytes)\nImageContent.with_descriptions       # Has AI-generated descriptions\nImageContent.by_format(['jpg', 'png'])\n\n# AudioContent specific (planned)\nAudioContent.by_duration(min: 30.seconds, max: 10.minutes)\nAudioContent.by_sample_rate(44100)\nAudioContent.with_transcripts        # Has speech-to-text transcripts\n</code></pre>"},{"location":"api-reference/api-models/#content-statistics","title":"Content Statistics","text":"<pre><code># TextContent statistics\nTextContent.stats\n# Returns:\n# {\n#   total_text_contents: 1000,\n#   by_model: { 'text-embedding-3-large': 600, 'text-embedding-3-small': 400 },\n#   total_embeddings: 15000,\n#   average_word_count: 1250,\n#   average_chunk_size: 1000\n# }\n\n# Processing statistics\nContent.processing_stats             # Embedding generation statistics\nContent.model_usage_stats            # Usage by embedding model\nContent.error_rate_by_type           # Error rates by content type\n</code></pre>"},{"location":"api-reference/api-models/#embedding-class-methods","title":"Embedding Class Methods","text":""},{"location":"api-reference/api-models/#advanced-search-methods","title":"Advanced Search Methods","text":"<pre><code># Vector similarity search with filters\nEmbedding.search_similar(\n  query_embedding,\n  limit: 20,\n  threshold: 0.75,\n  filters: {\n    embeddable_type: 'Ragdoll::TextContent',\n    embedding_model: 'text-embedding-3-large',\n    document_type: 'pdf',\n    created_after: 1.month.ago\n  }\n)\n\n# Batch similarity search\nEmbedding.batch_search_similar(\n  [embedding1, embedding2, embedding3],\n  limit: 10,\n  aggregate_results: true\n)\n\n# Specialized search methods\nEmbedding.find_duplicates(threshold: 0.95)  # Near-duplicate detection\nEmbedding.find_outliers(threshold: 0.3)     # Low-similarity outliers\nEmbedding.cluster_similar(max_clusters: 10) # K-means clustering\n</code></pre>"},{"location":"api-reference/api-models/#usage-analytics","title":"Usage Analytics","text":"<pre><code># Usage tracking\nEmbedding.most_used(limit: 100)     # Highest usage_count\nEmbedding.recently_used(since: 1.hour.ago)\nEmbedding.trending(period: 1.day)   # Increasing usage pattern\nEmbedding.popular_content_types     # Usage by content type\n\n# Performance analytics\nEmbedding.search_performance_stats  # Search timing statistics\nEmbedding.model_performance_comparison  # Compare model effectiveness\nEmbedding.quality_metrics           # Embedding quality assessment\n\n# Cache optimization\nEmbedding.precompute_popular!       # Cache popular embeddings\nEmbedding.optimize_indexes!         # Rebuild vector indexes\n</code></pre>"},{"location":"api-reference/api-models/#batch-operations_1","title":"Batch Operations","text":"<pre><code># Batch embedding operations\nEmbedding.regenerate_for_model!(\n  old_model: 'text-embedding-ada-002',\n  new_model: 'text-embedding-3-large'\n)\n\nEmbedding.update_usage_analytics!   # Recalculate usage scores\nEmbedding.cleanup_orphaned!         # Remove embeddings without content\nEmbedding.normalize_vectors!        # L2 normalize all vectors\n\n# Database maintenance\nEmbedding.rebuild_vector_indexes!   # Rebuild pgvector indexes\nEmbedding.vacuum_embeddings_table!  # PostgreSQL VACUUM operation\nEmbedding.analyze_vector_distribution!  # Update query planner statistics\n</code></pre>"},{"location":"api-reference/api-models/#model-validations","title":"Model Validations","text":"<p>Ragdoll implements comprehensive validation rules to ensure data integrity:</p>"},{"location":"api-reference/api-models/#document-model-validations","title":"Document Model Validations","text":""},{"location":"api-reference/api-models/#required-fields","title":"Required Fields","text":"<pre><code>class Document &lt; ActiveRecord::Base\n  validates :location, presence: true\n  validates :title, presence: true\n  validates :document_type, presence: true\n  validates :status, presence: true\n  validates :file_modified_at, presence: true\nend\n</code></pre>"},{"location":"api-reference/api-models/#format-validations","title":"Format Validations","text":"<pre><code># Document type validation\nvalidates :document_type, \n  inclusion: { \n    in: %w[text image audio pdf docx html markdown mixed],\n    message: \"must be a valid document type\"\n  }\n\n# Status validation\nvalidates :status,\n  inclusion: {\n    in: %w[pending processing processed error],\n    message: \"must be a valid processing status\"\n  }\n\n# Location format validation\nvalidates :location, format: {\n  with: /\\A(https?:\\/\\/|\\/).*\\z/,\n  message: \"must be a valid URL or absolute file path\"\n}\n\n# Metadata JSON validation\nvalidate :validate_metadata_structure\n\nprivate\n\ndef validate_metadata_structure\n  return unless metadata.present?\n\n  # Validate metadata against document type schema\n  schema_errors = MetadataSchemas.validate_metadata(document_type, metadata)\n  schema_errors.each { |error| errors.add(:metadata, error) }\nend\n</code></pre>"},{"location":"api-reference/api-models/#custom-validators","title":"Custom Validators","text":"<pre><code># Custom location validator\nvalidate :validate_location_accessibility\n\ndef validate_location_accessibility\n  return unless location.present?\n\n  # For file paths, check if file exists and is readable\n  if location.start_with?('/')\n    unless File.exist?(location) &amp;&amp; File.readable?(location)\n      errors.add(:location, \"file does not exist or is not readable\")\n    end\n  end\n\n  # For URLs, validate format more strictly\n  if location.start_with?('http')\n    begin\n      uri = URI.parse(location)\n      unless uri.is_a?(URI::HTTP) || uri.is_a?(URI::HTTPS)\n        errors.add(:location, \"must be a valid HTTP or HTTPS URL\")\n      end\n    rescue URI::InvalidURIError\n      errors.add(:location, \"is not a valid URL\")\n    end\n  end\nend\n\n# File size validation\nvalidate :validate_reasonable_file_size\n\ndef validate_reasonable_file_size\n  if location.present? &amp;&amp; File.exist?(location)\n    file_size = File.size(location)\n    max_size = 100.megabytes  # Configurable limit\n\n    if file_size &gt; max_size\n      errors.add(:location, \"file size (#{file_size} bytes) exceeds maximum (#{max_size} bytes)\")\n    end\n  end\nend\n</code></pre>"},{"location":"api-reference/api-models/#content-model-validations","title":"Content Model Validations","text":""},{"location":"api-reference/api-models/#base-content-validations","title":"Base Content Validations","text":"<pre><code>class Content &lt; ActiveRecord::Base\n  validates :type, presence: true\n  validates :embedding_model, presence: true\n  validates :document_id, presence: true\n\n  # Ensure valid STI type\n  validates :type, inclusion: {\n    in: %w[\n      Ragdoll::TextContent\n      Ragdoll::ImageContent\n      Ragdoll::AudioContent\n    ],\n    message: \"must be a valid content type\"\n  }\n\n  # Validate embedding model exists\n  validate :validate_embedding_model_exists\n\n  private\n\n  def validate_embedding_model_exists\n    return unless embedding_model.present?\n\n    valid_models = Ragdoll.config.embedding_config.keys.map(&amp;:to_s)\n    unless valid_models.include?(embedding_model)\n      errors.add(:embedding_model, \"'#{embedding_model}' is not a configured embedding model\")\n    end\n  end\nend\n</code></pre>"},{"location":"api-reference/api-models/#textcontent-specific-validations","title":"TextContent Specific Validations","text":"<pre><code>class TextContent &lt; Content\n  validates :content, presence: true\n  validates :content, length: {\n    minimum: 10,\n    maximum: 1_000_000,  # 1MB text limit\n    message: \"must be between 10 and 1,000,000 characters\"\n  }\n\n  # Validate chunk configuration\n  validate :validate_chunk_configuration\n\n  private\n\n  def validate_chunk_configuration\n    chunk_size_val = chunk_size\n    overlap_val = overlap\n\n    if chunk_size_val &lt;= 0\n      errors.add(:chunk_size, \"must be greater than 0\")\n    end\n\n    if overlap_val &lt; 0\n      errors.add(:overlap, \"cannot be negative\")\n    end\n\n    if overlap_val &gt;= chunk_size_val\n      errors.add(:overlap, \"must be less than chunk_size\")\n    end\n  end\nend\n</code></pre>"},{"location":"api-reference/api-models/#imagecontent-specific-validations","title":"ImageContent Specific Validations","text":"<pre><code>class ImageContent &lt; Content\n  validates :data, presence: true\n\n  # Validate image metadata\n  validate :validate_image_metadata\n\n  private\n\n  def validate_image_metadata\n    return unless metadata.present?\n\n    # Validate dimensions if present\n    if metadata['width'] &amp;&amp; metadata['height']\n      width = metadata['width'].to_i\n      height = metadata['height'].to_i\n\n      if width &lt;= 0 || height &lt;= 0\n        errors.add(:metadata, \"image dimensions must be positive integers\")\n      end\n\n      # Reasonable size limits\n      if width &gt; 50000 || height &gt; 50000\n        errors.add(:metadata, \"image dimensions are unreasonably large\")\n      end\n    end\n\n    # Validate file format\n    if metadata['file_type']\n      valid_formats = %w[jpg jpeg png gif bmp webp svg ico tiff tif]\n      unless valid_formats.include?(metadata['file_type'].downcase)\n        errors.add(:metadata, \"unsupported image format: #{metadata['file_type']}\")\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"api-reference/api-models/#embedding-model-validations","title":"Embedding Model Validations","text":""},{"location":"api-reference/api-models/#vector-and-content-validations","title":"Vector and Content Validations","text":"<pre><code>class Embedding &lt; ActiveRecord::Base\n  validates :embeddable_id, presence: true\n  validates :embeddable_type, presence: true\n  validates :chunk_index, presence: true\n  validates :content, presence: true\n  validates :embedding_vector, presence: true\n\n  # Unique chunk index per content\n  validates :chunk_index, uniqueness: {\n    scope: [:embeddable_id, :embeddable_type],\n    message: \"must be unique within the same content\"\n  }\n\n  # Vector dimension validation\n  validate :validate_embedding_dimensions\n\n  # Content length validation\n  validates :content, length: {\n    minimum: 1,\n    maximum: 10000,  # Reasonable chunk size limit\n    message: \"must be between 1 and 10,000 characters\"\n  }\n\n  # Usage count validation\n  validates :usage_count, numericality: {\n    greater_than_or_equal_to: 0,\n    message: \"cannot be negative\"\n  }\n\n  private\n\n  def validate_embedding_dimensions\n    return unless embedding_vector.present?\n\n    # Get expected dimensions for the model\n    expected_dimensions = get_expected_dimensions\n    actual_dimensions = embedding_vector.length\n\n    if actual_dimensions != expected_dimensions\n      errors.add(\n        :embedding_vector,\n        \"has #{actual_dimensions} dimensions, expected #{expected_dimensions}\"\n      )\n    end\n\n    # Validate vector values\n    if embedding_vector.any? { |val| !val.is_a?(Numeric) }\n      errors.add(:embedding_vector, \"must contain only numeric values\")\n    end\n\n    # Check for NaN or infinite values\n    if embedding_vector.any? { |val| val.nan? || val.infinite? }\n      errors.add(:embedding_vector, \"cannot contain NaN or infinite values\")\n    end\n  end\n\n  def get_expected_dimensions\n    model_name = embeddable&amp;.embedding_model\n    return 1536 unless model_name  # Default OpenAI dimension\n\n    # Look up dimensions from configuration\n    config = Ragdoll.config.embedding_config\n    config.dig(model_name.to_sym, :dimensions) || 1536\n  end\nend\n</code></pre>"},{"location":"api-reference/api-models/#error-handling","title":"Error Handling","text":""},{"location":"api-reference/api-models/#validation-error-processing","title":"Validation Error Processing","text":"<pre><code># Custom error handling for validation failures\nclass ValidationErrorHandler\n  def self.handle_document_errors(document)\n    return { success: true } if document.valid?\n\n    {\n      success: false,\n      errors: {\n        validation_errors: document.errors.full_messages,\n        field_errors: document.errors.messages,\n        error_count: document.errors.count\n      }\n    }\n  end\n\n  def self.handle_content_errors(content)\n    return { success: true } if content.valid?\n\n    {\n      success: false,\n      content_type: content.class.name,\n      errors: {\n        validation_errors: content.errors.full_messages,\n        field_errors: content.errors.messages,\n        suggested_fixes: generate_fix_suggestions(content.errors)\n      }\n    }\n  end\n\n  private\n\n  def self.generate_fix_suggestions(errors)\n    suggestions = []\n\n    errors.each do |field, messages|\n      case field\n      when :content\n        if messages.any? { |m| m.include?('too short') }\n          suggestions &lt;&lt; \"Ensure content has at least 10 characters\"\n        end\n      when :embedding_model\n        suggestions &lt;&lt; \"Use a configured embedding model: #{available_models.join(', ')}\"\n      when :chunk_size\n        suggestions &lt;&lt; \"Set chunk_size to a positive integer (recommended: 1000)\"\n      end\n    end\n\n    suggestions\n  end\n\n  def self.available_models\n    Ragdoll.config.embedding_config.keys.map(&amp;:to_s)\n  end\nend\n</code></pre>"},{"location":"api-reference/api-models/#validation-callbacks","title":"Validation Callbacks","text":"<pre><code># Before validation callbacks for cleanup\nclass Document &lt; ActiveRecord::Base\n  before_validation :normalize_location\n  before_validation :set_default_file_modified_at\n  before_validation :sanitize_metadata\n\n  private\n\n  def normalize_location\n    return unless location.present?\n\n    # Convert relative paths to absolute paths\n    if location.start_with?('./')\n      self.location = File.expand_path(location)\n    end\n\n    # Normalize URL protocols\n    if location.match?(/^https?:\\/\\//i)\n      self.location = location.downcase.gsub(/^http:/i, 'https:')\n    end\n  end\n\n  def sanitize_metadata\n    return unless metadata.present?\n\n    # Remove nil values and empty strings\n    self.metadata = metadata.reject { |k, v| v.nil? || v == '' }\n\n    # Ensure arrays are actually arrays\n    ['tags', 'keywords'].each do |field|\n      if metadata[field].is_a?(String)\n        metadata[field] = metadata[field].split(',').map(&amp;:strip)\n      end\n    end\n  end\nend\n</code></pre> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"api-reference/api-services/","title":"Services Reference","text":""},{"location":"api-reference/api-services/#business-logic-and-processing-services","title":"Business Logic and Processing Services","text":"<p>Ragdoll implements a comprehensive service layer architecture that handles document processing, content analysis, vector generation, and search operations. All services are designed for production use with PostgreSQL and pgvector.</p>"},{"location":"api-reference/api-services/#service-architecture-overview","title":"Service Architecture Overview","text":"<pre><code>flowchart TD\n    A[Client] --&gt; B[DocumentProcessor]\n    A --&gt; C[SearchEngine]\n    A --&gt; D[DocumentManagement]\n\n    B --&gt; E[EmbeddingService]\n    B --&gt; F[TextGenerationService]\n    B --&gt; G[TextChunker]\n\n    C --&gt; E\n    C --&gt; H[Ragdoll::Embedding]\n\n    D --&gt; I[Ragdoll::Document]\n    D --&gt; J[Ragdoll::Content]\n\n    E --&gt; K[RubyLLM]\n    F --&gt; K\n\n    I --&gt; L[PostgreSQL + pgvector]\n    H --&gt; L\n    J --&gt; L</code></pre>"},{"location":"api-reference/api-services/#core-services","title":"Core Services","text":""},{"location":"api-reference/api-services/#documentprocessor","title":"DocumentProcessor","text":"<p>Responsibility: Multi-format document parsing and content extraction with metadata</p> <p>Implementation: <code>/lib/ragdoll/core/document_processor.rb</code></p> <pre><code>class DocumentProcessor\n  class ParseError &lt; DocumentError; end\n  class UnsupportedFormatError &lt; ParseError; end\n\n  # Parse document from file path\n  def self.parse(file_path)\n    new(file_path).parse\n    # Returns: { content:, metadata:, document_type: }\n  end\n\n  # Parse from Shrine attached file\n  def self.parse_attachment(attached_file)\n    attached_file.open { |tempfile| new(tempfile.path, attached_file).parse }\n  end\n\n  # Create document with file processing\n  def self.create_document_from_file(file_path, **options)\n    # Returns: Document model instance with content extracted\n  end\nend\n</code></pre> <p>Supported Formats: - PDF: Uses <code>pdf-reader</code> gem for text extraction and metadata - DOCX: Uses <code>docx</code> gem for Office document processing - HTML: Strip tags and extract clean text content - Text/Markdown: Direct file reading with encoding detection - Images: Uses <code>ImageMagick</code> for metadata and AI description via <code>Ragdoll::ImageDescriptionService</code></p> <p>Usage Examples: <pre><code># Basic document parsing\nresult = DocumentProcessor.parse('/path/to/document.pdf')\nputs result[:content]      # Extracted text content\nputs result[:metadata]     # Document properties (title, author, etc.)\nputs result[:document_type] # \"pdf\"\n\n# Error handling\nbegin\n  result = DocumentProcessor.parse('/path/to/corrupted.pdf')\nrescue DocumentProcessor::ParseError =&gt; e\n  puts \"Failed to parse document: #{e.message}\"\nend\n\n# Image processing with AI description\nresult = DocumentProcessor.parse('/path/to/image.jpg')\nputs result[:content]  # AI-generated description\nputs result[:metadata][:width]   # Image dimensions\nputs result[:metadata][:height]  # Image dimensions\n</code></pre></p>"},{"location":"api-reference/api-services/#documentmanagement","title":"DocumentManagement","text":"<p>Responsibility: High-level document CRUD operations and database persistence</p> <p>Implementation: <code>/lib/ragdoll/core/document_management.rb</code></p> <pre><code>class DocumentManagement\n  class &lt;&lt; self\n    # Create document with duplicate detection\n    def add_document(location, content, metadata = {})\n      # Returns: document_id (string)\n    end\n\n    # Retrieve document with full content\n    def get_document(id)\n      # Returns: document hash with content and metadata\n    end\n\n    # Update document properties\n    def update_document(id, **updates)\n      # Returns: updated document hash\n    end\n\n    # Delete document and all associated data\n    def delete_document(id)\n      # Returns: true on success, nil if not found\n    end\n\n    # Paginated document listing\n    def list_documents(options = {})\n      # Returns: array of document hashes\n    end\n\n    # System-wide statistics\n    def get_document_stats\n      # Returns: comprehensive statistics hash\n    end\n\n    # Direct embedding creation (advanced usage)\n    def add_embedding(embeddable_id, chunk_index, embedding_vector, metadata = {})\n      # Returns: embedding_id (string)\n    end\n  end\nend\n</code></pre> <p>Features: - Duplicate Prevention: Detects existing documents by location and modification time - Path Normalization: Converts relative paths to absolute paths automatically - Multi-modal Content: Handles text, image, and audio content via STI models - Background Integration: Triggers embedding generation jobs automatically</p> <p>Usage Examples: <pre><code># Add document with metadata\ndoc_id = DocumentManagement.add_document(\n  '/path/to/document.pdf',\n  'Document content here',\n  {\n    title: 'Research Paper',\n    author: 'John Doe',\n    document_type: 'pdf',\n    tags: ['research', 'ai']\n  }\n)\n\n# Retrieve with full content\ndocument = DocumentManagement.get_document(doc_id)\nputs document[:title]\nputs document[:content]\nputs document[:metadata][:author]\n\n# Update metadata\nDocumentManagement.update_document(doc_id, \n  metadata: { classification: 'academic' }\n)\n\n# Get system statistics\nstats = DocumentManagement.get_document_stats\nputs \"Total documents: #{stats[:total_documents]}\"\nputs \"By type: #{stats[:by_type]}\"\nputs \"Multi-modal: #{stats[:multi_modal_documents]}\"\n</code></pre></p>"},{"location":"api-reference/api-services/#embeddingservice","title":"EmbeddingService","text":"<p>Responsibility: Vector embedding generation using multiple LLM providers</p> <p>Implementation: <code>/lib/ragdoll/core/embedding_service.rb</code></p> <pre><code>class EmbeddingService\n  class EmbeddingError &lt; Error; end\n\n  def initialize(client: nil)\n    @client = client\n    configure_ruby_llm unless @client\n  end\n\n  # Generate single embedding\n  def generate_embedding(text)\n    # Returns: Array of floats (embedding vector) or nil\n  end\n\n  # Batch processing for efficiency\n  def generate_embeddings_batch(texts)\n    # Returns: Array of embedding vectors\n  end\n\n  # Vector similarity calculation\n  def cosine_similarity(embedding1, embedding2)\n    # Returns: Float similarity score (0.0 to 1.0)\n  end\nend\n</code></pre> <p>Provider Support: - OpenAI: <code>text-embedding-3-small</code>, <code>text-embedding-3-large</code> - Anthropic: Via API compatibility - Google: Vertex AI embedding models - Azure OpenAI: Enterprise deployment support - Ollama: Local embedding models - HuggingFace: Custom model deployment</p> <p>Usage Examples: <pre><code># Initialize service\nservice = EmbeddingService.new\n\n# Generate single embedding\nembedding = service.generate_embedding(\"This is sample text\")\nputs \"Embedding dimensions: #{embedding.length}\"\n\n# Batch processing for efficiency\ntexts = [\"Text 1\", \"Text 2\", \"Text 3\"]\nembeddings = service.generate_embeddings_batch(texts)\nputs \"Generated #{embeddings.length} embeddings\"\n\n# Calculate similarity\nsimilarity = service.cosine_similarity(embedding1, embedding2)\nputs \"Similarity: #{similarity.round(3)}\"\n\n# Error handling\nbegin\n  embedding = service.generate_embedding(\"\")\nrescue EmbeddingService::EmbeddingError =&gt; e\n  puts \"Embedding failed: #{e.message}\"\nend\n</code></pre></p>"},{"location":"api-reference/api-services/#searchengine","title":"SearchEngine","text":"<p>Responsibility: Semantic search operations using pgvector similarity</p> <p>Implementation: <code>/lib/ragdoll/core/search_engine.rb</code></p> <pre><code>class SearchEngine\n  def initialize(embedding_service)\n    @embedding_service = embedding_service\n  end\n\n  # Document-level semantic search\n  def search_documents(query, options = {})\n    # Returns: Array of document results with similarity scores\n  end\n\n  # Content chunk-level search\n  def search_similar_content(query_or_embedding, options = {})\n    # Returns: Array of content chunk matches\n  end\nend\n</code></pre> <p>Search Options: - <code>limit</code>: Maximum results (default: configured max_results) - <code>threshold</code>: Minimum similarity score (default: configured threshold)  - <code>filters</code>: Document type, embedding model, date range filters - <code>embeddable_type</code>: Filter by content type (text, image, audio)</p> <p>Usage Examples: <pre><code># Initialize with embedding service\nembedding_service = EmbeddingService.new\nsearch_engine = SearchEngine.new(embedding_service)\n\n# Basic semantic search\nresults = search_engine.search_documents(\n  \"machine learning algorithms\",\n  limit: 10,\n  threshold: 0.7\n)\n\nresults.each do |result|\n  puts \"Document: #{result[:document_title]}\"\n  puts \"Similarity: #{result[:similarity].round(3)}\"\n  puts \"Content: #{result[:content][0..200]}...\"\nend\n\n# Search with filters\nresults = search_engine.search_similar_content(\n  \"neural networks\",\n  filters: {\n    document_type: 'pdf',\n    embeddable_type: 'Ragdoll::TextContent'\n  }\n)\n\n# Direct embedding search\nquery_embedding = embedding_service.generate_embedding(\"query text\")\nresults = search_engine.search_similar_content(query_embedding)\n</code></pre></p>"},{"location":"api-reference/api-services/#textgenerationservice","title":"TextGenerationService","text":"<p>Responsibility: LLM-powered content analysis including summarization and keyword extraction</p> <p>Implementation: <code>/lib/ragdoll/core/text_generation_service.rb</code></p> <pre><code>class TextGenerationService\n  class GenerationError &lt; StandardError; end\n\n  def initialize(client: nil)\n    @configuration = Ragdoll.config\n    @client = client\n    configure_ruby_llm_if_possible unless @client\n  end\n\n  # Generate document summary\n  def generate_summary(text, max_length: nil)\n    # Returns: String summary or fallback extraction\n  end\n\n  # Extract important keywords\n  def extract_keywords(text, max_keywords: 20)\n    # Returns: Array of keyword strings\n  end\nend\n</code></pre> <p>Features: - Intelligent Fallbacks: Uses rule-based extraction when LLM APIs fail - Configurable Models: Supports different models for summaries vs keywords - Content-Length Aware: Skips processing for very short content - Error Recovery: Graceful degradation with informative logging</p> <p>Usage Examples: <pre><code># Initialize service\nservice = TextGenerationService.new\n\n# Generate summary\nlong_text = File.read('long_document.txt')\nsummary = service.generate_summary(long_text, max_length: 300)\nputs \"Summary: #{summary}\"\n\n# Extract keywords\nkeywords = service.extract_keywords(long_text, max_keywords: 15)\nputs \"Keywords: #{keywords.join(', ')}\"\n\n# Handle configuration-disabled scenarios\nRagdoll::Core.configure do |config|\n  config.summarization_config[:enable] = false\nend\n\n# Will use fallback extraction\nsummary = service.generate_summary(text)  # Returns first 500 chars\n</code></pre></p>"},{"location":"api-reference/api-services/#textchunker","title":"TextChunker","text":"<p>Responsibility: Intelligent text segmentation for optimal embedding generation</p> <p>Implementation: <code>/lib/ragdoll/core/text_chunker.rb</code></p> <pre><code>class TextChunker\n  # Basic chunking with overlap\n  def self.chunk(text, chunk_size: 1000, chunk_overlap: 200)\n    # Returns: Array of text chunks with metadata\n  end\n\n  # Structure-aware chunking (preserves paragraphs, sections)\n  def self.chunk_by_structure(text, options = {})\n    # Returns: Chunks respecting document structure\n  end\n\n  # Programming language-aware chunking\n  def self.chunk_code(text, language: nil)\n    # Returns: Chunks respecting code boundaries (functions, classes)\n  end\nend\n</code></pre> <p>Chunking Strategies: - Token-based: Splits based on approximate token counts - Sentence-aware: Preserves sentence boundaries - Paragraph-aware: Maintains paragraph structure - Code-aware: Respects programming language syntax</p> <p>Usage Examples: <pre><code># Basic chunking\ntext = File.read('large_document.txt')\nchunks = TextChunker.chunk(text, chunk_size: 1500, chunk_overlap: 300)\n\nchunks.each_with_index do |chunk, index|\n  puts \"Chunk #{index + 1}: #{chunk.length} characters\"\nend\n\n# Structure-aware chunking\nchunks = TextChunker.chunk_by_structure(text, \n  preserve_paragraphs: true,\n  min_chunk_size: 500\n)\n\n# Code chunking\ncode_text = File.read('source_code.rb')\ncode_chunks = TextChunker.chunk_code(code_text, language: 'ruby')\n</code></pre></p>"},{"location":"api-reference/api-services/#specialized-services","title":"Specialized Services","text":""},{"location":"api-reference/api-services/#metadatagenerator","title":"MetadataGenerator","text":"<p>Responsibility: AI-powered structured metadata creation</p> <p>Implementation: <code>/lib/ragdoll/core/services/metadata_generator.rb</code></p> <pre><code>class Ragdoll::MetadataGenerator\n  def generate_for_document(document)\n    # Returns: Hash of structured metadata following document-type schemas\n  end\n\n  private\n\n  def generate_text_metadata(content)\n    # Generates classification, topics, difficulty_level, etc.\n  end\n\n  def generate_image_metadata(description, image_metadata)\n    # Generates scene_type, objects, colors, mood, etc.\n  end\nend\n</code></pre> <p>Metadata Schemas: - Text Documents: classification, topics, difficulty_level, audience, language - Images: scene_type, objects, colors, mood, artistic_style - PDFs: academic_level, document_structure, reference_count - Code: programming_language, complexity, patterns</p>"},{"location":"api-reference/api-services/#imagedescriptionservice","title":"ImageDescriptionService","text":"<p>Responsibility: AI-powered image content description</p> <p>Implementation: <code>/lib/ragdoll/core/services/image_description_service.rb</code></p> <pre><code>class Ragdoll::ImageDescriptionService\n  def generate_description(image_path)\n    # Returns: String description of image content for embedding\n  end\nend\n</code></pre>"},{"location":"api-reference/api-services/#service-configuration","title":"Service Configuration","text":""},{"location":"api-reference/api-services/#provider-selection-and-setup","title":"Provider Selection and Setup","text":"<pre><code>Ragdoll::Core.configure do |config|\n  # LLM Provider Configuration\n  config.ruby_llm_config[:openai][:api_key] = ENV['OPENAI_API_KEY']\n  config.ruby_llm_config[:anthropic][:api_key] = ENV['ANTHROPIC_API_KEY']\n\n  # Model Selection by Task\n  config.models[:default] = 'openai/gpt-4o-mini'    # General purpose\n  config.models[:summary] = 'openai/gpt-4o'         # High-quality summarization\n  config.models[:keywords] = 'openai/gpt-4o-mini'   # Fast keyword extraction\n\n  # Embedding Models by Content Type\n  config.models[:embedding][:text] = 'text-embedding-3-small'\n  config.models[:embedding][:image] = 'clip-vit-large-patch14'\n  config.models[:embedding][:audio] = 'whisper-embedding-v1'\n\n  # Performance Tuning\n  config.embedding_config[:cache_embeddings] = true\n  config.embedding_config[:max_embedding_dimensions] = 3072\n\n  # Chunking Configuration  \n  config.chunking[:text][:max_tokens] = 1000\n  config.chunking[:text][:overlap] = 200\nend\n</code></pre>"},{"location":"api-reference/api-services/#error-handling-strategies","title":"Error Handling Strategies","text":"<pre><code># Service-level error handling\nbegin\n  result = DocumentProcessor.parse(file_path)\nrescue DocumentProcessor::UnsupportedFormatError =&gt; e\n  # Handle unsupported file format\n  logger.warn \"Unsupported format: #{e.message}\"\n  fallback_to_text_extraction(file_path)\nrescue DocumentProcessor::ParseError =&gt; e\n  # Handle parsing errors\n  logger.error \"Parse failed: #{e.message}\"\n  create_error_document(file_path, e.message)\nend\n\n# Embedding service with retries\nservice = EmbeddingService.new\nbegin\n  embedding = service.generate_embedding(text)\nrescue EmbeddingService::EmbeddingError =&gt; e\n  if e.message.include?('rate limit')\n    sleep(2)\n    retry\n  else\n    raise e\n  end\nend\n</code></pre>"},{"location":"api-reference/api-services/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api-reference/api-services/#service-composition","title":"Service Composition","text":"<pre><code># Complete document processing pipeline\nclass DocumentPipeline\n  def self.process_document(file_path)\n    # 1. Parse document\n    parsed = DocumentProcessor.parse(file_path)\n\n    # 2. Create database record\n    doc_id = DocumentManagement.add_document(\n      file_path, \n      parsed[:content], \n      parsed[:metadata]\n    )\n\n    # 3. Generate embeddings (via background job)\n    document = Ragdoll::Document.find(doc_id)\n    document.generate_embeddings_for_all_content!\n\n    # 4. Generate metadata (via background job)\n    document.generate_metadata!\n\n    doc_id\n  end\nend\n</code></pre>"},{"location":"api-reference/api-services/#service-testing-patterns","title":"Service Testing Patterns","text":"<pre><code># Mock LLM services for testing\nclass MockEmbeddingService &lt; EmbeddingService\n  def initialize\n    @client = MockLLMClient.new\n  end\n\n  def generate_embedding(text)\n    # Return consistent mock embedding for testing\n    Array.new(1536) { rand }\n  end\nend\n\n# Test with mock service\nRSpec.describe DocumentProcessor do\n  let(:embedding_service) { MockEmbeddingService.new }\n  let(:search_engine) { SearchEngine.new(embedding_service) }\n\n  it 'processes documents correctly' do\n    result = DocumentProcessor.parse('test/fixtures/sample.pdf')\n    expect(result[:content]).to be_present\n    expect(result[:document_type]).to eq('pdf')\n  end\nend\n</code></pre>"},{"location":"api-reference/api-services/#background-job-integration","title":"Background Job Integration","text":"<pre><code># Services work seamlessly with ActiveJob\nclass GenerateEmbeddings &lt; ActiveJob::Base\n  def perform(document_id)\n    document = Ragdoll::Document.find(document_id)\n\n    # Use services within jobs\n    chunker = TextChunker\n    embedding_service = EmbeddingService.new\n\n    chunks = chunker.chunk(document.content)\n    embeddings = embedding_service.generate_embeddings_batch(chunks)\n\n    # Store embeddings\n    chunks.zip(embeddings).each_with_index do |(chunk, embedding), index|\n      DocumentManagement.add_embedding(\n        document.id, index, embedding, \n        { content: chunk }\n      )\n    end\n  end\nend\n</code></pre>"},{"location":"api-reference/api-services/#performance-considerations","title":"Performance Considerations","text":""},{"location":"api-reference/api-services/#service-optimization","title":"Service Optimization","text":"<ul> <li>Batch Processing: Use <code>generate_embeddings_batch()</code> for multiple texts</li> <li>Caching: Enable embedding caching for repeated content</li> <li>Connection Pooling: Configure database pools for concurrent operations</li> <li>Background Jobs: Offload expensive operations to background processing</li> <li>Error Recovery: Implement exponential backoff for API rate limits</li> </ul>"},{"location":"api-reference/api-services/#monitoring-service-health","title":"Monitoring Service Health","text":"<pre><code># Service health check\nclass ServiceHealth\n  def self.check_all_services\n    {\n      database: Database.connected?,\n      embedding_api: test_embedding_service,\n      text_generation: test_text_generation,\n      document_parsing: test_document_parsing\n    }\n  end\n\n  private\n\n  def self.test_embedding_service\n    service = EmbeddingService.new\n    service.generate_embedding(\"test\").present?\n  rescue =&gt; e\n    false\n  end\nend\n</code></pre> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"cli/","title":"Ragdoll Command Line Interface","text":"<p>The Ragdoll CLI provides a powerful command-line interface for interacting with your Ragdoll RAG system. This tool allows you to manage documents, perform searches, monitor system health, and configure your setup directly from the terminal.</p>"},{"location":"cli/#overview","title":"Overview","text":"<p>The <code>ragdoll</code> command-line tool is part of the <code>ragdoll-cli</code> gem and provides access to all core Ragdoll functionality without needing to write code or use a web interface.</p>"},{"location":"cli/#key-features","title":"Key Features","text":"<ul> <li>Document Management: Upload, update, and delete documents</li> <li>Search Operations: Perform semantic and hybrid searches</li> <li>System Monitoring: Check health, stats, and status</li> <li>Configuration Management: Set up and manage LLM providers, database connections</li> <li>Bulk Operations: Process multiple files and directories</li> </ul>"},{"location":"cli/#quick-start","title":"Quick Start","text":"<pre><code># Install the CLI\ngem install ragdoll-cli\n\n# Check health\nragdoll health\n\n# Search for content\nragdoll search \"your query here\"\n\n# Upload a document\nragdoll update /path/to/document.pdf\n</code></pre>"},{"location":"cli/#available-commands","title":"Available Commands","text":"<ul> <li><code>config</code> - Manage configuration settings</li> <li><code>delete</code> - Remove documents from the system</li> <li><code>health</code> - Check system health and connectivity</li> <li><code>list</code> - List documents and system information</li> <li><code>search</code> - Perform searches across your content</li> <li><code>stats</code> - Display system statistics</li> <li><code>status</code> - Show current system status</li> <li><code>update</code> - Add or update documents</li> </ul>"},{"location":"cli/#getting-help","title":"Getting Help","text":"<p>Use the <code>--help</code> flag with any command to see detailed usage information:</p> <pre><code>ragdoll --help\nragdoll search --help\n</code></pre>"},{"location":"cli/#next-steps","title":"Next Steps","text":"<ul> <li>Install the CLI - Get the command line tool set up</li> <li>Command Reference - Detailed documentation for all commands</li> <li>Configuration - Set up your environment</li> <li>Examples - Common usage patterns and workflows</li> </ul>"},{"location":"cli/commands/","title":"Command Reference","text":"<p>Comprehensive reference for all Ragdoll CLI commands.</p>"},{"location":"cli/commands/#global-options","title":"Global Options","text":"<p>All commands support these global options:</p> <ul> <li><code>--config PATH</code> - Specify configuration file path</li> <li><code>--verbose</code> - Enable verbose output</li> <li><code>--quiet</code> - Suppress non-error output</li> <li><code>--help</code> - Show help information</li> </ul>"},{"location":"cli/commands/#config","title":"config","text":"<p>Manage Ragdoll configuration settings.</p>"},{"location":"cli/commands/#usage","title":"Usage","text":"<pre><code>ragdoll config &lt;subcommand&gt; [options]\n</code></pre>"},{"location":"cli/commands/#subcommands","title":"Subcommands","text":""},{"location":"cli/commands/#list","title":"list","text":"<p>Display current configuration:</p> <pre><code>ragdoll config list\nragdoll config list --section database\n</code></pre>"},{"location":"cli/commands/#set","title":"set","text":"<p>Set configuration values:</p> <pre><code>ragdoll config set database.url \"postgresql://localhost/ragdoll\"\nragdoll config set llm.provider \"openai\"\n</code></pre>"},{"location":"cli/commands/#get","title":"get","text":"<p>Get specific configuration values:</p> <pre><code>ragdoll config get database.url\nragdoll config get llm.provider\n</code></pre>"},{"location":"cli/commands/#validate","title":"validate","text":"<p>Validate configuration:</p> <pre><code>ragdoll config validate\n</code></pre>"},{"location":"cli/commands/#health","title":"health","text":"<p>Check system health and connectivity.</p>"},{"location":"cli/commands/#usage_1","title":"Usage","text":"<pre><code>ragdoll health [options]\n</code></pre>"},{"location":"cli/commands/#options","title":"Options","text":"<ul> <li><code>--database</code> - Check database connectivity only</li> <li><code>--llm</code> - Check LLM provider connectivity only</li> <li><code>--all</code> - Run all health checks (default)</li> </ul>"},{"location":"cli/commands/#examples","title":"Examples","text":"<pre><code># Full health check\nragdoll health\n\n# Database only\nragdoll health --database\n\n# With verbose output\nragdoll health --verbose\n</code></pre>"},{"location":"cli/commands/#search","title":"search","text":"<p>Perform searches across your content.</p>"},{"location":"cli/commands/#usage_2","title":"Usage","text":"<pre><code>ragdoll search &lt;query&gt; [options]\n</code></pre>"},{"location":"cli/commands/#options_1","title":"Options","text":"<ul> <li><code>--limit N</code> - Number of results to return (default: 10)</li> <li><code>--threshold FLOAT</code> - Similarity threshold (0.0-1.0)</li> <li><code>--format FORMAT</code> - Output format: json, table, simple (default: table)</li> <li><code>--include-metadata</code> - Include document metadata in results</li> </ul>"},{"location":"cli/commands/#examples_1","title":"Examples","text":"<pre><code># Basic search\nragdoll search \"machine learning algorithms\"\n\n# Limit results\nragdoll search \"python\" --limit 5\n\n# JSON output\nragdoll search \"database\" --format json\n\n# With metadata\nragdoll search \"AI\" --include-metadata\n</code></pre>"},{"location":"cli/commands/#update","title":"update","text":"<p>Add or update documents in the system.</p>"},{"location":"cli/commands/#usage_3","title":"Usage","text":"<pre><code>ragdoll update &lt;path&gt; [options]\n</code></pre>"},{"location":"cli/commands/#options_2","title":"Options","text":"<ul> <li><code>--recursive</code> - Process directories recursively</li> <li><code>--force</code> - Force update even if file hasn't changed</li> <li><code>--metadata KEY=VALUE</code> - Add custom metadata</li> <li><code>--extract-images</code> - Extract and process images from documents</li> </ul>"},{"location":"cli/commands/#examples_2","title":"Examples","text":"<pre><code># Single file\nragdoll update document.pdf\n\n# Directory (recursive)\nragdoll update ./documents --recursive\n\n# With metadata\nragdoll update report.pdf --metadata \"author=John Doe\" --metadata \"department=Research\"\n\n# Force update\nragdoll update document.pdf --force\n</code></pre>"},{"location":"cli/commands/#delete","title":"delete","text":"<p>Remove documents from the system.</p>"},{"location":"cli/commands/#usage_4","title":"Usage","text":"<pre><code>ragdoll delete &lt;identifier&gt; [options]\n</code></pre>"},{"location":"cli/commands/#options_3","title":"Options","text":"<ul> <li><code>--by-path</code> - Delete by file path</li> <li><code>--by-id</code> - Delete by document ID</li> <li><code>--confirm</code> - Skip confirmation prompt</li> </ul>"},{"location":"cli/commands/#examples_3","title":"Examples","text":"<pre><code># Delete by path\nragdoll delete /path/to/document.pdf --by-path\n\n# Delete by ID\nragdoll delete 12345 --by-id\n\n# Skip confirmation\nragdoll delete document.pdf --by-path --confirm\n</code></pre>"},{"location":"cli/commands/#list_1","title":"list","text":"<p>List documents and system information.</p>"},{"location":"cli/commands/#usage_5","title":"Usage","text":"<pre><code>ragdoll list [type] [options]\n</code></pre>"},{"location":"cli/commands/#types","title":"Types","text":"<ul> <li><code>documents</code> - List all documents (default)</li> <li><code>providers</code> - List available LLM providers</li> <li><code>embeddings</code> - List embedding models</li> </ul>"},{"location":"cli/commands/#options_4","title":"Options","text":"<ul> <li><code>--limit N</code> - Number of items to show</li> <li><code>--format FORMAT</code> - Output format: table, json, csv</li> <li><code>--filter PATTERN</code> - Filter results by pattern</li> </ul>"},{"location":"cli/commands/#examples_4","title":"Examples","text":"<pre><code># List documents\nragdoll list documents\n\n# List providers\nragdoll list providers\n\n# JSON format\nragdoll list documents --format json\n\n# Filter by pattern\nragdoll list documents --filter \"*.pdf\"\n</code></pre>"},{"location":"cli/commands/#stats","title":"stats","text":"<p>Display system statistics.</p>"},{"location":"cli/commands/#usage_6","title":"Usage","text":"<pre><code>ragdoll stats [options]\n</code></pre>"},{"location":"cli/commands/#options_5","title":"Options","text":"<ul> <li><code>--detailed</code> - Show detailed statistics</li> <li><code>--refresh</code> - Force refresh of cached stats</li> </ul>"},{"location":"cli/commands/#examples_5","title":"Examples","text":"<pre><code># Basic stats\nragdoll stats\n\n# Detailed view\nragdoll stats --detailed\n</code></pre>"},{"location":"cli/commands/#status","title":"status","text":"<p>Show current system status.</p>"},{"location":"cli/commands/#usage_7","title":"Usage","text":"<pre><code>ragdoll status [options]\n</code></pre>"},{"location":"cli/commands/#options_6","title":"Options","text":"<ul> <li><code>--watch</code> - Continuously monitor status</li> <li><code>--interval N</code> - Watch interval in seconds (default: 5)</li> </ul>"},{"location":"cli/commands/#examples_6","title":"Examples","text":"<pre><code># Current status\nragdoll status\n\n# Continuous monitoring\nragdoll status --watch\n\n# Custom interval\nragdoll status --watch --interval 10\n</code></pre>"},{"location":"cli/commands/#exit-codes","title":"Exit Codes","text":"<ul> <li><code>0</code> - Success</li> <li><code>1</code> - General error</li> <li><code>2</code> - Configuration error</li> <li><code>3</code> - Connection error</li> <li><code>4</code> - Not found error</li> </ul>"},{"location":"cli/configuration/","title":"CLI Configuration","text":"<p>Detailed configuration options for the Ragdoll CLI.</p>"},{"location":"cli/configuration/#configuration-sources","title":"Configuration Sources","text":"<p>The CLI loads configuration from multiple sources in this order:</p> <ol> <li>Command-line flags</li> <li>Environment variables</li> <li>Configuration file (<code>~/.ragdoll/config.yml</code>)</li> <li>System defaults</li> </ol>"},{"location":"cli/configuration/#configuration-file","title":"Configuration File","text":""},{"location":"cli/configuration/#location","title":"Location","text":"<p>Default location: <code>~/.ragdoll/config.yml</code></p> <p>Custom location: <pre><code>ragdoll --config /path/to/config.yml &lt;command&gt;\n</code></pre></p>"},{"location":"cli/configuration/#format","title":"Format","text":"<pre><code># Database configuration\ndatabase:\n  url: \"postgresql://user:password@localhost:5432/ragdoll\"\n  pool_size: 10\n  timeout: 30\n\n# LLM Provider configuration\nllm:\n  provider: \"openai\"  # openai, anthropic, ollama, etc.\n  api_key: \"your-api-key\"\n  base_url: \"https://api.openai.com/v1\"  # Optional for custom endpoints\n  model: \"gpt-4\"\n  temperature: 0.7\n  max_tokens: 2048\n\n# Embedding configuration  \nembeddings:\n  provider: \"openai\"\n  model: \"text-embedding-3-small\"\n  dimensions: 1536\n\n# Search configuration\nsearch:\n  default_limit: 10\n  similarity_threshold: 0.7\n  hybrid_search: true\n\n# Logging configuration\nlogging:\n  level: \"info\"  # debug, info, warn, error\n  file: \"~/.ragdoll/logs/ragdoll.log\"\n  max_size: \"10MB\"\n  max_files: 5\n\n# Processing configuration\nprocessing:\n  chunk_size: 1000\n  chunk_overlap: 200\n  extract_images: true\n  ocr_enabled: true\n\n# CLI-specific settings\ncli:\n  output_format: \"table\"  # table, json, csv\n  color_output: true\n  progress_bars: true\n  confirm_destructive: true\n</code></pre>"},{"location":"cli/configuration/#environment-variables","title":"Environment Variables","text":"<p>All configuration options can be set via environment variables using the pattern <code>RAGDOLL_&lt;SECTION&gt;_&lt;KEY&gt;</code>:</p> <pre><code># Database\nexport RAGDOLL_DATABASE_URL=\"postgresql://localhost/ragdoll\"\nexport RAGDOLL_DATABASE_POOL_SIZE=\"10\"\n\n# LLM\nexport RAGDOLL_LLM_PROVIDER=\"openai\"\nexport RAGDOLL_LLM_API_KEY=\"your-key\"\nexport RAGDOLL_LLM_MODEL=\"gpt-4\"\n\n# Embeddings\nexport RAGDOLL_EMBEDDINGS_PROVIDER=\"openai\"\nexport RAGDOLL_EMBEDDINGS_MODEL=\"text-embedding-3-small\"\n\n# Logging\nexport RAGDOLL_LOGGING_LEVEL=\"debug\"\nexport RAGDOLL_LOGGING_FILE=\"/var/log/ragdoll.log\"\n</code></pre>"},{"location":"cli/configuration/#llm-provider-configuration","title":"LLM Provider Configuration","text":""},{"location":"cli/configuration/#openai","title":"OpenAI","text":"<pre><code>llm:\n  provider: \"openai\"\n  api_key: \"sk-...\"\n  model: \"gpt-4\"\n  temperature: 0.7\n</code></pre>"},{"location":"cli/configuration/#anthropic","title":"Anthropic","text":"<pre><code>llm:\n  provider: \"anthropic\"\n  api_key: \"sk-ant-...\"\n  model: \"claude-3-sonnet-20240229\"\n  max_tokens: 4096\n</code></pre>"},{"location":"cli/configuration/#ollama-local","title":"Ollama (Local)","text":"<pre><code>llm:\n  provider: \"ollama\"\n  base_url: \"http://localhost:11434\"\n  model: \"llama2\"\n</code></pre>"},{"location":"cli/configuration/#azure-openai","title":"Azure OpenAI","text":"<pre><code>llm:\n  provider: \"azure\"\n  api_key: \"your-key\"\n  base_url: \"https://your-resource.openai.azure.com\"\n  model: \"gpt-4\"\n  api_version: \"2024-02-15-preview\"\n</code></pre>"},{"location":"cli/configuration/#database-configuration","title":"Database Configuration","text":""},{"location":"cli/configuration/#postgresql","title":"PostgreSQL","text":"<pre><code>database:\n  url: \"postgresql://user:password@host:port/database\"\n  pool_size: 10\n  timeout: 30\n  ssl_mode: \"require\"  # disable, allow, prefer, require\n</code></pre>"},{"location":"cli/configuration/#connection-string-format","title":"Connection String Format","text":"<pre><code>postgresql://[user[:password]@][host][:port][/dbname][?param1=value1&amp;...]\n</code></pre> <p>Common parameters: - <code>sslmode=require</code> - Require SSL connection - <code>application_name=ragdoll-cli</code> - Set application name - <code>connect_timeout=10</code> - Connection timeout in seconds</p>"},{"location":"cli/configuration/#configuration-management","title":"Configuration Management","text":""},{"location":"cli/configuration/#view-configuration","title":"View Configuration","text":"<pre><code># Show all configuration\nragdoll config list\n\n# Show specific section\nragdoll config list --section llm\n\n# Show single value\nragdoll config get llm.provider\n</code></pre>"},{"location":"cli/configuration/#update-configuration","title":"Update Configuration","text":"<pre><code># Set single value\nragdoll config set llm.model \"gpt-4-turbo\"\n\n# Set nested value\nragdoll config set database.pool_size 20\n</code></pre>"},{"location":"cli/configuration/#validate-configuration","title":"Validate Configuration","text":"<pre><code># Validate all settings\nragdoll config validate\n\n# Test connections\nragdoll health\n</code></pre>"},{"location":"cli/configuration/#reset-configuration","title":"Reset Configuration","text":"<pre><code># Reset to defaults\nragdoll config reset\n\n# Reset specific section\nragdoll config reset --section llm\n</code></pre>"},{"location":"cli/configuration/#security-best-practices","title":"Security Best Practices","text":""},{"location":"cli/configuration/#api-keys","title":"API Keys","text":"<ol> <li> <p>Use environment variables for sensitive values:    <pre><code>export RAGDOLL_LLM_API_KEY=\"your-key\"\n</code></pre></p> </li> <li> <p>Restrict file permissions:    <pre><code>chmod 600 ~/.ragdoll/config.yml\n</code></pre></p> </li> <li> <p>Use key management services in production:    <pre><code>export RAGDOLL_LLM_API_KEY=\"$(aws secretsmanager get-secret-value --secret-id ragdoll/api-key --query SecretString --output text)\"\n</code></pre></p> </li> </ol>"},{"location":"cli/configuration/#database-connections","title":"Database Connections","text":"<ol> <li> <p>Use SSL connections:    <pre><code>database:\n  url: \"postgresql://user:pass@host/db?sslmode=require\"\n</code></pre></p> </li> <li> <p>Limit connection privileges:</p> </li> <li>Create dedicated database user</li> <li> <p>Grant minimal required permissions</p> </li> <li> <p>Use connection pooling:    <pre><code>database:\n  pool_size: 10\n  timeout: 30\n</code></pre></p> </li> </ol>"},{"location":"cli/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli/configuration/#configuration-issues","title":"Configuration Issues","text":"<pre><code># Show configuration file location\nragdoll config path\n\n# Validate configuration\nragdoll config validate\n\n# Debug configuration loading\nragdoll --verbose config list\n</code></pre>"},{"location":"cli/configuration/#connection-problems","title":"Connection Problems","text":"<pre><code># Test database connection\nragdoll health --database --verbose\n\n# Test LLM connection\nragdoll health --llm --verbose\n</code></pre>"},{"location":"cli/configuration/#permission-errors","title":"Permission Errors","text":"<pre><code># Check file permissions\nls -la ~/.ragdoll/config.yml\n\n# Fix permissions\nchmod 600 ~/.ragdoll/config.yml\n</code></pre>"},{"location":"cli/examples/","title":"CLI Examples","text":"<p>Common usage patterns and workflows for the Ragdoll CLI.</p>"},{"location":"cli/examples/#getting-started","title":"Getting Started","text":""},{"location":"cli/examples/#initial-setup","title":"Initial Setup","text":"<pre><code># Install the CLI\ngem install ragdoll-cli\n\n# Configure database connection\nragdoll config set database.url \"postgresql://user:pass@localhost/ragdoll\"\n\n# Configure LLM provider\nragdoll config set llm.provider \"openai\"\nragdoll config set llm.api_key \"sk-your-key-here\"\n\n# Test the setup\nragdoll health\n</code></pre>"},{"location":"cli/examples/#first-document-upload","title":"First Document Upload","text":"<pre><code># Upload a single document\nragdoll update ~/Documents/research-paper.pdf\n\n# Upload with metadata\nragdoll update ~/Documents/report.pdf \\\n  --metadata \"author=Jane Doe\" \\\n  --metadata \"department=Research\" \\\n  --metadata \"category=quarterly-report\"\n\n# Upload and extract images\nragdoll update ~/Documents/presentation.pptx --extract-images\n</code></pre>"},{"location":"cli/examples/#document-management","title":"Document Management","text":""},{"location":"cli/examples/#bulk-document-processing","title":"Bulk Document Processing","text":"<pre><code># Process entire directory\nragdoll update ~/Documents/papers --recursive\n\n# Process specific file types\nfind ~/Documents -name \"*.pdf\" -exec ragdoll update {} \\;\n\n# Process with custom metadata based on path\nfor file in ~/Documents/research/*.pdf; do\n  ragdoll update \"$file\" --metadata \"category=research\"\ndone\n</code></pre>"},{"location":"cli/examples/#updating-existing-documents","title":"Updating Existing Documents","text":"<pre><code># Force update (even if file hasn't changed)\nragdoll update document.pdf --force\n\n# Update with new metadata\nragdoll update document.pdf \\\n  --metadata \"status=reviewed\" \\\n  --metadata \"last-updated=$(date)\"\n</code></pre>"},{"location":"cli/examples/#removing-documents","title":"Removing Documents","text":"<pre><code># Delete by file path\nragdoll delete ~/Documents/old-document.pdf --by-path\n\n# Delete by document ID\nragdoll list documents --format json | jq '.[] | select(.title | contains(\"obsolete\")) | .id' | \\\n  xargs -I {} ragdoll delete {} --by-id --confirm\n</code></pre>"},{"location":"cli/examples/#search-operations","title":"Search Operations","text":""},{"location":"cli/examples/#basic-searches","title":"Basic Searches","text":"<pre><code># Simple search\nragdoll search \"machine learning algorithms\"\n\n# Search with result limit\nragdoll search \"Python programming\" --limit 5\n\n# Search with similarity threshold\nragdoll search \"neural networks\" --threshold 0.8\n</code></pre>"},{"location":"cli/examples/#advanced-search-queries","title":"Advanced Search Queries","text":"<pre><code># Search with JSON output for processing\nragdoll search \"data science\" --format json &gt; search-results.json\n\n# Search and extract specific fields\nragdoll search \"AI research\" --format json | \\\n  jq '.[] | {title: .title, score: .similarity_score, path: .file_path}'\n\n# Search with metadata inclusion\nragdoll search \"quarterly report\" --include-metadata --format json | \\\n  jq '.[] | select(.metadata.department == \"Research\")'\n</code></pre>"},{"location":"cli/examples/#search-workflows","title":"Search Workflows","text":"<pre><code># Create a search report\ncat &lt;&lt; 'EOF' &gt; search-report.sh\n#!/bin/bash\nQUERY=\"$1\"\necho \"# Search Results for: $QUERY\"\necho \"Generated: $(date)\"\necho \"\"\nragdoll search \"$QUERY\" --include-metadata --format json | \\\n  jq -r '.[] | \"## \\(.title)\\n**Score:** \\(.similarity_score)\\n**Path:** \\(.file_path)\\n**Summary:** \\(.summary // \"No summary available\")\\n\"'\nEOF\nchmod +x search-report.sh\n\n# Usage\n./search-report.sh \"artificial intelligence\" &gt; ai-search-report.md\n</code></pre>"},{"location":"cli/examples/#system-monitoring","title":"System Monitoring","text":""},{"location":"cli/examples/#health-monitoring","title":"Health Monitoring","text":"<pre><code># Basic health check\nragdoll health\n\n# Detailed health check with timing\ntime ragdoll health --verbose\n\n# Monitor system status continuously\nragdoll status --watch --interval 30\n</code></pre>"},{"location":"cli/examples/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code># Monitor search performance\necho \"machine learning,$(time (ragdoll search \"machine learning\" --limit 1 &gt;/dev/null) 2&gt;&amp;1 | grep real)\"\n\n# Batch performance testing\nfor query in \"AI\" \"ML\" \"data science\" \"algorithms\"; do\n  echo -n \"$query: \"\n  time ragdoll search \"$query\" --limit 5 &gt;/dev/null\ndone 2&gt;&amp;1 | grep real\n</code></pre>"},{"location":"cli/examples/#system-statistics","title":"System Statistics","text":"<pre><code># Basic statistics\nragdoll stats\n\n# Detailed statistics with formatting\nragdoll stats --detailed --format json | jq '{\n  documents: .total_documents,\n  embeddings: .total_embeddings,\n  storage: .storage_size,\n  last_update: .last_updated\n}'\n</code></pre>"},{"location":"cli/examples/#batch-operations","title":"Batch Operations","text":""},{"location":"cli/examples/#processing-multiple-files","title":"Processing Multiple Files","text":"<pre><code># Process files in parallel\nfind ~/Documents -name \"*.pdf\" -print0 | \\\n  xargs -0 -P 4 -I {} ragdoll update {}\n\n# Process with progress tracking\ntotal=$(find ~/Documents -name \"*.pdf\" | wc -l)\ncount=0\nfind ~/Documents -name \"*.pdf\" | while read file; do\n  count=$((count + 1))\n  echo \"Processing $count/$total: $file\"\n  ragdoll update \"$file\"\ndone\n</code></pre>"},{"location":"cli/examples/#bulk-metadata-updates","title":"Bulk Metadata Updates","text":"<pre><code># Add category metadata to all PDFs in research directory\nfind ~/Documents/research -name \"*.pdf\" | while read file; do\n  ragdoll update \"$file\" --metadata \"category=research\"\ndone\n\n# Update metadata based on file location\nfor dir in ~/Documents/*/; do\n  category=$(basename \"$dir\")\n  find \"$dir\" -name \"*.pdf\" | while read file; do\n    ragdoll update \"$file\" --metadata \"category=$category\"\n  done\ndone\n</code></pre>"},{"location":"cli/examples/#integration-examples","title":"Integration Examples","text":""},{"location":"cli/examples/#backup-and-sync","title":"Backup and Sync","text":"<pre><code># Backup document list\nragdoll list documents --format json &gt; documents-backup-$(date +%Y%m%d).json\n\n# Sync documents from backup\njq -r '.[].file_path' documents-backup.json | while read path; do\n  if [ -f \"$path\" ]; then\n    ragdoll update \"$path\"\n  fi\ndone\n</code></pre>"},{"location":"cli/examples/#web-scraping-integration","title":"Web Scraping Integration","text":"<pre><code># Download and process web content\ncurl -s \"https://example.com/article\" | \\\n  html2text &gt; temp-article.txt &amp;&amp; \\\n  ragdoll update temp-article.txt \\\n    --metadata \"source=web\" \\\n    --metadata \"url=https://example.com/article\" &amp;&amp; \\\n  rm temp-article.txt\n</code></pre>"},{"location":"cli/examples/#git-integration","title":"Git Integration","text":"<pre><code># Process files changed in git\ngit diff --name-only HEAD~1 HEAD | \\\n  grep -E '\\.(pdf|docx|txt|md)$' | \\\n  xargs -I {} ragdoll update {}\n\n# Add git metadata\nfor file in $(git ls-files '*.md'); do\n  commit=$(git log -1 --format=\"%H\" -- \"$file\")\n  author=$(git log -1 --format=\"%an\" -- \"$file\")\n  ragdoll update \"$file\" \\\n    --metadata \"git-commit=$commit\" \\\n    --metadata \"git-author=$author\"\ndone\n</code></pre>"},{"location":"cli/examples/#automation-scripts","title":"Automation Scripts","text":""},{"location":"cli/examples/#daily-processing-script","title":"Daily Processing Script","text":"<pre><code>#!/bin/bash\n# daily-ragdoll-sync.sh\n\nLOG_FILE=\"$HOME/.ragdoll/logs/daily-sync.log\"\necho \"$(date): Starting daily Ragdoll sync\" &gt;&gt; \"$LOG_FILE\"\n\n# Check system health\nif ! ragdoll health &gt;/dev/null 2&gt;&amp;1; then\n  echo \"$(date): Health check failed!\" &gt;&gt; \"$LOG_FILE\"\n  exit 1\nfi\n\n# Process new documents\nfind ~/Documents -name \"*.pdf\" -mtime -1 | while read file; do\n  echo \"$(date): Processing $file\" &gt;&gt; \"$LOG_FILE\"\n  ragdoll update \"$file\" 2&gt;&amp;1 | tee -a \"$LOG_FILE\"\ndone\n\n# Clean up old logs\nfind ~/.ragdoll/logs -name \"*.log\" -mtime +7 -delete\n\necho \"$(date): Daily sync completed\" &gt;&gt; \"$LOG_FILE\"\n</code></pre>"},{"location":"cli/examples/#search-api-wrapper","title":"Search API Wrapper","text":"<pre><code>#!/bin/bash\n# ragdoll-api.sh - Simple HTTP API wrapper\n\nPORT=${PORT:-8080}\n\nhandle_search() {\n  local query=$(echo \"$1\" | sed 's/+/ /g')\n  ragdoll search \"$query\" --format json\n}\n\nwhile true; do\n  echo -e \"HTTP/1.1 200 OK\\nContent-Type: application/json\\n\" | nc -l \"$PORT\" -c \"\n    read method path protocol\n    if [[ \\$path =~ ^/search\\\\?q=(.+) ]]; then\n      handle_search \\${BASH_REMATCH[1]}\n    else\n      echo '{\\\"error\\\": \\\"Not found\\\"}'\n    fi\n  \"\ndone\n</code></pre>"},{"location":"cli/examples/#troubleshooting-workflows","title":"Troubleshooting Workflows","text":""},{"location":"cli/examples/#diagnostic-script","title":"Diagnostic Script","text":"<pre><code>#!/bin/bash\n# ragdoll-diagnostics.sh\n\necho \"=== Ragdoll CLI Diagnostics ===\"\necho \"Timestamp: $(date)\"\necho \"CLI Version: $(ragdoll --version)\"\necho \"\"\n\necho \"=== Configuration ===\"\nragdoll config list\necho \"\"\n\necho \"=== Health Check ===\"\nragdoll health --verbose\necho \"\"\n\necho \"=== System Stats ===\"\nragdoll stats --detailed\necho \"\"\n\necho \"=== Recent Log Entries ===\"\ntail -n 20 ~/.ragdoll/logs/ragdoll.log 2&gt;/dev/null || echo \"No log file found\"\n</code></pre>"},{"location":"cli/examples/#performance-benchmark","title":"Performance Benchmark","text":"<pre><code>#!/bin/bash\n# benchmark-ragdoll.sh\n\nQUERIES=(\"artificial intelligence\" \"machine learning\" \"data science\" \"neural networks\" \"algorithms\")\n\necho \"=== Ragdoll Performance Benchmark ===\"\necho \"Timestamp: $(date)\"\necho \"\"\n\nfor query in \"${QUERIES[@]}\"; do\n  echo \"Testing query: '$query'\"\n  time ragdoll search \"$query\" --limit 10 &gt;/dev/null\n  echo \"\"\ndone\n</code></pre> <p>These examples provide a comprehensive foundation for using the Ragdoll CLI effectively in various scenarios and workflows.</p>"},{"location":"cli/installation/","title":"CLI Installation","text":"<p>This guide covers installing and setting up the Ragdoll command-line interface.</p>"},{"location":"cli/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ruby 3.2.0 or higher</li> <li>PostgreSQL with pgvector extension</li> <li>Access to a running Ragdoll system</li> </ul>"},{"location":"cli/installation/#installation","title":"Installation","text":""},{"location":"cli/installation/#install-from-rubygems","title":"Install from RubyGems","text":"<pre><code>gem install ragdoll-cli\n</code></pre>"},{"location":"cli/installation/#install-from-source","title":"Install from Source","text":"<pre><code>git clone https://github.com/MadBomber/ragdoll-cli.git\ncd ragdoll-cli\nbundle install\nrake install\n</code></pre>"},{"location":"cli/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>ragdoll --version\nragdoll health\n</code></pre>"},{"location":"cli/installation/#configuration","title":"Configuration","text":"<p>The CLI requires configuration to connect to your Ragdoll system:</p>"},{"location":"cli/installation/#environment-variables","title":"Environment Variables","text":"<pre><code>export RAGDOLL_DATABASE_URL=\"postgresql://user:pass@localhost/ragdoll\"\nexport RAGDOLL_LLM_PROVIDER=\"openai\"\nexport RAGDOLL_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"cli/installation/#configuration-file","title":"Configuration File","text":"<p>Create <code>~/.ragdoll/config.yml</code>:</p> <pre><code>database:\n  url: \"postgresql://user:pass@localhost/ragdoll\"\n\nllm:\n  provider: \"openai\"\n  api_key: \"your-api-key\"\n  model: \"gpt-4\"\n\nlogging:\n  level: \"info\"\n  file: \"~/.ragdoll/logs/ragdoll.log\"\n</code></pre>"},{"location":"cli/installation/#using-ragdoll-config","title":"Using ragdoll config","text":"<pre><code># Set database URL\nragdoll config set database.url \"postgresql://user:pass@localhost/ragdoll\"\n\n# Set LLM provider\nragdoll config set llm.provider \"openai\"\nragdoll config set llm.api_key \"your-api-key\"\n\n# View current configuration\nragdoll config list\n</code></pre>"},{"location":"cli/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli/installation/#connection-issues","title":"Connection Issues","text":"<pre><code># Test database connectivity\nragdoll health --database\n\n# Test LLM provider connectivity  \nragdoll health --llm\n\n# Verbose output for debugging\nragdoll health --verbose\n</code></pre>"},{"location":"cli/installation/#permission-issues","title":"Permission Issues","text":"<pre><code># Ensure CLI is in PATH\nwhich ragdoll\n\n# Check gem installation\ngem list ragdoll-cli\n</code></pre>"},{"location":"cli/installation/#configuration-problems","title":"Configuration Problems","text":"<pre><code># Show configuration file location\nragdoll config path\n\n# Validate configuration\nragdoll config validate\n\n# Reset to defaults\nragdoll config reset\n</code></pre>"},{"location":"cli/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Commands - Learn about available commands</li> <li>Configuration - Advanced configuration options</li> <li>Examples - Common usage patterns</li> </ul>"},{"location":"deployment/deployment/","title":"Production Deployment Guide","text":"<p>This guide covers deploying Ragdoll in production environments with PostgreSQL + pgvector, focusing on performance, scalability, reliability, and security considerations for enterprise workloads.</p>"},{"location":"deployment/deployment/#overview","title":"Overview","text":"<p>Ragdoll is designed for production deployment with:</p> <ul> <li>PostgreSQL + pgvector: Production-grade vector search capabilities</li> <li>Background Processing: Scalable document processing with ActiveJob</li> <li>Multi-Modal Support: Text, image, and audio content handling</li> <li>Enterprise Features: Comprehensive logging, monitoring, and analytics</li> <li>High Availability: Connection pooling, error handling, and failover support</li> </ul>"},{"location":"deployment/deployment/#infrastructure-requirements","title":"Infrastructure Requirements","text":""},{"location":"deployment/deployment/#minimum-requirements","title":"Minimum Requirements","text":"<pre><code># Production minimum specifications\nDatabase Server:\n  CPU: 4 cores\n  RAM: 8 GB\n  Storage: 100 GB SSD\n  PostgreSQL: 12+ with pgvector extension\n\nApplication Server:\n  CPU: 2 cores  \n  RAM: 4 GB\n  Storage: 20 GB\n\nBackground Workers:\n  CPU: 2 cores per worker\n  RAM: 2 GB per worker\n  Recommended: 2-4 worker processes\n</code></pre>"},{"location":"deployment/deployment/#recommended-production-setup","title":"Recommended Production Setup","text":"<pre><code># High-performance production setup\nDatabase Server:\n  CPU: 8-16 cores\n  RAM: 32-64 GB\n  Storage: 500 GB+ NVMe SSD\n  Network: 10 Gbps\n  PostgreSQL: 14+ with pgvector extension\n\nApplication Servers (2+ instances):\n  CPU: 4-8 cores\n  RAM: 8-16 GB  \n  Storage: 50 GB SSD\n\nBackground Workers (4-8 instances):\n  CPU: 4 cores\n  RAM: 4 GB\n  Specialized by job type (embeddings, processing, analysis)\n\nLoad Balancer:\n  HAProxy, nginx, or cloud load balancer\n  SSL termination\n  Health checks\n\nMonitoring:\n  Prometheus + Grafana\n  Application metrics\n  Database monitoring\n</code></pre>"},{"location":"deployment/deployment/#postgresql-pgvector-setup","title":"PostgreSQL + pgvector Setup","text":""},{"location":"deployment/deployment/#installation","title":"Installation","text":""},{"location":"deployment/deployment/#ubuntudebian","title":"Ubuntu/Debian","text":"<pre><code># Install PostgreSQL 14+\nsudo apt update\nsudo apt install postgresql-14 postgresql-contrib-14\n\n# Install pgvector extension\nsudo apt install postgresql-14-pgvector\n\n# Or compile from source\ngit clone --branch v0.5.1 https://github.com/pgvector/pgvector.git\ncd pgvector\nmake\nsudo make install\n</code></pre>"},{"location":"deployment/deployment/#rhelcentos","title":"RHEL/CentOS","text":"<pre><code># Install PostgreSQL\nsudo dnf install postgresql14-server postgresql14-contrib\n\n# Install pgvector\nsudo dnf install postgresql14-pgvector\n\n# Or compile from source (requires development packages)\nsudo dnf groupinstall \"Development Tools\"\nsudo dnf install postgresql14-devel\ngit clone --branch v0.5.1 https://github.com/pgvector/pgvector.git\ncd pgvector\nmake\nsudo make install\n</code></pre>"},{"location":"deployment/deployment/#docker-setup","title":"Docker Setup","text":"<pre><code># Dockerfile for PostgreSQL + pgvector\nFROM postgres:14\n\n# Install pgvector\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y postgresql-14-pgvector &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n# Custom initialization\nCOPY init-scripts/ /docker-entrypoint-initdb.d/\n</code></pre>"},{"location":"deployment/deployment/#database-configuration","title":"Database Configuration","text":""},{"location":"deployment/deployment/#postgresqlconf-optimization","title":"postgresql.conf Optimization","text":"<pre><code># postgresql.conf optimizations for Ragdoll\n\n# Memory settings\nshared_buffers = 2GB                    # 25% of system RAM\neffective_cache_size = 6GB              # 75% of system RAM\nwork_mem = 256MB                        # For sorting/hashing operations\nmaintenance_work_mem = 1GB              # For index creation\n\n# Connection settings\nmax_connections = 200\nshared_preload_libraries = 'vector'     # Enable pgvector\n\n# Write performance\nwal_buffers = 64MB\ncheckpoint_completion_target = 0.9\nmax_wal_size = 2GB\nmin_wal_size = 512MB\n\n# Query optimization\ndefault_statistics_target = 1000        # Better query planning\nrandom_page_cost = 1.1                  # SSD optimization\neffective_io_concurrency = 200          # SSD optimization\n\n# Logging\nlog_destination = 'stderr'\nlogging_collector = on\nlog_directory = '/var/log/postgresql'\nlog_filename = 'postgresql-%Y-%m-%d.log'\nlog_min_duration_statement = 1000      # Log slow queries\nlog_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '\n</code></pre>"},{"location":"deployment/deployment/#database-initialization","title":"Database Initialization","text":"<pre><code>-- Create database and user\nCREATE DATABASE ragdoll_production;\nCREATE USER ragdoll_user WITH PASSWORD 'secure_password';\nGRANT ALL PRIVILEGES ON DATABASE ragdoll_production TO ragdoll_user;\n\n-- Connect to ragdoll database\n\\c ragdoll_production\n\n-- Enable pgvector extension\nCREATE EXTENSION IF NOT EXISTS vector;\n\n-- Grant usage to ragdoll user\nGRANT USAGE ON SCHEMA public TO ragdoll_user;\nGRANT CREATE ON SCHEMA public TO ragdoll_user;\n</code></pre>"},{"location":"deployment/deployment/#index-optimization","title":"Index Optimization","text":"<pre><code>-- Optimized indexes for production performance\n\n-- Vector similarity search (IVFFlat)\nCREATE INDEX CONCURRENTLY idx_embeddings_vector_ivfflat \nON ragdoll_embeddings \nUSING ivfflat (embedding_vector vector_cosine_ops) \nWITH (lists = 1000);\n\n-- Usage analytics indexes\nCREATE INDEX CONCURRENTLY idx_embeddings_usage_analytics \nON ragdoll_embeddings (usage_count DESC, returned_at DESC);\n\n-- Polymorphic content indexes\nCREATE INDEX CONCURRENTLY idx_embeddings_content_type \nON ragdoll_embeddings (embeddable_type, embeddable_id);\n\n-- Document status and type indexes\nCREATE INDEX CONCURRENTLY idx_documents_status_type \nON ragdoll_documents (status, document_type);\n\n-- Full-text search indexes\nCREATE INDEX CONCURRENTLY idx_embeddings_content_fulltext \nON ragdoll_embeddings \nUSING GIN (to_tsvector('english', content));\n\n-- Metadata search indexes (JSONB)\nCREATE INDEX CONCURRENTLY idx_documents_metadata_gin \nON ragdoll_documents \nUSING GIN (metadata);\n\n-- Composite indexes for common queries\nCREATE INDEX CONCURRENTLY idx_embeddings_search_composite\nON ragdoll_embeddings (embeddable_type, usage_count DESC, returned_at DESC);\n\n-- Analyze tables for accurate statistics\nANALYZE ragdoll_documents;\nANALYZE ragdoll_embeddings;\nANALYZE ragdoll_text_contents;\nANALYZE ragdoll_image_contents;\nANALYZE ragdoll_audio_contents;\n</code></pre>"},{"location":"deployment/deployment/#application-configuration","title":"Application Configuration","text":""},{"location":"deployment/deployment/#production-configuration","title":"Production Configuration","text":"<pre><code># config/environments/production.rb\nRagdoll::Core.configure do |config|\n  # LLM Provider\n  config.llm_provider = :openai\n  config.openai_api_key = ENV['OPENAI_API_KEY']\n  config.embedding_model = 'text-embedding-3-small'\n  config.summary_model = 'gpt-4'\n  config.keywords_model = 'gpt-3.5-turbo'\n\n  # Production database\n  config.database_config = {\n    adapter: 'postgresql',\n    host: ENV['DATABASE_HOST'] || 'localhost',\n    port: ENV['DATABASE_PORT'] || 5432,\n    database: ENV['DATABASE_NAME'] || 'ragdoll_production',\n    username: ENV['DATABASE_USERNAME'] || 'ragdoll_user',\n    password: ENV['DATABASE_PASSWORD'],\n    pool: ENV.fetch('RAILS_MAX_THREADS', 25).to_i,\n    timeout: 5000,\n    checkout_timeout: 5,\n    reaping_frequency: 10,\n    idle_timeout: 300,\n    auto_migrate: false,  # Handle migrations separately\n\n    # SSL configuration\n    sslmode: ENV['DATABASE_SSL_MODE'] || 'require',\n    sslcert: ENV['DATABASE_SSL_CERT'],\n    sslkey: ENV['DATABASE_SSL_KEY'],\n    sslrootcert: ENV['DATABASE_SSL_CA'],\n\n    # Performance settings\n    prepared_statements: true,\n    advisory_locks: true\n  }\n\n  # Production logging\n  config.log_level = :info\n  config.log_file = ENV['RAGDOLL_LOG_FILE'] || '/var/log/ragdoll/ragdoll.log'\n  config.log_format = :json\n  config.log_rotation = 'daily'\n  config.log_max_size = 100.megabytes\n\n  # Background processing\n  config.enable_background_processing = true\n  config.job_queue_prefix = ENV['RAGDOLL_QUEUE_PREFIX'] || 'ragdoll_production'\n  config.job_timeout = 600.seconds\n  config.max_retry_attempts = 3\n\n  # Performance settings\n  config.chunk_size = 1000\n  config.chunk_overlap = 200\n  config.search_similarity_threshold = 0.75\n  config.max_search_results = 50\n\n  # Enable production features\n  config.enable_usage_analytics = true\n  config.enable_document_summarization = true\n  config.enable_keyword_extraction = true\n  config.enable_performance_monitoring = true\n\n  # Cache settings\n  config.search_cache_ttl = 300.seconds\n  config.embedding_cache_ttl = 3600.seconds\n  config.metadata_cache_ttl = 1800.seconds\n\n  # Security settings\n  config.enable_request_logging = false    # May contain sensitive data\n  config.log_embedding_content = false     # Don't log actual content\n  config.sanitize_error_messages = true\nend\n</code></pre>"},{"location":"deployment/deployment/#environment-variables","title":"Environment Variables","text":"<pre><code># .env.production\n# Database configuration\nDATABASE_HOST=postgres.example.com\nDATABASE_PORT=5432\nDATABASE_NAME=ragdoll_production\nDATABASE_USERNAME=ragdoll_user\nDATABASE_PASSWORD=your_secure_password\nDATABASE_SSL_MODE=require\nDATABASE_POOL=25\n\n# LLM API keys\nOPENAI_API_KEY=sk-your-openai-key\nANTHROPIC_API_KEY=sk-ant-your-anthropic-key\nGOOGLE_API_KEY=your-google-api-key\n\n# Application settings\nRAILS_ENV=production\nRAILS_MAX_THREADS=25\nRAGDOLL_LOG_FILE=/var/log/ragdoll/ragdoll.log\nRAGDOLL_QUEUE_PREFIX=ragdoll_production\n\n# Redis (for Sidekiq)\nREDIS_URL=redis://redis.example.com:6379/0\n\n# Monitoring\nPROMETHEUS_METRICS_PORT=9394\nHEALTH_CHECK_PORT=8080\n\n# File storage\nRAGDOLL_STORAGE_PATH=/var/lib/ragdoll/storage\nRAGDOLL_CACHE_PATH=/var/lib/ragdoll/cache\n</code></pre>"},{"location":"deployment/deployment/#background-processing-setup","title":"Background Processing Setup","text":""},{"location":"deployment/deployment/#sidekiq-configuration","title":"Sidekiq Configuration","text":"<pre><code># config/initializers/sidekiq.rb\nSidekiq.configure_server do |config|\n  config.redis = { url: ENV['REDIS_URL'] || 'redis://localhost:6379/0' }\n\n  # Queue configuration\n  config.queues = %w[\n    embeddings:3\n    processing:2  \n    analysis:1\n    default:1\n  ]\n\n  # Concurrency settings\n  config.concurrency = ENV.fetch('SIDEKIQ_CONCURRENCY', 10).to_i\n\n  # Error handling\n  config.death_handlers &lt;&lt; lambda do |job, exception|\n    # Custom error handling for failed jobs\n    RagdollErrorHandler.handle_job_death(job, exception)\n  end\nend\n\nSidekiq.configure_client do |config|\n  config.redis = { url: ENV['REDIS_URL'] || 'redis://localhost:6379/0' }\nend\n</code></pre>"},{"location":"deployment/deployment/#docker-compose-setup","title":"Docker Compose Setup","text":"<pre><code># docker-compose.production.yml\nversion: '3.8'\n\nservices:\n  postgres:\n    image: pgvector/pgvector:pg14\n    environment:\n      POSTGRES_DB: ragdoll_production\n      POSTGRES_USER: ragdoll_user\n      POSTGRES_PASSWORD: ${DATABASE_PASSWORD}\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n      - ./config/postgresql.conf:/etc/postgresql/postgresql.conf\n    ports:\n      - \"5432:5432\"\n    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n\n  redis:\n    image: redis:7-alpine\n    volumes:\n      - redis_data:/data\n    ports:\n      - \"6379:6379\"\n\n  ragdoll-app:\n    build: .\n    environment:\n      - DATABASE_URL=postgresql://ragdoll_user:${DATABASE_PASSWORD}@postgres:5432/ragdoll_production\n      - REDIS_URL=redis://redis:6379/0\n      - RAILS_ENV=production\n    volumes:\n      - storage_data:/var/lib/ragdoll/storage\n      - log_data:/var/log/ragdoll\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      - postgres\n      - redis\n\n  ragdoll-embeddings:\n    build: .\n    command: bundle exec sidekiq -q embeddings -c 3\n    environment:\n      - DATABASE_URL=postgresql://ragdoll_user:${DATABASE_PASSWORD}@postgres:5432/ragdoll_production\n      - REDIS_URL=redis://redis:6379/0\n      - RAILS_ENV=production\n    volumes:\n      - storage_data:/var/lib/ragdoll/storage\n      - log_data:/var/log/ragdoll\n    depends_on:\n      - postgres\n      - redis\n\n  ragdoll-processing:\n    build: .\n    command: bundle exec sidekiq -q processing -c 2\n    environment:\n      - DATABASE_URL=postgresql://ragdoll_user:${DATABASE_PASSWORD}@postgres:5432/ragdoll_production\n      - REDIS_URL=redis://redis:6379/0\n      - RAILS_ENV=production\n    volumes:\n      - storage_data:/var/lib/ragdoll/storage\n      - log_data:/var/log/ragdoll\n    depends_on:\n      - postgres\n      - redis\n\n  ragdoll-analysis:\n    build: .\n    command: bundle exec sidekiq -q analysis -c 1\n    environment:\n      - DATABASE_URL=postgresql://ragdoll_user:${DATABASE_PASSWORD}@postgres:5432/ragdoll_production\n      - REDIS_URL=redis://redis:6379/0\n      - RAILS_ENV=production\n    volumes:\n      - storage_data:/var/lib/ragdoll/storage\n      - log_data:/var/log/ragdoll\n    depends_on:\n      - postgres\n      - redis\n\nvolumes:\n  postgres_data:\n  redis_data:\n  storage_data:\n  log_data:\n</code></pre>"},{"location":"deployment/deployment/#load-balancing-and-high-availability","title":"Load Balancing and High Availability","text":""},{"location":"deployment/deployment/#nginx-configuration","title":"nginx Configuration","text":"<pre><code># /etc/nginx/sites-available/ragdoll\nupstream ragdoll_app {\n    server app1.example.com:3000 max_fails=3 fail_timeout=30s;\n    server app2.example.com:3000 max_fails=3 fail_timeout=30s;\n    server app3.example.com:3000 max_fails=3 fail_timeout=30s backup;\n}\n\nserver {\n    listen 80;\n    listen 443 ssl http2;\n    server_name ragdoll.example.com;\n\n    # SSL configuration\n    ssl_certificate /etc/ssl/certs/ragdoll.example.com.pem;\n    ssl_certificate_key /etc/ssl/private/ragdoll.example.com.key;\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;\n    ssl_prefer_server_ciphers off;\n\n    # Security headers\n    add_header X-Frame-Options DENY;\n    add_header X-Content-Type-Options nosniff;\n    add_header X-XSS-Protection \"1; mode=block\";\n    add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\";\n\n    # File upload size\n    client_max_body_size 100M;\n\n    # Proxy settings\n    location / {\n        proxy_pass http://ragdoll_app;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\n        # Timeouts\n        proxy_connect_timeout 60s;\n        proxy_send_timeout 60s;\n        proxy_read_timeout 60s;\n\n        # Health checks\n        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;\n    }\n\n    # Health check endpoint\n    location /health {\n        proxy_pass http://ragdoll_app/health;\n        access_log off;\n    }\n\n    # Static files\n    location /assets {\n        root /var/www/ragdoll/public;\n        expires 1y;\n        add_header Cache-Control \"public, immutable\";\n    }\n}\n</code></pre>"},{"location":"deployment/deployment/#health-checks","title":"Health Checks","text":"<pre><code># config/routes.rb\nRails.application.routes.draw do\n  get '/health', to: 'health#check'\n  get '/health/detailed', to: 'health#detailed'\nend\n\n# app/controllers/health_controller.rb\nclass HealthController &lt; ApplicationController\n  def check\n    health_status = Ragdoll::Core.client.health_check\n\n    if health_status[:status] == 'healthy'\n      render json: { status: 'ok', timestamp: Time.current }, status: :ok\n    else\n      render json: { \n        status: 'error', \n        details: health_status, \n        timestamp: Time.current \n      }, status: :service_unavailable\n    end\n  end\n\n  def detailed\n    render json: Ragdoll::Core.client.detailed_health_check\n  end\nend\n</code></pre>"},{"location":"deployment/deployment/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"deployment/deployment/#prometheus-metrics","title":"Prometheus Metrics","text":"<pre><code># config/initializers/prometheus.rb\nrequire 'prometheus/client'\nrequire 'prometheus/client/rack/collector'\nrequire 'prometheus/client/rack/exporter'\n\n# Custom metrics for Ragdoll\nmodule RagdollMetrics\n  PROMETHEUS = Prometheus::Client.registry\n\n  SEARCH_DURATION = PROMETHEUS.histogram(\n    :ragdoll_search_duration_seconds,\n    docstring: 'Time spent on search operations',\n    labels: [:query_type, :content_type]\n  )\n\n  DOCUMENT_PROCESSING = PROMETHEUS.counter(\n    :ragdoll_documents_processed_total,\n    docstring: 'Total number of documents processed',\n    labels: [:document_type, :status]\n  )\n\n  EMBEDDING_GENERATION = PROMETHEUS.histogram(\n    :ragdoll_embedding_generation_seconds,\n    docstring: 'Time spent generating embeddings',\n    labels: [:model, :content_type]\n  )\n\n  BACKGROUND_JOBS = PROMETHEUS.counter(\n    :ragdoll_background_jobs_total,\n    docstring: 'Total background jobs processed',\n    labels: [:job_type, :status]\n  )\nend\n\n# Instrument search operations\nmodule SearchInstrumentation\n  def search_similar_content(query, options = {})\n    start_time = Time.current\n    result = super\n\n    RagdollMetrics::SEARCH_DURATION.observe(\n      Time.current - start_time,\n      labels: { \n        query_type: 'semantic',\n        content_type: options[:content_types]&amp;.join(',') || 'all'\n      }\n    )\n\n    result\n  end\nend\n\nRagdoll::SearchEngine.prepend(SearchInstrumentation)\n</code></pre>"},{"location":"deployment/deployment/#grafana-dashboard","title":"Grafana Dashboard","text":"<pre><code>{\n  \"dashboard\": {\n    \"title\": \"Ragdoll Production Dashboard\",\n    \"panels\": [\n      {\n        \"title\": \"Search Performance\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(ragdoll_search_duration_seconds[5m])\",\n            \"legendFormat\": \"Search Rate\"\n          },\n          {\n            \"expr\": \"histogram_quantile(0.95, ragdoll_search_duration_seconds)\",\n            \"legendFormat\": \"95th Percentile\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Document Processing\",\n        \"type\": \"graph\", \n        \"targets\": [\n          {\n            \"expr\": \"rate(ragdoll_documents_processed_total[5m])\",\n            \"legendFormat\": \"Processing Rate\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Background Jobs\",\n        \"type\": \"singlestat\",\n        \"targets\": [\n          {\n            \"expr\": \"sidekiq_queue_size\",\n            \"legendFormat\": \"Queue Depth\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Database Performance\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"postgresql_stat_database_tup_returned\",\n            \"legendFormat\": \"Rows Returned\"\n          },\n          {\n            \"expr\": \"postgresql_stat_database_tup_fetched\", \n            \"legendFormat\": \"Rows Fetched\"\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"deployment/deployment/#log-aggregation","title":"Log Aggregation","text":"<pre><code># docker-compose.logging.yml\nversion: '3.8'\n\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0\n    environment:\n      - discovery.type=single-node\n      - \"ES_JAVA_OPTS=-Xms2g -Xmx2g\"\n    ports:\n      - \"9200:9200\"\n    volumes:\n      - elasticsearch_data:/usr/share/elasticsearch/data\n\n  logstash:\n    image: docker.elastic.co/logstash/logstash:8.8.0\n    volumes:\n      - ./config/logstash.conf:/usr/share/logstash/pipeline/logstash.conf\n    ports:\n      - \"5044:5044\"\n    depends_on:\n      - elasticsearch\n\n  kibana:\n    image: docker.elastic.co/kibana/kibana:8.8.0\n    ports:\n      - \"5601:5601\"\n    environment:\n      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200\n    depends_on:\n      - elasticsearch\n\nvolumes:\n  elasticsearch_data:\n</code></pre>"},{"location":"deployment/deployment/#security-configuration","title":"Security Configuration","text":""},{"location":"deployment/deployment/#application-security","title":"Application Security","text":"<pre><code># Security configuration\nRagdoll::Core.configure do |config|\n  # API key encryption\n  config.encrypt_api_keys = true\n  config.encryption_key = ENV['RAGDOLL_ENCRYPTION_KEY']\n\n  # Request validation\n  config.validate_request_origin = true\n  config.allowed_origins = ENV['ALLOWED_ORIGINS']&amp;.split(',') || []\n\n  # Content filtering\n  config.content_filtering = {\n    max_file_size: 100.megabytes,\n    allowed_file_types: %w[pdf docx txt html md mp3 wav jpg png],\n    scan_for_malware: true,\n    sanitize_content: true\n  }\n\n  # Access logging\n  config.audit_logging = true\n  config.log_access_patterns = true\n  config.sensitive_data_masking = true\nend\n</code></pre>"},{"location":"deployment/deployment/#database-security","title":"Database Security","text":"<pre><code>-- Row-level security example\nALTER TABLE ragdoll_documents ENABLE ROW LEVEL SECURITY;\n\nCREATE POLICY documents_access_policy ON ragdoll_documents\nFOR ALL TO ragdoll_user\nUSING (\n  -- Add your access control logic here\n  tenant_id = current_setting('app.current_tenant_id')::uuid\n);\n\n-- Audit logging\nCREATE EXTENSION IF NOT EXISTS pgaudit;\nALTER SYSTEM SET pgaudit.log = 'all';\n</code></pre>"},{"location":"deployment/deployment/#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"deployment/deployment/#database-backup","title":"Database Backup","text":"<pre><code>#!/bin/bash\n# backup_ragdoll.sh\n\n# Configuration\nDB_HOST=\"${DATABASE_HOST:-localhost}\"\nDB_NAME=\"${DATABASE_NAME:-ragdoll_production}\"\nDB_USER=\"${DATABASE_USERNAME:-ragdoll_user}\"\nBACKUP_DIR=\"/var/backups/ragdoll\"\nRETENTION_DAYS=30\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR\"\n\n# Generate backup filename\nBACKUP_FILE=\"$BACKUP_DIR/ragdoll_$(date +%Y%m%d_%H%M%S).sql.gz\"\n\n# Create backup\npg_dump -h \"$DB_HOST\" -U \"$DB_USER\" -d \"$DB_NAME\" \\\n  --verbose \\\n  --no-password \\\n  --format=custom \\\n  --compress=9 \\\n  | gzip &gt; \"$BACKUP_FILE\"\n\n# Verify backup\nif [ $? -eq 0 ]; then\n  echo \"Backup completed successfully: $BACKUP_FILE\"\n\n  # Remove old backups\n  find \"$BACKUP_DIR\" -name \"ragdoll_*.sql.gz\" -mtime +$RETENTION_DAYS -delete\nelse\n  echo \"Backup failed!\" &gt;&amp;2\n  exit 1\nfi\n</code></pre>"},{"location":"deployment/deployment/#recovery-procedures","title":"Recovery Procedures","text":"<pre><code>#!/bin/bash\n# restore_ragdoll.sh\n\nBACKUP_FILE=\"$1\"\nDB_HOST=\"${DATABASE_HOST:-localhost}\"\nDB_NAME=\"${DATABASE_NAME:-ragdoll_production}\"\nDB_USER=\"${DATABASE_USERNAME:-ragdoll_user}\"\n\nif [ -z \"$BACKUP_FILE\" ]; then\n  echo \"Usage: $0 &lt;backup_file.sql.gz&gt;\"\n  exit 1\nfi\n\n# Stop application services\ndocker-compose stop ragdoll-app ragdoll-embeddings ragdoll-processing ragdoll-analysis\n\n# Restore database\ngunzip -c \"$BACKUP_FILE\" | pg_restore -h \"$DB_HOST\" -U \"$DB_USER\" -d \"$DB_NAME\" --clean --if-exists\n\n# Restart services\ndocker-compose start ragdoll-app ragdoll-embeddings ragdoll-processing ragdoll-analysis\n\necho \"Database restore completed\"\n</code></pre>"},{"location":"deployment/deployment/#performance-tuning","title":"Performance Tuning","text":""},{"location":"deployment/deployment/#database-optimization","title":"Database Optimization","text":"<pre><code>-- Maintenance tasks\nVACUUM ANALYZE ragdoll_documents;\nVACUUM ANALYZE ragdoll_embeddings;\nREINDEX INDEX CONCURRENTLY idx_embeddings_vector_ivfflat;\n\n-- Statistics update\nANALYZE ragdoll_documents;\nANALYZE ragdoll_embeddings;\n\n-- Index maintenance\nSELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE schemaname = 'public'\nORDER BY idx_scan ASC;\n</code></pre>"},{"location":"deployment/deployment/#application-optimization","title":"Application Optimization","text":"<pre><code># Connection pooling optimization\nRagdoll::Core.configure do |config|\n  config.database_config = {\n    # ... other settings\n    pool: ENV.fetch('DATABASE_POOL', 25).to_i,\n    checkout_timeout: 5,\n    reaping_frequency: 10,\n    idle_timeout: 300,\n\n    # Query optimization\n    prepared_statements: true,\n    statement_timeout: 30000,  # 30 seconds\n    advisory_locks: true\n  }\n\n  # Cache optimization\n  config.search_cache_ttl = 300.seconds\n  config.embedding_cache_ttl = 3600.seconds\n  config.enable_query_cache = true\n\n  # Batch processing optimization\n  config.batch_processing_size = 100\n  config.parallel_processing = true\n  config.worker_concurrency = 4\nend\n</code></pre>"},{"location":"deployment/deployment/#deployment-checklist","title":"Deployment Checklist","text":""},{"location":"deployment/deployment/#pre-deployment","title":"Pre-Deployment","text":"<ul> <li> Database server provisioned with adequate resources</li> <li> pgvector extension installed and tested</li> <li> Application servers configured with proper resource limits</li> <li> Background worker processes planned and configured</li> <li> Load balancer configured with health checks</li> <li> SSL certificates installed and configured</li> <li> Environment variables securely configured</li> <li> Database migrations tested in staging</li> <li> Backup and recovery procedures tested</li> <li> Monitoring and alerting configured</li> </ul>"},{"location":"deployment/deployment/#post-deployment","title":"Post-Deployment","text":"<ul> <li> Health checks passing</li> <li> Database performance metrics within acceptable ranges</li> <li> Background job processing functioning correctly</li> <li> Search operations performing within SLA</li> <li> Log aggregation and monitoring operational</li> <li> Backup procedures validated</li> <li> Security scanning completed</li> <li> Performance baseline established</li> <li> Documentation updated with production specifics</li> <li> Team trained on operational procedures</li> </ul>"},{"location":"deployment/deployment/#ongoing-maintenance","title":"Ongoing Maintenance","text":"<ul> <li> Regular database maintenance (VACUUM, ANALYZE)</li> <li> Index performance monitoring and optimization</li> <li> Log rotation and cleanup</li> <li> Security updates and patches</li> <li> Performance monitoring and tuning</li> <li> Backup validation and recovery testing</li> <li> Capacity planning and scaling decisions</li> <li> Configuration optimization based on usage patterns</li> </ul> <p>This deployment guide provides a comprehensive foundation for running Ragdoll in production environments, ensuring high performance, reliability, and maintainability for enterprise document intelligence applications.</p>"},{"location":"deployment/environment-variables/","title":"Environment Variables Reference","text":"<p>This document provides a comprehensive reference for all environment variables used in the Ragdoll Core system, organized by category with their default values and usage examples.</p>"},{"location":"deployment/environment-variables/#model-configuration","title":"Model Configuration","text":"<p>These environment variables control which LLM models are used for different purposes in the Ragdoll system.</p>"},{"location":"deployment/environment-variables/#text-generation-models","title":"Text Generation Models","text":"Variable Default Value Purpose <code>RAGDOLL_DEFAULT_TEXT_MODEL</code> <code>openai/gpt-4o</code> Default model for general text generation tasks <code>RAGDOLL_SUMMARY_MODEL</code> <code>openai/gpt-4o</code> Model used for document summarization <code>RAGDOLL_KEYWORDS_MODEL</code> <code>openai/gpt-4o</code> Model used for keyword extraction"},{"location":"deployment/environment-variables/#embedding-models","title":"Embedding Models","text":"Variable Default Value Purpose <code>RAGDOLL_TEXT_EMBEDDING_MODEL</code> <code>openai/text-embedding-3-small</code> Model for generating text embeddings <code>RAGDOLL_IMAGE_EMBEDDING_MODEL</code> <code>openai/clip-vit-base-patch32</code> Model for generating image embeddings <code>RAGDOLL_AUDIO_EMBEDDING_MODEL</code> <code>openai/whisper-1</code> Model for generating audio embeddings"},{"location":"deployment/environment-variables/#database-configuration","title":"Database Configuration","text":"<p>These environment variables configure the PostgreSQL database connection.</p> Variable Default Value Purpose <code>RAGDOLL_DATABASE_PASSWORD</code> (none) Password for database authentication <code>POSTGRES_SUPERUSER</code> <code>postgres</code> Superuser for database setup operations <code>POSTGRES_SUPERUSER_PASSWORD</code> (none) Password for the PostgreSQL superuser"},{"location":"deployment/environment-variables/#llm-provider-configuration","title":"LLM Provider Configuration","text":"<p>These environment variables configure access to various LLM providers.</p>"},{"location":"deployment/environment-variables/#openai","title":"OpenAI","text":"Variable Default Value Purpose <code>OPENAI_API_KEY</code> (none) API key for OpenAI services <code>OPENAI_ORGANIZATION</code> (none) OpenAI organization ID <code>OPENAI_PROJECT</code> (none) OpenAI project ID <code>OPENAI_API_BASE</code> (none) Custom OpenAI API base URL"},{"location":"deployment/environment-variables/#anthropic","title":"Anthropic","text":"Variable Default Value Purpose <code>ANTHROPIC_API_KEY</code> (none) API key for Anthropic Claude services"},{"location":"deployment/environment-variables/#google","title":"Google","text":"Variable Default Value Purpose <code>GOOGLE_API_KEY</code> (none) API key for Google AI services <code>GOOGLE_PROJECT_ID</code> (none) Google Cloud project ID <code>GEMINI_API_KEY</code> (none) API key for Google Gemini services"},{"location":"deployment/environment-variables/#azure-openai","title":"Azure OpenAI","text":"Variable Default Value Purpose <code>AZURE_OPENAI_API_KEY</code> (none) API key for Azure OpenAI services <code>AZURE_OPENAI_ENDPOINT</code> (none) Azure OpenAI endpoint URL <code>AZURE_OPENAI_API_VERSION</code> <code>2024-02-01</code> Azure OpenAI API version"},{"location":"deployment/environment-variables/#ollama","title":"Ollama","text":"Variable Default Value Purpose <code>OLLAMA_ENDPOINT</code> <code>http://localhost:11434</code> Ollama server endpoint <code>OLLAMA_API_BASE</code> <code>http://localhost:11434</code> Alternative Ollama API base (used in image service)"},{"location":"deployment/environment-variables/#other-providers","title":"Other Providers","text":"Variable Default Value Purpose <code>HUGGINGFACE_API_KEY</code> (none) API key for Hugging Face services <code>OPENROUTER_API_KEY</code> (none) API key for OpenRouter services <code>DEEPSEEK_API_KEY</code> (none) API key for DeepSeek services <code>BEDROCK_ACCESS_KEY_ID</code> (none) AWS Bedrock access key ID <code>BEDROCK_SECRET_ACCESS_KEY</code> (none) AWS Bedrock secret access key <code>BEDROCK_REGION</code> (none) AWS Bedrock region <code>BEDROCK_SESSION_TOKEN</code> (none) AWS Bedrock session token"},{"location":"deployment/environment-variables/#legacy-environment-variables","title":"Legacy Environment Variables","text":"<p>These environment variables are deprecated but may still be referenced in older code:</p> Variable Status Replacement <code>DATABASE_PASSWORD</code> Deprecated Use <code>RAGDOLL_DATABASE_PASSWORD</code> <code>OPENAI_ORGANIZATION_ID</code> Legacy alias Use <code>OPENAI_ORGANIZATION</code> <code>OPENAI_PROJECT_ID</code> Legacy alias Use <code>OPENAI_PROJECT</code>"},{"location":"deployment/environment-variables/#usage-examples","title":"Usage Examples","text":""},{"location":"deployment/environment-variables/#basic-configuration","title":"Basic Configuration","text":"<pre><code># Required for OpenAI integration\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Database configuration\nexport RAGDOLL_DATABASE_PASSWORD=\"your_password\"\n\n# Optional: Use different models\nexport RAGDOLL_DEFAULT_TEXT_MODEL=\"openai/gpt-3.5-turbo\"\nexport RAGDOLL_TEXT_EMBEDDING_MODEL=\"openai/text-embedding-ada-002\"\n</code></pre>"},{"location":"deployment/environment-variables/#development-setup","title":"Development Setup","text":"<pre><code># Development database\nexport RAGDOLL_DATABASE_PASSWORD=\"dev_password\"\nexport POSTGRES_SUPERUSER=\"postgres\"\nexport POSTGRES_SUPERUSER_PASSWORD=\"postgres\"\n\n# Local Ollama setup\nexport OLLAMA_ENDPOINT=\"http://localhost:11434\"\n</code></pre>"},{"location":"deployment/environment-variables/#production-environment","title":"Production Environment","text":"<pre><code># Production OpenAI configuration\nexport OPENAI_API_KEY=\"sk-prod-...\"\nexport OPENAI_ORGANIZATION=\"org-...\"\nexport OPENAI_PROJECT=\"proj_...\"\n\n# Production database\nexport RAGDOLL_DATABASE_PASSWORD=\"$(cat /secrets/db-password)\"\n\n# Azure OpenAI for enterprise\nexport AZURE_OPENAI_API_KEY=\"...\"\nexport AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com/\"\nexport AZURE_OPENAI_API_VERSION=\"2024-02-01\"\n</code></pre>"},{"location":"deployment/environment-variables/#environment-variable-patterns","title":"Environment Variable Patterns","text":""},{"location":"deployment/environment-variables/#naming-convention","title":"Naming Convention","text":"<ul> <li>All Ragdoll-specific variables use the <code>RAGDOLL_</code> prefix</li> <li>Provider-specific variables follow the provider's standard naming</li> <li>Database variables use the <code>RAGDOLL_DATABASE_</code> prefix for clarity</li> </ul>"},{"location":"deployment/environment-variables/#default-value-handling","title":"Default Value Handling","text":"<p>The codebase uses a consistent pattern for environment variable handling:</p> <p>ENV.fetch pattern: All environment variable access uses <code>ENV.fetch(\"VAR_NAME\", default_value)</code> for consistency and explicit default handling:</p> <ol> <li>Variables with defaults: <code>ENV.fetch(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\")</code></li> <li>Variables without defaults: <code>ENV.fetch(\"OPENAI_API_KEY\", nil)</code> (for secrets that shouldn't have fallback values)</li> </ol> <p>This approach provides: - Explicit defaults: Clear indication of what happens when a variable is unset - Consistent behavior: Same pattern throughout the codebase - Better debugging: Easier to track environment variable usage - Security: Secrets explicitly default to <code>nil</code> rather than having fallback values</p>"},{"location":"deployment/environment-variables/#security-considerations","title":"Security Considerations","text":"<ul> <li>API keys and secrets use <code>ENV.fetch(\"KEY\", nil)</code> to explicitly return <code>nil</code> when unset</li> <li>Database passwords are handled securely with the RAGDOLL_ prefix</li> <li>Never commit actual API keys or passwords to version control</li> <li>Use environment-specific configuration files or secret management systems</li> <li>The proc-based lazy evaluation (<code>-&gt; { ENV.fetch(\"KEY\", nil) }</code>) ensures credentials are only accessed when needed</li> </ul>"},{"location":"deployment/environment-variables/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/environment-variables/#common-issues","title":"Common Issues","text":"<ol> <li>Missing API Key: Ensure the required provider API key is set</li> <li>Database Connection: Verify <code>RAGDOLL_DATABASE_PASSWORD</code> is set correctly</li> <li>Model Not Found: Check that the model name in the environment variable is correct</li> <li>Ollama Connection: Ensure Ollama is running and <code>OLLAMA_ENDPOINT</code> points to the correct URL</li> </ol>"},{"location":"deployment/environment-variables/#debug-environment-variables","title":"Debug Environment Variables","text":"<p>To check which environment variables are currently set:</p> <pre><code># Show all Ragdoll-related variables\nenv | grep RAGDOLL\n\n# Show all LLM provider variables\nenv | grep -E \"(OPENAI|ANTHROPIC|GOOGLE|GEMINI|AZURE|OLLAMA|HUGGINGFACE|OPENROUTER|DEEPSEEK|BEDROCK)\"\n</code></pre>"},{"location":"deployment/environment-variables/#configuration-validation","title":"Configuration Validation","text":"<p>Use the Ragdoll configuration system to validate your environment setup:</p> <pre><code># Check if configuration loads correctly\nconfig = Ragdoll::Core.configuration\nputs \"Configuration loaded successfully!\"\n\n# Test database connection\nRagdoll::Core::Database.setup(config.database_config)\nputs \"Database connection successful!\"\n</code></pre>"},{"location":"deployment/performance/","title":"Performance Tuning","text":""},{"location":"deployment/performance/#optimization-strategies-and-monitoring","title":"Optimization Strategies and Monitoring","text":"<p>Ragdoll is designed for high-performance RAG operations with PostgreSQL and pgvector at its foundation. This guide covers comprehensive optimization strategies for production deployments.</p>"},{"location":"deployment/performance/#key-performance-areas","title":"Key Performance Areas","text":"<ul> <li>Database optimization: PostgreSQL + pgvector tuning</li> <li>Vector search performance: Index strategies and similarity search optimization</li> <li>Memory management: Embedding caching and object lifecycle</li> <li>Background job performance: ActiveJob and queue optimization</li> <li>Application-level optimization: Service layer and API response tuning</li> </ul>"},{"location":"deployment/performance/#database-performance","title":"Database Performance","text":""},{"location":"deployment/performance/#postgresql-pgvector-optimization","title":"PostgreSQL + pgvector Optimization","text":""},{"location":"deployment/performance/#vector-index-configuration","title":"Vector Index Configuration","text":"<pre><code>-- IVFFlat index for embeddings (default in Ragdoll)\nCREATE INDEX CONCURRENTLY ragdoll_embeddings_vector_idx \nON ragdoll_embeddings \nUSING ivfflat (embedding_vector vector_cosine_ops) \nWITH (lists = 100);\n\n-- HNSW index for better performance (PostgreSQL 14+)\nCREATE INDEX CONCURRENTLY ragdoll_embeddings_hnsw_idx \nON ragdoll_embeddings \nUSING hnsw (embedding_vector vector_cosine_ops) \nWITH (m = 16, ef_construction = 64);\n</code></pre>"},{"location":"deployment/performance/#connection-pool-configuration","title":"Connection Pool Configuration","text":"<pre><code># config/database.yml or Ragdoll configuration\nRagdoll::Core.configure do |config|\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_production',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    pool: 20,              # Connection pool size\n    timeout: 5000,         # Connection timeout in ms\n    checkout_timeout: 5,   # Pool checkout timeout\n    reaping_frequency: 10  # Dead connection reaping\n  }\nend\n</code></pre>"},{"location":"deployment/performance/#query-optimization","title":"Query Optimization","text":"<pre><code># Optimize embedding searches with proper scoping\nresults = Ragdoll::Embedding\n  .includes(:embeddable) # Eager load to avoid N+1\n  .nearest_neighbors(:embedding_vector, query_vector, distance: \"cosine\")\n  .limit(50)\n  .where(\"neighbor_distance &lt; ?\", 0.3) # Threshold filtering\n</code></pre>"},{"location":"deployment/performance/#memory-allocation-tuning","title":"Memory Allocation Tuning","text":"<pre><code>-- PostgreSQL configuration (postgresql.conf)\nshared_buffers = 256MB          # 25% of RAM for small systems\neffective_cache_size = 1GB      # Available memory for caching\nwork_mem = 4MB                  # Per-operation memory\nmaintenance_work_mem = 64MB     # Maintenance operations\nmax_connections = 100           # Concurrent connections\n\n-- pgvector specific\nhnsw.ef_search = 40             # HNSW search parameter\n</code></pre>"},{"location":"deployment/performance/#vector-search-optimization","title":"Vector Search Optimization","text":""},{"location":"deployment/performance/#index-strategy-selection","title":"Index Strategy Selection","text":"<pre><code>flowchart TD\n    A[Vector Search Requirements] --&gt; B{Dataset Size}\n    B --&gt;|&lt; 100K vectors| C[IVFFlat Index]\n    B --&gt;|&gt; 100K vectors| D[HNSW Index]\n\n    C --&gt; E[Good recall/performance balance]\n    D --&gt; F[Better performance, higher memory]\n\n    E --&gt; G[lists = sqrt(rows)]\n    F --&gt; H[m=16, ef_construction=64]\n\n    G --&gt; I[Moderate build time]\n    H --&gt; J[Longer build time]</code></pre>"},{"location":"deployment/performance/#similarity-threshold-tuning","title":"Similarity Threshold Tuning","text":"<pre><code># Performance vs accuracy trade-offs\nclass SearchEngine\n  PERFORMANCE_THRESHOLDS = {\n    high_precision: 0.85,   # Fewer, more relevant results\n    balanced: 0.70,         # Default Ragdoll setting\n    high_recall: 0.50       # More results, lower precision\n  }\n\n  def search_optimized(query, mode: :balanced)\n    threshold = PERFORMANCE_THRESHOLDS[mode]\n    search_similar_content(query, threshold: threshold)\n  end\nend\n</code></pre>"},{"location":"deployment/performance/#batch-processing-optimization","title":"Batch Processing Optimization","text":"<pre><code># Efficient batch embedding generation\nclass EmbeddingService\n  def generate_embeddings_batch_optimized(texts, batch_size: 50)\n    texts.each_slice(batch_size).flat_map do |batch|\n      generate_embeddings_batch(batch)\n    end\n  end\nend\n</code></pre>"},{"location":"deployment/performance/#memory-management","title":"Memory Management","text":""},{"location":"deployment/performance/#embedding-cache-strategies","title":"Embedding Cache Strategies","text":"<pre><code># In-memory embedding cache\nclass EmbeddingCache\n  def initialize(max_size: 1000)\n    @cache = {}\n    @max_size = max_size\n    @access_times = {}\n  end\n\n  def get_or_generate(text)\n    key = Digest::SHA256.hexdigest(text)\n\n    if @cache.key?(key)\n      @access_times[key] = Time.current\n      return @cache[key]\n    end\n\n    embedding = generate_embedding(text)\n    store_with_eviction(key, embedding)\n    embedding\n  end\n\n  private\n\n  def store_with_eviction(key, embedding)\n    if @cache.size &gt;= @max_size\n      # Evict least recently used\n      lru_key = @access_times.min_by { |_, time| time }&amp;.first\n      @cache.delete(lru_key)\n      @access_times.delete(lru_key)\n    end\n\n    @cache[key] = embedding\n    @access_times[key] = Time.current\n  end\nend\n</code></pre>"},{"location":"deployment/performance/#object-lifecycle-management","title":"Object Lifecycle Management","text":"<pre><code># Efficient document processing\nclass DocumentProcessor\n  def self.process_large_document(file_path)\n    File.open(file_path, 'r') do |file|\n      file.each_line.lazy\n          .each_slice(1000)  # Process in chunks\n          .each do |chunk|\n            process_chunk(chunk.join)\n            GC.start if chunk.size % 10 == 0  # Periodic GC\n          end\n    end\n  end\nend\n</code></pre>"},{"location":"deployment/performance/#memory-leak-prevention","title":"Memory Leak Prevention","text":"<pre><code># Monitor memory usage in background jobs\nclass GenerateEmbeddings &lt; ActiveJob::Base\n  def perform(document_id)\n    memory_before = get_memory_usage\n\n    # Process document\n    process_document(document_id)\n\n    memory_after = get_memory_usage\n\n    if (memory_after - memory_before) &gt; 100.megabytes\n      Rails.logger.warn \"High memory usage detected: #{memory_after - memory_before} MB\"\n    end\n  end\n\n  private\n\n  def get_memory_usage\n    `ps -o rss= -p #{Process.pid}`.to_i.kilobytes\n  end\nend\n</code></pre>"},{"location":"deployment/performance/#background-job-performance","title":"Background Job Performance","text":""},{"location":"deployment/performance/#queue-configuration","title":"Queue Configuration","text":"<pre><code># Sidekiq configuration for production\nRagdoll::Core.configure do |config|\n  # Use Sidekiq for background processing\n  ActiveJob::Base.queue_adapter = :sidekiq\nend\n\n# config/sidekiq.yml\n:concurrency: 10\n:queues:\n  - [critical, 4]\n  - [embeddings, 3]\n  - [default, 2]\n  - [low, 1]\n</code></pre>"},{"location":"deployment/performance/#worker-scaling-strategies","title":"Worker Scaling Strategies","text":"<pre><code># Dynamic worker scaling based on queue size\nclass WorkerScaler\n  def self.scale_workers\n    queue_size = Sidekiq::Queue.new('embeddings').size\n\n    case queue_size\n    when 0..10\n      target_workers = 2\n    when 11..50\n      target_workers = 5\n    when 51..100\n      target_workers = 10\n    else\n      target_workers = 15\n    end\n\n    adjust_worker_count(target_workers)\n  end\nend\n</code></pre>"},{"location":"deployment/performance/#batch-processing-techniques","title":"Batch Processing Techniques","text":"<pre><code># Efficient batch job processing\nclass BatchEmbeddingJob &lt; ActiveJob::Base\n  def perform(document_ids)\n    document_ids.each_slice(10) do |batch|\n      process_batch(batch)\n\n      # Yield control to other jobs\n      sleep(0.1) if batch.size == 10\n    end\n  end\n\n  private\n\n  def process_batch(document_ids)\n    documents = Document.where(id: document_ids).includes(:contents)\n\n    # Batch generate embeddings\n    contents = documents.flat_map(&amp;:contents)\n    texts = contents.map(&amp;:content)\n    embeddings = EmbeddingService.new.generate_embeddings_batch(texts)\n\n    # Bulk insert embeddings\n    embedding_data = contents.zip(embeddings).map do |content, embedding|\n      {\n        embeddable: content,\n        embedding_vector: embedding,\n        content: content.content,\n        chunk_index: 0,\n        created_at: Time.current,\n        updated_at: Time.current\n      }\n    end\n\n    Ragdoll::Embedding.insert_all(embedding_data)\n  end\nend\n</code></pre>"},{"location":"deployment/performance/#application-performance","title":"Application Performance","text":""},{"location":"deployment/performance/#service-layer-optimization","title":"Service Layer Optimization","text":"<pre><code># Optimized search engine with caching\nclass SearchEngine\n  def initialize(embedding_service)\n    @embedding_service = embedding_service\n    @query_cache = LRUCache.new(100)\n  end\n\n  def search_similar_content(query, **options)\n    cache_key = \"#{query}_#{options.hash}\"\n\n    @query_cache.fetch(cache_key) do\n      perform_search(query, **options)\n    end\n  end\n\n  private\n\n  def perform_search(query, **options)\n    # Cached embedding generation\n    query_embedding = @embedding_service.generate_embedding_cached(query)\n\n    # Optimized database query\n    Ragdoll::Embedding.search_similar(\n      query_embedding,\n      limit: options[:limit] || 20,\n      threshold: options[:threshold] || 0.7\n    )\n  end\nend\n</code></pre>"},{"location":"deployment/performance/#api-response-optimization","title":"API Response Optimization","text":"<pre><code># Streaming responses for large result sets\nclass Client\n  def search_stream(query:, **options)\n    Enumerator.new do |yielder|\n      results = search_similar_content(query: query, **options)\n\n      results.find_each(batch_size: 100) do |result|\n        yielder &lt;&lt; format_result(result)\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"deployment/performance/#monitoring-and-profiling","title":"Monitoring and Profiling","text":""},{"location":"deployment/performance/#key-metrics-to-track","title":"Key Metrics to Track","text":"<pre><code># Custom metrics collection\nclass PerformanceMonitor\n  METRICS = {\n    embedding_generation_time: 'histogram',\n    search_query_time: 'histogram',\n    database_connection_pool: 'gauge',\n    memory_usage: 'gauge',\n    background_job_queue_size: 'gauge'\n  }\n\n  def self.track_embedding_generation(text_length)\n    start_time = Time.current\n    yield\n    duration = Time.current - start_time\n\n    # Log metrics (integrate with your monitoring system)\n    Rails.logger.info({\n      metric: 'embedding_generation',\n      duration_ms: (duration * 1000).round(2),\n      text_length: text_length,\n      timestamp: Time.current.iso8601\n    }.to_json)\n  end\nend\n</code></pre>"},{"location":"deployment/performance/#performance-benchmarking","title":"Performance Benchmarking","text":"<pre><code># Benchmark suite for performance regression testing\nclass PerformanceBenchmark\n  def self.run_search_benchmark\n    queries = [\n      \"machine learning algorithms\",\n      \"natural language processing\",\n      \"computer vision techniques\"\n    ]\n\n    client = Ragdoll::Core.client\n\n    queries.each do |query|\n      iterations = 100\n      total_time = 0\n\n      iterations.times do\n        start_time = Time.current\n        client.search(query: query)\n        total_time += Time.current - start_time\n      end\n\n      avg_time = (total_time / iterations * 1000).round(2)\n      puts \"Query: #{query} - Average time: #{avg_time}ms\"\n    end\n  end\nend\n</code></pre>"},{"location":"deployment/performance/#scaling-strategies","title":"Scaling Strategies","text":""},{"location":"deployment/performance/#horizontal-scaling-architecture","title":"Horizontal Scaling Architecture","text":"<pre><code>flowchart TB\n    A[Load Balancer] --&gt; B[App Server 1]\n    A --&gt; C[App Server 2]\n    A --&gt; D[App Server N]\n\n    B --&gt; E[PostgreSQL Primary]\n    C --&gt; E\n    D --&gt; E\n\n    E --&gt; F[PostgreSQL Read Replica 1]\n    E --&gt; G[PostgreSQL Read Replica 2]\n\n    B --&gt; H[Redis Sidekiq]\n    C --&gt; H\n    D --&gt; H\n\n    H --&gt; I[Sidekiq Worker Pool]</code></pre>"},{"location":"deployment/performance/#database-sharding-considerations","title":"Database Sharding Considerations","text":"<pre><code># Document-based sharding strategy\nclass ShardedDocument &lt; Document\n  def self.shard_key(document_id)\n    Digest::SHA1.hexdigest(document_id.to_s)[0..7].to_i(16) % 4\n  end\n\n  def self.find_sharded(document_id)\n    shard = shard_key(document_id)\n    connection_handler.retrieve_connection(\"shard_#{shard}\")\n    find(document_id)\n  end\nend\n</code></pre>"},{"location":"deployment/performance/#load-testing-strategy","title":"Load Testing Strategy","text":"<pre><code># Load testing with realistic usage patterns\nclass LoadTest\n  def self.simulate_concurrent_searches(concurrent_users: 50)\n    threads = []\n\n    concurrent_users.times do |i|\n      threads &lt;&lt; Thread.new do\n        client = Ragdoll::Core.client\n\n        10.times do\n          query = generate_random_query\n          start_time = Time.current\n\n          begin\n            client.search(query: query)\n            duration = Time.current - start_time\n\n            puts \"Thread #{i}: #{query} - #{(duration * 1000).round(2)}ms\"\n          rescue =&gt; e\n            puts \"Thread #{i}: Error - #{e.message}\"\n          end\n\n          sleep(rand(0.5..2.0)) # Realistic user behavior\n        end\n      end\n    end\n\n    threads.each(&amp;:join)\n  end\nend\n</code></pre>"},{"location":"deployment/performance/#production-optimization-checklist","title":"Production Optimization Checklist","text":"<ul> <li> Configure PostgreSQL connection pooling (20+ connections)</li> <li> Set up pgvector indexes (IVFFlat or HNSW)</li> <li> Implement embedding caching strategy</li> <li> Configure background job queues with priorities</li> <li> Set up performance monitoring and alerting</li> <li> Implement database read replicas for search queries</li> <li> Configure memory limits and garbage collection</li> <li> Set up load testing and performance benchmarks</li> <li> Implement query result caching</li> <li> Configure log aggregation and analysis</li> </ul> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"deployment/security/","title":"Security Considerations","text":""},{"location":"deployment/security/#production-security-best-practices","title":"Production Security Best Practices","text":"<p>Ragdoll processes sensitive documents and integrates with external LLM APIs, requiring comprehensive security measures. This guide covers data protection, access control, API security, and compliance considerations for production deployments.</p>"},{"location":"deployment/security/#security-architecture-overview","title":"Security Architecture Overview","text":"<pre><code>flowchart TD\n    A[Client Application] --&gt;|HTTPS/TLS| B[Ragdoll]\n    B --&gt;|Encrypted Connection| C[PostgreSQL + pgvector]\n    B --&gt;|API Keys + TLS| D[LLM Providers]\n    B --&gt;|Encrypted Storage| E[File Storage]\n\n    subgraph \"Security Layers\"\n        F[Input Validation]\n        G[Authentication]\n        H[Authorization]\n        I[Encryption]\n        J[Audit Logging]\n    end\n\n    B --&gt; F\n    B --&gt; G\n    B --&gt; H\n    B --&gt; I\n    B --&gt; J</code></pre>"},{"location":"deployment/security/#data-protection","title":"Data Protection","text":""},{"location":"deployment/security/#encryption-at-rest","title":"Encryption at Rest","text":""},{"location":"deployment/security/#database-encryption","title":"Database Encryption","text":"<pre><code>-- PostgreSQL encryption configuration\n-- Enable transparent data encryption (TDE)\nALTER SYSTEM SET ssl = on;\nALTER SYSTEM SET ssl_cert_file = '/path/to/server.crt';\nALTER SYSTEM SET ssl_key_file = '/path/to/server.key';\nALTER SYSTEM SET ssl_ca_file = '/path/to/ca.crt';\n\n-- Enable encryption for specific tablespaces\nCREATE TABLESPACE encrypted_space \n  LOCATION '/encrypted/data' \n  WITH (encryption_key_id = 'your-key-id');\n\n-- Move sensitive tables to encrypted tablespace\nALTER TABLE ragdoll_documents SET TABLESPACE encrypted_space;\nALTER TABLE ragdoll_embeddings SET TABLESPACE encrypted_space;\n</code></pre>"},{"location":"deployment/security/#file-storage-encryption","title":"File Storage Encryption","text":"<pre><code># Shrine configuration with encryption\nclass DocumentUploader &lt; Shrine\n  plugin :encryption, algorithm: 'aes-256-gcm'\n\n  Attacher.validate do\n    validate_max_size 50.megabytes\n    validate_mime_type %w[application/pdf text/plain image/jpeg image/png]\n    validate_extension %w[pdf txt jpg jpeg png docx]\n  end\n\n  # Encrypt files before storage\n  process(:store) do |io, context|\n    if context[:record].sensitive?\n      encrypted_io = encrypt_file(io)\n      [encrypted_io, { encryption: true }]\n    else\n      io\n    end\n  end\n\n  private\n\n  def encrypt_file(io)\n    require 'openssl'\n\n    cipher = OpenSSL::Cipher.new('aes-256-gcm')\n    cipher.encrypt\n\n    key = ENV['FILE_ENCRYPTION_KEY'] || raise('FILE_ENCRYPTION_KEY not set')\n    iv = cipher.random_iv\n\n    cipher.key = key[0..31]  # 32 bytes for AES-256\n    cipher.iv = iv\n\n    encrypted_data = cipher.update(io.read) + cipher.final\n    auth_tag = cipher.auth_tag\n\n    # Prepend IV and auth tag to encrypted data\n    combined_data = iv + auth_tag + encrypted_data\n    StringIO.new(combined_data)\n  end\nend\n</code></pre>"},{"location":"deployment/security/#key-management","title":"Key Management","text":"<pre><code># Secure key management configuration\nclass KeyManager\n  def self.setup_encryption_keys\n    # Use environment-specific key derivation\n    master_key = ENV['MASTER_ENCRYPTION_KEY'] || \n                 raise(SecurityError, 'MASTER_ENCRYPTION_KEY required')\n\n    # Derive application-specific keys\n    {\n      database_key: derive_key(master_key, 'database'),\n      file_encryption_key: derive_key(master_key, 'files'),\n      api_signing_key: derive_key(master_key, 'api')\n    }\n  end\n\n  private\n\n  def self.derive_key(master_key, purpose)\n    require 'openssl'\n\n    OpenSSL::PKCS5.pbkdf2_hmac(\n      master_key,\n      purpose,  # Use purpose as salt\n      10_000,   # Iterations\n      32,       # Key length (256 bits)\n      OpenSSL::Digest::SHA256.new\n    )\n  end\nend\n</code></pre>"},{"location":"deployment/security/#encryption-in-transit","title":"Encryption in Transit","text":""},{"location":"deployment/security/#tlsssl-configuration","title":"TLS/SSL Configuration","text":"<pre><code># Database connection with SSL\nRagdoll::Core.configure do |config|\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_production',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'db.example.com',\n    port: 5432,\n    sslmode: 'require',           # Require SSL connection\n    sslcert: '/path/to/client.crt',\n    sslkey: '/path/to/client.key',\n    sslrootcert: '/path/to/ca.crt'\n  }\nend\n</code></pre>"},{"location":"deployment/security/#api-communication-security","title":"API Communication Security","text":"<pre><code># Secure HTTP client configuration\nclass SecureAPIClient\n  def initialize\n    @http_client = Faraday.new do |conn|\n      conn.use :ssl, {\n        verify: true,\n        ca_file: '/etc/ssl/certs/ca-certificates.crt',\n        version: 'TLSv1_2'  # Minimum TLS version\n      }\n\n      conn.request :retry, {\n        max: 3,\n        retry_statuses: [429, 500, 502, 503, 504]\n      }\n\n      conn.response :json\n      conn.adapter Faraday.default_adapter\n    end\n  end\n\n  def make_request(url, headers = {})\n    secure_headers = headers.merge({\n      'User-Agent' =&gt; 'Ragdoll/1.0',\n      'Accept': 'application/json',\n      'Content-Type': 'application/json'\n    })\n\n    @http_client.post(url, secure_headers)\n  end\nend\n</code></pre>"},{"location":"deployment/security/#access-control","title":"Access Control","text":""},{"location":"deployment/security/#authentication-patterns","title":"Authentication Patterns","text":""},{"location":"deployment/security/#api-key-management","title":"API Key Management","text":"<pre><code># Secure API key configuration\nclass APIKeyManager\n  REQUIRED_KEYS = %w[\n    OPENAI_API_KEY\n    ANTHROPIC_API_KEY\n    DATABASE_PASSWORD\n    MASTER_ENCRYPTION_KEY\n  ].freeze\n\n  def self.validate_keys!\n    missing_keys = REQUIRED_KEYS.select { |key| ENV[key].nil? || ENV[key].empty? }\n\n    if missing_keys.any?\n      raise SecurityError, \"Missing required API keys: #{missing_keys.join(', ')}\"\n    end\n\n    # Validate key formats\n    validate_openai_key(ENV['OPENAI_API_KEY'])\n    validate_anthropic_key(ENV['ANTHROPIC_API_KEY'])\n  end\n\n  def self.rotate_keys!\n    # Implement key rotation logic\n    puts \"Key rotation initiated at #{Time.current}\"\n\n    # 1. Generate new keys\n    # 2. Update configuration\n    # 3. Test connectivity\n    # 4. Revoke old keys\n    # 5. Log rotation event\n  end\n\n  private\n\n  def self.validate_openai_key(key)\n    unless key&amp;.start_with?('sk-') &amp;&amp; key.length &gt;= 50\n      raise SecurityError, 'Invalid OpenAI API key format'\n    end\n  end\n\n  def self.validate_anthropic_key(key)\n    unless key&amp;.start_with?('sk-ant-') &amp;&amp; key.length &gt;= 100\n      raise SecurityError, 'Invalid Anthropic API key format'\n    end\n  end\nend\n\n# Initialize security checks\nAPIKeyManager.validate_keys!\n</code></pre>"},{"location":"deployment/security/#token-based-authentication","title":"Token-Based Authentication","text":"<pre><code># JWT token authentication for API access\nclass TokenAuthenticator\n  def initialize(secret_key = ENV['JWT_SECRET_KEY'])\n    @secret_key = secret_key || raise('JWT_SECRET_KEY required')\n  end\n\n  def generate_token(user_id, permissions = [])\n    payload = {\n      user_id: user_id,\n      permissions: permissions,\n      iat: Time.current.to_i,\n      exp: (Time.current + 1.hour).to_i  # 1 hour expiry\n    }\n\n    JWT.encode(payload, @secret_key, 'HS256')\n  end\n\n  def verify_token(token)\n    decoded = JWT.decode(token, @secret_key, true, algorithm: 'HS256')\n    payload = decoded.first\n\n    # Check expiration\n    if Time.current.to_i &gt; payload['exp']\n      raise SecurityError, 'Token expired'\n    end\n\n    payload\n  rescue JWT::DecodeError =&gt; e\n    raise SecurityError, \"Invalid token: #{e.message}\"\n  end\nend\n</code></pre>"},{"location":"deployment/security/#authorization","title":"Authorization","text":""},{"location":"deployment/security/#role-based-access-control","title":"Role-Based Access Control","text":"<pre><code># RBAC implementation\nclass AccessController\n  PERMISSIONS = {\n    'admin' =&gt; %w[read write delete manage],\n    'editor' =&gt; %w[read write],\n    'viewer' =&gt; %w[read],\n    'api_user' =&gt; %w[read search]\n  }.freeze\n\n  def self.check_permission(user_role, action, resource = nil)\n    user_permissions = PERMISSIONS[user_role] || []\n\n    unless user_permissions.include?(action)\n      raise SecurityError, \"Access denied: #{user_role} cannot #{action}\"\n    end\n\n    # Additional resource-level checks\n    if resource &amp;&amp; !resource_accessible?(user_role, resource)\n      raise SecurityError, \"Resource access denied\"\n    end\n\n    true\n  end\n\n  private\n\n  def self.resource_accessible?(user_role, resource)\n    # Implement resource-specific access logic\n    case resource\n    when Ragdoll::Document\n      # Check document ownership or sharing permissions\n      true  # Placeholder\n    else\n      true\n    end\n  end\nend\n</code></pre>"},{"location":"deployment/security/#file-upload-security","title":"File Upload Security","text":""},{"location":"deployment/security/#file-validation","title":"File Validation","text":"<pre><code># Comprehensive file validation\nclass FileValidator\n  ALLOWED_MIME_TYPES = %w[\n    application/pdf\n    text/plain\n    text/markdown\n    text/html\n    application/vnd.openxmlformats-officedocument.wordprocessingml.document\n    image/jpeg\n    image/png\n    image/gif\n  ].freeze\n\n  MAX_FILE_SIZE = 50.megabytes\n\n  def self.validate_file(file_path)\n    # Check file existence and readability\n    unless File.exist?(file_path) &amp;&amp; File.readable?(file_path)\n      raise SecurityError, \"File not accessible: #{file_path}\"\n    end\n\n    # Check file size\n    file_size = File.size(file_path)\n    if file_size &gt; MAX_FILE_SIZE\n      raise SecurityError, \"File too large: #{file_size} bytes (max: #{MAX_FILE_SIZE})\"\n    end\n\n    # Validate MIME type\n    mime_type = detect_mime_type(file_path)\n    unless ALLOWED_MIME_TYPES.include?(mime_type)\n      raise SecurityError, \"Unsupported file type: #{mime_type}\"\n    end\n\n    # Content-based validation\n    validate_file_content(file_path, mime_type)\n\n    true\n  end\n\n  private\n\n  def self.detect_mime_type(file_path)\n    # Use multiple methods for MIME type detection\n    require 'mimemagic'\n\n    # File extension check\n    ext_mime = MIME::Types.type_for(file_path).first&amp;.content_type\n\n    # Magic number check\n    magic_mime = MimeMagic.by_path(file_path)&amp;.type\n\n    # Verify consistency\n    if ext_mime &amp;&amp; magic_mime &amp;&amp; ext_mime != magic_mime\n      raise SecurityError, \"MIME type mismatch: extension=#{ext_mime}, magic=#{magic_mime}\"\n    end\n\n    magic_mime || ext_mime || 'application/octet-stream'\n  end\n\n  def self.validate_file_content(file_path, mime_type)\n    case mime_type\n    when 'application/pdf'\n      validate_pdf_content(file_path)\n    when /^image\\//\n      validate_image_content(file_path)\n    end\n  end\n\n  def self.validate_pdf_content(file_path)\n    # Basic PDF structure validation\n    File.open(file_path, 'rb') do |file|\n      header = file.read(8)\n      unless header.start_with?('%PDF-')\n        raise SecurityError, \"Invalid PDF header\"\n      end\n    end\n  end\n\n  def self.validate_image_content(file_path)\n    # Validate image using ImageMagick\n    require 'rmagick'\n\n    begin\n      img = Magick::Image.read(file_path).first\n\n      # Check for reasonable dimensions\n      if img.columns &gt; 10_000 || img.rows &gt; 10_000\n        raise SecurityError, \"Image dimensions too large: #{img.columns}x#{img.rows}\"\n      end\n    rescue Magick::ImageMagickError =&gt; e\n      raise SecurityError, \"Invalid image file: #{e.message}\"\n    end\n  end\nend\n</code></pre>"},{"location":"deployment/security/#secure-file-storage","title":"Secure File Storage","text":"<pre><code># Secure file storage implementation\nclass SecureFileStorage\n  def initialize\n    @storage_path = ENV['SECURE_STORAGE_PATH'] || '/secure/storage'\n    @file_permissions = 0o600  # Owner read/write only\n    @dir_permissions = 0o700   # Owner access only\n  end\n\n  def store_file(source_path, secure_filename)\n    # Sanitize filename\n    safe_filename = sanitize_filename(secure_filename)\n\n    # Create secure directory structure\n    date_path = Date.current.strftime('%Y/%m/%d')\n    secure_dir = File.join(@storage_path, date_path)\n    FileUtils.mkdir_p(secure_dir, mode: @dir_permissions)\n\n    # Generate unique secure path\n    secure_path = File.join(secure_dir, \"#{SecureRandom.uuid}_#{safe_filename}\")\n\n    # Copy file with secure permissions\n    FileUtils.cp(source_path, secure_path)\n    FileUtils.chmod(@file_permissions, secure_path)\n\n    # Log file storage event\n    SecurityLogger.log_file_storage(source_path, secure_path)\n\n    secure_path\n  end\n\n  private\n\n  def sanitize_filename(filename)\n    # Remove dangerous characters\n    sanitized = filename.gsub(/[^\\w\\-_\\.]/, '_')\n\n    # Prevent path traversal\n    sanitized = File.basename(sanitized)\n\n    # Limit length\n    sanitized = sanitized[0..100] if sanitized.length &gt; 100\n\n    sanitized\n  end\nend\n</code></pre>"},{"location":"deployment/security/#api-security","title":"API Security","text":""},{"location":"deployment/security/#input-validation","title":"Input Validation","text":"<pre><code># Comprehensive input validation\nclass InputValidator\n  # SQL injection prevention\n  def self.validate_search_query(query)\n    # Remove potential SQL injection patterns\n    dangerous_patterns = [\n      /';\\s*(drop|delete|update|insert|create|alter)/i,\n      /union\\s+select/i,\n      /exec\\s*\\(/i,\n      /&lt;script[^&gt;]*&gt;/i\n    ]\n\n    dangerous_patterns.each do |pattern|\n      if query.match?(pattern)\n        raise SecurityError, \"Potentially dangerous query pattern detected\"\n      end\n    end\n\n    # Limit query length\n    if query.length &gt; 1000\n      raise SecurityError, \"Query too long (max 1000 characters)\"\n    end\n\n    query.strip\n  end\n\n  # Parameter sanitization\n  def self.sanitize_parameters(params)\n    sanitized = {}\n\n    params.each do |key, value|\n      sanitized_key = sanitize_key(key)\n\n      case value\n      when String\n        sanitized[sanitized_key] = sanitize_string(value)\n      when Integer\n        sanitized[sanitized_key] = value.clamp(0, 1_000_000)\n      when Array\n        sanitized[sanitized_key] = value.map { |v| sanitize_string(v.to_s) }\n      when Hash\n        sanitized[sanitized_key] = sanitize_parameters(value)\n      else\n        sanitized[sanitized_key] = value.to_s\n      end\n    end\n\n    sanitized\n  end\n\n  private\n\n  def self.sanitize_key(key)\n    # Only allow alphanumeric keys with underscores\n    key.to_s.gsub(/[^a-zA-Z0-9_]/, '')\n  end\n\n  def self.sanitize_string(str)\n    # Remove control characters and limit length\n    str.gsub(/[\\x00-\\x1f\\x7f]/, '').strip[0..10_000]\n  end\nend\n</code></pre>"},{"location":"deployment/security/#rate-limiting","title":"Rate Limiting","text":"<pre><code># Rate limiting implementation\nclass RateLimiter\n  def initialize(redis_client = Redis.new)\n    @redis = redis_client\n  end\n\n  def check_rate_limit(identifier, limit: 100, window: 3600)\n    key = \"rate_limit:#{identifier}\"\n    current_time = Time.current.to_i\n    window_start = current_time - window\n\n    # Clean old entries\n    @redis.zremrangebyscore(key, '-inf', window_start)\n\n    # Count current requests\n    current_count = @redis.zcard(key)\n\n    if current_count &gt;= limit\n      remaining_time = @redis.zrange(key, 0, 0, with_scores: true).first&amp;.last&amp;.to_i\n      reset_time = remaining_time + window if remaining_time\n\n      raise SecurityError, \"Rate limit exceeded. Reset at #{Time.at(reset_time)}\"\n    end\n\n    # Add current request\n    @redis.zadd(key, current_time, SecureRandom.uuid)\n    @redis.expire(key, window)\n\n    {\n      remaining: limit - current_count - 1,\n      reset_time: current_time + window\n    }\n  end\nend\n</code></pre>"},{"location":"deployment/security/#database-security","title":"Database Security","text":""},{"location":"deployment/security/#connection-security","title":"Connection Security","text":"<pre><code># Secure database configuration\nclass DatabaseSecurity\n  def self.configure_secure_connection\n    Ragdoll::Core.configure do |config|\n      config.database_config = {\n        adapter: 'postgresql',\n        database: ENV['DB_NAME'],\n        username: ENV['DB_USER'],\n        password: ENV['DB_PASSWORD'],\n        host: ENV['DB_HOST'],\n        port: ENV['DB_PORT']&amp;.to_i || 5432,\n\n        # SSL configuration\n        sslmode: 'require',\n        sslcert: ENV['DB_SSL_CERT_PATH'],\n        sslkey: ENV['DB_SSL_KEY_PATH'],\n        sslrootcert: ENV['DB_SSL_CA_PATH'],\n\n        # Connection pool security\n        pool: 20,\n        timeout: 5000,\n        checkout_timeout: 5,\n        idle_timeout: 300,\n\n        # Query logging (be careful with sensitive data)\n        logger: ENV['RAILS_ENV'] == 'production' ? nil : Logger.new(STDOUT)\n      }\n    end\n  end\n\n  def self.create_restricted_user\n    # SQL commands to create restricted database user\n    &lt;&lt;~SQL\n      -- Create restricted user for application\n      CREATE USER ragdoll_app WITH PASSWORD '#{ENV['DB_APP_PASSWORD']}';\n\n      -- Grant only necessary permissions\n      GRANT CONNECT ON DATABASE ragdoll_production TO ragdoll_app;\n      GRANT USAGE ON SCHEMA public TO ragdoll_app;\n      GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO ragdoll_app;\n      GRANT USAGE ON ALL SEQUENCES IN SCHEMA public TO ragdoll_app;\n\n      -- Revoke dangerous permissions\n      REVOKE CREATE ON SCHEMA public FROM ragdoll_app;\n      REVOKE ALL ON pg_user FROM ragdoll_app;\n      REVOKE ALL ON pg_database FROM ragdoll_app;\n    SQL\n  end\nend\n</code></pre>"},{"location":"deployment/security/#data-protection_1","title":"Data Protection","text":"<pre><code># PII data handling\nclass PIIProtection\n  SENSITIVE_FIELDS = %w[email phone ssn credit_card].freeze\n\n  def self.anonymize_content(content)\n    anonymized = content.dup\n\n    # Email anonymization\n    anonymized.gsub!(/\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b/, '[EMAIL]')\n\n    # Phone number anonymization\n    anonymized.gsub!(/\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b/, '[PHONE]')\n\n    # SSN anonymization\n    anonymized.gsub!(/\\b\\d{3}-\\d{2}-\\d{4}\\b/, '[SSN]')\n\n    # Credit card anonymization\n    anonymized.gsub!(/\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b/, '[CREDIT_CARD]')\n\n    anonymized\n  end\n\n  def self.contains_pii?(content)\n    patterns = [\n      /\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b/,  # Email\n      /\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b/,                          # Phone\n      /\\b\\d{3}-\\d{2}-\\d{4}\\b/,                                  # SSN\n      /\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b/             # Credit card\n    ]\n\n    patterns.any? { |pattern| content.match?(pattern) }\n  end\nend\n</code></pre>"},{"location":"deployment/security/#llm-provider-security","title":"LLM Provider Security","text":""},{"location":"deployment/security/#api-key-protection","title":"API Key Protection","text":"<pre><code># Secure LLM API integration\nclass LLMSecurity\n  def self.validate_api_responses(response)\n    # Check for potential data leakage in responses\n    if response.is_a?(Hash) &amp;&amp; response['error']\n      error_msg = response['error']['message']\n\n      # Remove potentially sensitive information from error messages\n      sanitized_error = error_msg.gsub(/api[_-]?key[^\\s]*/i, '[API_KEY]')\n      sanitized_error = sanitized_error.gsub(/token[^\\s]*/i, '[TOKEN]')\n\n      response['error']['message'] = sanitized_error\n    end\n\n    response\n  end\n\n  def self.audit_llm_request(provider, model, request_data)\n    # Log LLM requests for audit purposes (without sensitive data)\n    audit_data = {\n      timestamp: Time.current.iso8601,\n      provider: provider,\n      model: model,\n      request_size: request_data.to_s.length,\n      request_hash: Digest::SHA256.hexdigest(request_data.to_s),\n      user_id: Current.user&amp;.id,\n      ip_address: Current.ip_address\n    }\n\n    SecurityLogger.log_llm_request(audit_data)\n  end\nend\n</code></pre>"},{"location":"deployment/security/#data-privacy","title":"Data Privacy","text":"<pre><code># Content privacy protection\nclass ContentPrivacy\n  def self.sanitize_for_llm(content)\n    # Remove sensitive information before sending to LLM\n    sanitized = PIIProtection.anonymize_content(content)\n\n    # Remove internal references\n    sanitized.gsub!(/\\b(internal|confidential|proprietary)\\b/i, '[REDACTED]')\n\n    # Limit content size to prevent data leakage\n    if sanitized.length &gt; 50_000\n      sanitized = sanitized[0..50_000] + \"\\n[CONTENT TRUNCATED FOR SECURITY]\"\n    end\n\n    sanitized\n  end\n\n  def self.should_process_with_llm?(content, metadata = {})\n    # Check if content is safe to send to external LLM\n    return false if PIIProtection.contains_pii?(content)\n    return false if metadata[:confidentiality] == 'high'\n    return false if content.match?(/confidential|secret|private/i)\n\n    true\n  end\nend\n</code></pre>"},{"location":"deployment/security/#compliance","title":"Compliance","text":""},{"location":"deployment/security/#gdpr-compliance","title":"GDPR Compliance","text":"<pre><code># GDPR compliance implementation\nclass GDPRCompliance\n  def self.handle_data_subject_request(request_type, user_id)\n    case request_type\n    when 'access'\n      export_user_data(user_id)\n    when 'rectification'\n      allow_data_correction(user_id)\n    when 'erasure'\n      delete_user_data(user_id)\n    when 'portability'\n      export_user_data_portable(user_id)\n    end\n  end\n\n  private\n\n  def self.delete_user_data(user_id)\n    # Comprehensive data deletion\n    Ragdoll::Document.where(user_id: user_id).destroy_all\n    Ragdoll::Embedding.joins(:embeddable)\n                                   .where(ragdoll_contents: { user_id: user_id })\n                                   .destroy_all\n\n    # Log deletion for audit\n    SecurityLogger.log_data_deletion(user_id, Time.current)\n  end\nend\n</code></pre>"},{"location":"deployment/security/#security-auditing","title":"Security Auditing","text":"<pre><code># Comprehensive security logging\nclass SecurityLogger\n  def self.log_security_event(event_type, details = {})\n    log_entry = {\n      timestamp: Time.current.iso8601,\n      event_type: event_type,\n      user_id: Current.user&amp;.id,\n      ip_address: Current.ip_address,\n      user_agent: Current.user_agent,\n      session_id: Current.session_id,\n      details: details\n    }\n\n    # Log to secure audit file\n    audit_logger.info(log_entry.to_json)\n\n    # Send to SIEM if configured\n    send_to_siem(log_entry) if ENV['SIEM_ENDPOINT']\n  end\n\n  def self.log_file_storage(source, destination)\n    log_security_event('file_stored', {\n      source_path: source,\n      destination_path: destination,\n      file_size: File.size(destination)\n    })\n  end\n\n  def self.log_data_deletion(user_id, timestamp)\n    log_security_event('data_deleted', {\n      target_user_id: user_id,\n      deletion_timestamp: timestamp\n    })\n  end\n\n  private\n\n  def self.audit_logger\n    @audit_logger ||= Logger.new(\n      ENV['SECURITY_LOG_PATH'] || '/var/log/ragdoll/security.log',\n      10,  # Keep 10 old log files\n      100.megabytes  # Max 100MB per file\n    )\n  end\nend\n</code></pre>"},{"location":"deployment/security/#security-monitoring","title":"Security Monitoring","text":""},{"location":"deployment/security/#threat-detection","title":"Threat Detection","text":"<pre><code># Anomaly detection system\nclass ThreatDetector\n  def self.detect_anomalies\n    {\n      unusual_access_patterns: detect_unusual_access,\n      bulk_data_access: detect_bulk_access,\n      failed_auth_attempts: detect_auth_failures,\n      suspicious_queries: detect_suspicious_queries\n    }\n  end\n\n  private\n\n  def self.detect_unusual_access\n    # Detect access patterns outside normal hours\n    current_hour = Time.current.hour\n\n    if (current_hour &lt; 6 || current_hour &gt; 22) &amp;&amp; \n       Rails.env.production? &amp;&amp; \n       recent_activity_count &gt; 100\n      {\n        type: 'off_hours_access',\n        severity: 'medium',\n        count: recent_activity_count\n      }\n    end\n  end\n\n  def self.detect_bulk_access\n    # Detect potential data exfiltration\n    recent_searches = SearchLog.where('created_at &gt; ?', 1.hour.ago)\n\n    if recent_searches.count &gt; 1000\n      {\n        type: 'bulk_access',\n        severity: 'high',\n        count: recent_searches.count\n      }\n    end\n  end\nend\n</code></pre>"},{"location":"deployment/security/#incident-response","title":"Incident Response","text":"<pre><code># Security incident response system\nclass IncidentResponse\n  def self.handle_security_incident(incident_type, severity, details)\n    incident_id = SecureRandom.uuid\n\n    # Log incident\n    SecurityLogger.log_security_event('security_incident', {\n      incident_id: incident_id,\n      type: incident_type,\n      severity: severity,\n      details: details\n    })\n\n    # Immediate response based on severity\n    case severity\n    when 'critical'\n      # Immediate lockdown\n      initiate_lockdown_procedures\n      notify_security_team\n      escalate_to_management\n    when 'high'\n      # Enhanced monitoring\n      increase_monitoring_level\n      notify_security_team\n    when 'medium'\n      # Standard response\n      log_for_review\n    end\n\n    incident_id\n  end\n\n  private\n\n  def self.initiate_lockdown_procedures\n    # Disable API access\n    Rails.cache.write('api_lockdown', true, expires_in: 1.hour)\n\n    # Revoke active sessions\n    ActiveSession.where('created_at &gt; ?', 24.hours.ago).destroy_all\n\n    # Alert administrators\n    SecurityAlert.create!(\n      type: 'lockdown_initiated',\n      message: 'Security lockdown initiated due to critical incident'\n    )\n  end\nend\n</code></pre>"},{"location":"deployment/security/#production-security-checklist","title":"Production Security Checklist","text":""},{"location":"deployment/security/#pre-deployment-security-checklist","title":"Pre-Deployment Security Checklist","text":"<ul> <li> Database Security</li> <li> PostgreSQL SSL/TLS enabled</li> <li> Database user permissions restricted</li> <li> Connection encryption configured</li> <li> <p> Backup encryption enabled</p> </li> <li> <p> API Security</p> </li> <li> All API keys rotated and secured</li> <li> Rate limiting implemented</li> <li> Input validation comprehensive</li> <li> <p> Error messages sanitized</p> </li> <li> <p> File Security</p> </li> <li> File upload validation implemented</li> <li> File storage encryption configured</li> <li> File access permissions restricted</li> <li> <p> Malware scanning enabled</p> </li> <li> <p> Monitoring &amp; Logging</p> </li> <li> Security event logging enabled</li> <li> Audit trails comprehensive</li> <li> Anomaly detection configured</li> <li> <p> Incident response procedures documented</p> </li> <li> <p> Compliance</p> </li> <li> GDPR compliance measures implemented</li> <li> Data retention policies configured</li> <li> Privacy controls operational</li> <li> Audit procedures established</li> </ul>"},{"location":"deployment/security/#regular-security-maintenance","title":"Regular Security Maintenance","text":"<pre><code># Monthly security maintenance script\n#!/bin/bash\n\n# Rotate API keys\necho \"Rotating API keys...\"\nrake security:rotate_keys\n\n# Update security certificates\necho \"Updating SSL certificates...\"\nrake security:update_certificates\n\n# Run security audit\necho \"Running security audit...\"\nrake security:audit\n\n# Generate security report\necho \"Generating security report...\"\nrake security:report\n\n# Check for security vulnerabilities\necho \"Checking for vulnerabilities...\"\nbundle audit\n</code></pre> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"development/architecture/","title":"Architecture Overview","text":"<p>Ragdoll Core is a database-oriented RAG (Retrieval-Augmented Generation) framework built on ActiveRecord with PostgreSQL as the primary data store. The architecture follows a layered service-oriented design with clear separation of concerns, optimized for performance and extensibility.</p>"},{"location":"development/architecture/#system-design-and-component-relationships","title":"System Design and Component Relationships","text":""},{"location":"development/architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    subgraph \"Client Interface Layer\"\n        Client[Ragdoll::Core::Client&lt;br/&gt;\ud83c\udfd7\ufe0f Facade Pattern]\n        API[Public API Methods&lt;br/&gt;\ud83d\udd0c High-level Interface]\n        Delegation[Module Delegation&lt;br/&gt;\ud83d\udce1 Method Forwarding]\n    end\n\n    subgraph \"Service Layer\"\n        DocProc[Document Processor&lt;br/&gt;\ud83d\udcc4 Multi-format Parsing]\n        DocMgmt[Document Management&lt;br/&gt;\ud83d\uddc2\ufe0f CRUD Operations]\n        EmbedSvc[Embedding Service&lt;br/&gt;\ud83e\udde0 Vector Generation]\n        SearchEng[Search Engine&lt;br/&gt;\ud83d\udd0d Semantic &amp; Hybrid Search]\n        TextChunk[Text Chunker&lt;br/&gt;\u2702\ufe0f Content Segmentation]\n        TextGen[Text Generation Service&lt;br/&gt;\ud83d\udcac LLM Integration]\n        Config[Configuration Manager&lt;br/&gt;\u2699\ufe0f Settings &amp; Providers]\n    end\n\n    subgraph \"Background Processing Layer\"\n        ActiveJob[ActiveJob Integration&lt;br/&gt;\u26a1 Queue Management]\n        EmbedJob[Generate Embeddings Job&lt;br/&gt;\ud83d\udd04 Async Processing]\n        MetaJob[Metadata Generation Job&lt;br/&gt;\ud83d\udcca LLM Enhancement]\n    end\n\n    subgraph \"Data Layer\"\n        DocModel[Document Model&lt;br/&gt;\ud83d\udcd1 Central Entity]\n        TextContent[Text Content&lt;br/&gt;\ud83d\udcdd Text Processing]\n        ImageContent[Image Content&lt;br/&gt;\ud83d\uddbc\ufe0f Image Analysis]\n        AudioContent[Audio Content&lt;br/&gt;\ud83c\udfb5 Audio Processing]\n        EmbedModel[Embedding Model&lt;br/&gt;\ud83c\udfaf Vector Storage]\n        SearchModel[Search Model&lt;br/&gt;\ud83d\udd0d Query Tracking]\n        SearchResultModel[Search Result Model&lt;br/&gt;\ud83d\udcca Analytics]\n    end\n\n    subgraph \"Infrastructure Layer\"\n        PostgreSQL[(PostgreSQL + pgvector&lt;br/&gt;\ud83d\udc18 Vector Database)]\n        Shrine[Shrine File Storage&lt;br/&gt;\ud83d\udcc1 Upload Management]\n        FullText[Full-text Search&lt;br/&gt;\ud83d\udd0e GIN Indexes]\n        VectorSearch[Vector Similarity&lt;br/&gt;\ud83d\udcd0 IVFFlat Indexes]\n    end\n\n    subgraph \"External Services\"\n        LLMProviders[LLM Providers&lt;br/&gt;\ud83e\udd16 OpenAI, Anthropic, etc.]\n        FileStorage[File Storage Backends&lt;br/&gt;\u2601\ufe0f Local, S3, etc.]\n    end\n\n    %% Client Layer Connections\n    Client --&gt; DocProc\n    Client --&gt; DocMgmt\n    Client --&gt; SearchEng\n    Client --&gt; EmbedSvc\n    API --&gt; Client\n    Delegation --&gt; Client\n\n    %% Service Layer Connections\n    DocProc --&gt; TextChunk\n    DocMgmt --&gt; DocProc\n    DocMgmt --&gt; EmbedJob\n    SearchEng --&gt; EmbedSvc\n    EmbedSvc --&gt; LLMProviders\n    TextGen --&gt; LLMProviders\n\n    %% Background Processing\n    EmbedJob --&gt; EmbedSvc\n    EmbedJob --&gt; TextChunk\n    MetaJob --&gt; TextGen\n    ActiveJob --&gt; EmbedJob\n    ActiveJob --&gt; MetaJob\n\n    %% Data Layer Connections\n    DocModel --&gt; TextContent\n    DocModel --&gt; ImageContent\n    DocModel --&gt; AudioContent\n    TextContent --&gt; EmbedModel\n    ImageContent --&gt; EmbedModel\n    AudioContent --&gt; EmbedModel\n    SearchEng --&gt; SearchModel\n    SearchModel --&gt; SearchResultModel\n    SearchResultModel --&gt; EmbedModel\n\n    %% Infrastructure Connections\n    DocModel --&gt; PostgreSQL\n    EmbedModel --&gt; PostgreSQL\n    SearchModel --&gt; PostgreSQL\n    SearchResultModel --&gt; PostgreSQL\n    EmbedModel --&gt; VectorSearch\n    SearchModel --&gt; VectorSearch\n    DocModel --&gt; FullText\n    DocProc --&gt; Shrine\n    Shrine --&gt; FileStorage\n    VectorSearch --&gt; PostgreSQL\n    FullText --&gt; PostgreSQL\n\n    %% Styling\n    classDef clientLayer fill:#e1f5fe\n    classDef serviceLayer fill:#f3e5f5\n    classDef processLayer fill:#fff3e0\n    classDef dataLayer fill:#e8f5e8\n    classDef infraLayer fill:#fce4ec\n    classDef externalLayer fill:#f1f8e9\n\n    class Client,API,Delegation clientLayer\n    class DocProc,DocMgmt,EmbedSvc,SearchEng,TextChunk,TextGen,Config serviceLayer\n    class ActiveJob,EmbedJob,MetaJob processLayer\n    class DocModel,TextContent,ImageContent,AudioContent,EmbedModel dataLayer\n    class PostgreSQL,Shrine,FullText,VectorSearch infraLayer\n    class LLMProviders,FileStorage externalLayer</code></pre>"},{"location":"development/architecture/#data-flow-through-the-system","title":"Data Flow Through the System","text":""},{"location":"development/architecture/#1-document-ingestion-flow","title":"1. Document Ingestion Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Client as Ragdoll::Core::Client\n    participant DocProc as Document Processor\n    participant Shrine as File Storage\n    participant DB as PostgreSQL\n    participant Job as Background Job\n    participant EmbedSvc as Embedding Service\n    participant LLM as LLM Provider\n\n    User-&gt;&gt;Client: add_document(path)\n    Client-&gt;&gt;DocProc: parse(file_path)\n    Client-&gt;&gt;DocMgmt: add_document(path, content, metadata)\n    DocMgmt-&gt;&gt;DB: store_document()\n    DocMgmt--&gt;&gt;Client: return document_id\n    Client--&gt;&gt;User: success response\n\n    Client-&gt;&gt;Job: queue GenerateEmbeddings\n    Job-&gt;&gt;EmbedSvc: process_document()\n    EmbedSvc-&gt;&gt;LLM: generate_embeddings()\n    LLM--&gt;&gt;EmbedSvc: vector_embeddings\n    EmbedSvc-&gt;&gt;DB: store_embeddings()\n    Job-&gt;&gt;DB: update_status(processed)</code></pre>"},{"location":"development/architecture/#2-search-and-retrieval-flow","title":"2. Search and Retrieval Flow","text":"<pre><code>flowchart TD\n    A[Query Input] --&gt; B{Search Type?}\n\n    B --&gt;|Semantic| C[Generate Query Embedding]\n    B --&gt;|Full-text| D[PostgreSQL Text Search]\n    B --&gt;|Hybrid| E[Both Semantic + Text]\n\n    C --&gt; F[pgvector Similarity Search]\n    D --&gt; G[GIN Index Search]\n    E --&gt; H[Weighted Result Combination]\n\n    F --&gt; I[Result Ranking]\n    G --&gt; I\n    H --&gt; I\n\n    I --&gt; J[Usage Analytics Update]\n    J --&gt; K[Context Assembly]\n    K --&gt; L[Return Results]\n\n    style A fill:#e1f5fe\n    style L fill:#e8f5e8\n    style I fill:#fff3e0</code></pre>"},{"location":"development/architecture/#3-rag-enhancement-flow","title":"3. RAG Enhancement Flow","text":"<pre><code>graph LR\n    subgraph \"Context Retrieval\"\n        A[User Prompt] --&gt; B[get_context]\n        B --&gt; C[search_similar_content]\n        C --&gt; D[Similarity Search]\n        D --&gt; E[Top K Results]\n    end\n\n    subgraph \"Prompt Enhancement\"\n        E --&gt; F[build_enhanced_prompt]\n        F --&gt; G[Template Application]\n        G --&gt; H[Context Integration]\n    end\n\n    subgraph \"Output Generation\"\n        H --&gt; I[Enhanced Prompt]\n        I --&gt; J[LLM Processing]\n        J --&gt; K[Contextual Response]\n    end\n\n    style A fill:#e1f5fe\n    style I fill:#fff3e0\n    style K fill:#e8f5e8</code></pre>"},{"location":"development/architecture/#integration-points-with-external-services","title":"Integration Points with External Services","text":"<ul> <li>LLM Providers: OpenAI, Anthropic, Google, Azure, Ollama via <code>ruby_llm</code> gem</li> <li>File Storage: Shrine gem with configurable backends (filesystem, cloud storage)</li> <li>Background Processing: ActiveJob with multiple adapter support</li> <li>Search Extensions: Optional OpenSearch/Elasticsearch integration</li> <li>Monitoring: Built-in analytics and optional external monitoring tools</li> </ul>"},{"location":"development/architecture/#core-components","title":"Core Components","text":""},{"location":"development/architecture/#1-client-interface-layer","title":"1. Client Interface Layer","text":"<p>Primary Component: <code>Ragdoll::Core::Client</code></p> <p>Responsibilities: - High-level API facade for all RAG operations - Document lifecycle management (add, update, delete, list) - Context retrieval and prompt enhancement - Multi-modal search capabilities (semantic, hybrid, full-text) - Health monitoring and system analytics</p> <p>Key Methods: <pre><code># Document Management\nadd_document(path:) \u2192 result_hash\nadd_text(content:, title:, **options) \u2192 document_id\nadd_directory(path:, recursive: false) \u2192 results_array\n\n# Search &amp; Retrieval\nsearch(query:, **options) \u2192 search_results\nhybrid_search(query:, **options) \u2192 weighted_results\nget_context(query:, limit: 10, **options) \u2192 context_hash\nenhance_prompt(prompt:, context_limit: 5, **options) \u2192 enhanced_prompt\n\n# Analytics &amp; Health\nstats() \u2192 system_statistics\nhealthy?() \u2192 boolean\nsearch_analytics(days: 30) \u2192 analytics_data\n</code></pre></p> <p>Design Pattern: Facade Pattern - Simplifies complex subsystem interactions</p>"},{"location":"development/architecture/#2-document-processing-pipeline","title":"2. Document Processing Pipeline","text":"<p>Primary Component: <code>Ragdoll::DocumentProcessor</code></p> <p>Responsibilities: - Multi-format document parsing (PDF, DOCX, HTML, Markdown, Text) - Content extraction and normalization - Format-specific handling and validation - Error handling for malformed documents</p> <p>Related Component: <code>Ragdoll::DocumentManagement</code></p> <p>Responsibilities: - Document CRUD operations (create, read, update, delete) - Database persistence and retrieval - Document metadata management - Integration with background job processing</p> <p>Architecture Details: <pre><code>class DocumentProcessor\n  # Strategy pattern for format-specific parsing\n  PARSERS = {\n    '.pdf' =&gt; :parse_pdf,\n    '.docx' =&gt; :parse_docx,\n    '.html' =&gt; :parse_html,\n    '.md' =&gt; :parse_markdown\n  }.freeze\n\n  def parse(file_path)\n    # Returns structured result with content, metadata, and document_type\n    {\n      content: extracted_text,\n      metadata: { title:, author:, creation_date:, ... },\n      document_type: mime_type\n    }\n  end\nend\n</code></pre></p> <p>Architecture Details: <pre><code>class DocumentManagement\n  # Handles all document database operations\n  def self.add_document(location, content, metadata = {})\n    # Create document record with metadata\n    # Returns document ID for further processing\n  end\n\n  def self.get_document(id)\n    # Retrieve document with all associated content\n  end\n\n  def self.update_document(id, **updates)\n    # Update document metadata and properties\n  end\nend\n</code></pre></p> <p>Integration Points: - DocumentProcessor for content parsing - Background jobs for asynchronous processing - ActiveRecord models for data persistence</p>"},{"location":"development/architecture/#3-embedding-generation-system","title":"3. Embedding Generation System","text":"<p>Primary Component: <code>Ragdoll::EmbeddingService</code></p> <p>Responsibilities: - Vector embedding generation using multiple LLM providers - Text preprocessing and normalization - Batch processing for efficiency - Cosine similarity calculations - Provider failover and error handling</p> <p>Provider Integration: <pre><code>class EmbeddingService\n  SUPPORTED_PROVIDERS = [\n    :openai, :anthropic, :google, :azure, \n    :ollama, :huggingface, :openrouter\n  ].freeze\n\n  def generate_embedding(text)\n    # Unified interface to multiple providers via ruby_llm\n    @client.embed(text: clean_text(text))\n  end\n\n  def generate_embeddings_batch(texts)\n    # Optimized batch processing\n    texts.each_slice(batch_size).flat_map { |batch| process_batch(batch) }\n  end\nend\n</code></pre></p> <p>Design Pattern: Adapter Pattern - Unifies diverse LLM provider APIs</p>"},{"location":"development/architecture/#4-search-and-retrieval-engine","title":"4. Search and Retrieval Engine","text":"<p>Primary Component: <code>Ragdoll::SearchEngine</code></p> <p>Responsibilities: - Semantic search using vector embeddings and pgvector - Hybrid search combining semantic and full-text approaches - Multi-modal content search across text, image, and audio - Query embedding generation and similarity matching - Search result ranking and relevance scoring</p> <p>Search Types:</p> <ol> <li>Semantic Search: Vector similarity using cosine distance</li> <li>Hybrid Search: Combines semantic search with keyword extraction and filtering</li> <li>Content-Type Search: Specialized search for text, image, or audio content</li> <li>Filtered Search: Document-type and metadata-based filtering</li> </ol> <p>Performance Optimizations: <pre><code>class SearchEngine\n  def search_similar_content(query, **options)\n    # Generate query embedding for semantic search\n    embedding = @embedding_service.generate_embedding(query)\n\n    # Use pgvector for efficient similarity search\n    Ragdoll::Embedding.search_similar(\n      embedding,\n      limit: options[:limit],\n      threshold: options[:threshold],\n      filters: options[:filters]\n    )\n  end\n\n  def search_documents(query, options = {})\n    # High-level search interface with filtering\n    search_similar_content(query, options)\n  end\nend\n</code></pre></p>"},{"location":"development/architecture/#5-database-abstraction-layer","title":"5. Database Abstraction Layer","text":"<p>Primary Component: <code>Ragdoll::Core::Database</code></p> <p>Responsibilities: - Database connection management and health monitoring - Automatic migration execution on startup - Schema versioning and compatibility checks - Development database reset capabilities - Connection pooling optimization</p> <p>Key Features: <pre><code>class Database\n  def self.setup(config)\n    establish_connection(config)\n    run_migrations if config[:auto_migrate]\n    verify_extensions  # Ensure pgvector is available\n  end\n\n  def self.healthy?\n    connection.active? &amp;&amp; verify_schema_version\n  rescue StandardError\n    false\n  end\nend\n</code></pre></p>"},{"location":"development/architecture/#6-background-job-system","title":"6. Background Job System","text":"<p>Primary Component: <code>Ragdoll::GenerateEmbeddingsJob</code></p> <p>Responsibilities: - Asynchronous embedding generation for new content - LLM-powered metadata enhancement - Text extraction and chunking for large documents - Error handling and retry logic - Progress tracking and status updates</p> <p>Job Architecture: <pre><code>class GenerateEmbeddings &lt; ActiveJob::Base\n  def perform(document_id)\n    document = Ragdoll::Document.find(document_id)\n\n    # Process each content type polymorphically\n    document.content_items.each do |content|\n      generate_embeddings_for_content(content)\n    end\n\n    document.update!(status: 'processed', processed_at: Time.current)\n  end\nend\n</code></pre></p> <p>Design Pattern: Observer Pattern - Jobs triggered by model lifecycle events</p>"},{"location":"development/architecture/#architecture-decisions","title":"Architecture Decisions","text":""},{"location":"development/architecture/#choice-of-activerecord-for-orm","title":"Choice of ActiveRecord for ORM","text":"<p>Decision: Use ActiveRecord as the primary ORM with PostgreSQL</p> <p>Rationale: - Performance: Direct SQL generation with optimization capabilities - Ecosystem: Rich plugin ecosystem (pgvector, full-text search) - Migrations: Built-in schema versioning and migration management - Associations: Powerful polymorphic associations for multi-modal content - Connection Management: Built-in pooling and health monitoring</p> <p>Trade-offs: - \u2705 Familiar Rails conventions and patterns - \u2705 Excellent PostgreSQL integration - \u2705 Rich querying capabilities with Arel - \u274c Not database-agnostic (PostgreSQL-specific features used) - \u274c Potential N+1 query issues (mitigated with careful includes)</p>"},{"location":"development/architecture/#polymorphic-multi-modal-design","title":"Polymorphic Multi-Modal Design","text":"<p>Decision: Use polymorphic associations for content types (text, image, audio)</p> <p>Rationale: - Extensibility: Easy addition of new content types without schema changes - Consistency: Unified embedding storage across all content types - Performance: Single embedding table with efficient indexing - Flexibility: Content-type specific processing while maintaining common interfaces</p> <p>Implementation: <pre><code>class Document &lt; ActiveRecord::Base\n  has_many :text_contents, dependent: :destroy\n  has_many :image_contents, dependent: :destroy\n  has_many :audio_contents, dependent: :destroy\nend\n\nclass Embedding &lt; ActiveRecord::Base\n  belongs_to :embeddable, polymorphic: true  # Text, Image, or Audio content\nend\n</code></pre></p> <p>Database Schema Relationships:</p> <pre><code>erDiagram\n    DOCUMENTS {\n        uuid id PK\n        string title\n        string status\n        jsonb metadata \"LLM-generated content analysis\"\n        jsonb file_metadata \"System file properties\"\n        text content \"Extracted text content\"\n        string document_type\n        timestamp created_at\n        timestamp updated_at\n        timestamp processed_at\n    }\n\n    TEXT_CONTENTS {\n        uuid id PK\n        uuid document_id FK\n        text content\n        integer chunk_index\n        integer start_position\n        integer end_position\n        jsonb metadata\n        timestamp created_at\n        timestamp updated_at\n    }\n\n    IMAGE_CONTENTS {\n        uuid id PK\n        uuid document_id FK\n        string file_data \"Shrine attachment\"\n        string alt_text\n        jsonb metadata \"Image analysis\"\n        integer width\n        integer height\n        string format\n        timestamp created_at\n        timestamp updated_at\n    }\n\n    AUDIO_CONTENTS {\n        uuid id PK\n        uuid document_id FK\n        string file_data \"Shrine attachment\"\n        text transcript\n        integer duration_seconds\n        string format\n        jsonb metadata \"Audio analysis\"\n        timestamp created_at\n        timestamp updated_at\n    }\n\n    EMBEDDINGS {\n        uuid id PK\n        uuid embeddable_id FK\n        string embeddable_type \"Polymorphic type\"\n        vector vector \"pgvector embedding\"\n        string model_name\n        integer vector_dimensions\n        integer usage_count\n        timestamp last_used_at\n        timestamp created_at\n        timestamp updated_at\n    }\n\n    SEARCHES {\n        uuid id PK\n        text query \"Original search query\"\n        vector query_embedding \"Query vector for similarity\"\n        string search_type \"semantic, hybrid, fulltext\"\n        integer results_count \"Number of results returned\"\n        float max_similarity_score\n        float min_similarity_score\n        float avg_similarity_score\n        jsonb search_filters \"Applied filters\"\n        jsonb search_options \"Search configuration\"\n        integer execution_time_ms \"Query performance\"\n        string session_id \"User session\"\n        string user_id \"User identifier\"\n        timestamp created_at\n        timestamp updated_at\n    }\n\n    SEARCH_RESULTS {\n        uuid id PK\n        uuid search_id FK\n        uuid embedding_id FK\n        float similarity_score \"Result similarity\"\n        integer result_rank \"Position in results\"\n        boolean clicked \"User engagement\"\n        timestamp clicked_at \"Click timestamp\"\n        timestamp created_at\n        timestamp updated_at\n    }\n\n    %% Relationships\n    DOCUMENTS ||--o{ TEXT_CONTENTS : \"has_many\"\n    DOCUMENTS ||--o{ IMAGE_CONTENTS : \"has_many\"\n    DOCUMENTS ||--o{ AUDIO_CONTENTS : \"has_many\"\n\n    TEXT_CONTENTS ||--o{ EMBEDDINGS : \"embeddable (polymorphic)\"\n    IMAGE_CONTENTS ||--o{ EMBEDDINGS : \"embeddable (polymorphic)\"\n    AUDIO_CONTENTS ||--o{ EMBEDDINGS : \"embeddable (polymorphic)\"\n\n    SEARCHES ||--o{ SEARCH_RESULTS : \"has_many\"\n    EMBEDDINGS ||--o{ SEARCH_RESULTS : \"has_many\"</code></pre>"},{"location":"development/architecture/#dual-metadata-architecture","title":"Dual Metadata Architecture","text":"<p>Decision: Separate LLM-generated content metadata from file metadata</p> <p>Rationale: - Separation of Concerns: Technical file properties vs. semantic content analysis - Search Optimization: Different indexing strategies for different metadata types - Cost Management: LLM metadata generation only when needed - Extensibility: Easy addition of domain-specific metadata schemas</p> <p>Schema Design: <pre><code>class Document &lt; ActiveRecord::Base\n  # System-generated file properties\n  store :file_metadata, accessors: [:file_size, :mime_type, :dimensions, :processing_params]\n\n  # LLM-generated content analysis\n  store :metadata, accessors: [:summary, :keywords, :classification, :topics, :sentiment]\nend\n</code></pre></p>"},{"location":"development/architecture/#search-tracking-architecture","title":"Search Tracking Architecture","text":"<p>Decision: Comprehensive search analytics with vector similarity for query analysis</p> <p>Rationale: - User Behavior Analytics: Track search patterns, click-through rates, and engagement - Query Similarity: Use vector embeddings to find similar searches and improve relevance - Performance Monitoring: Measure search execution times and optimize slow queries - Session Tracking: Associate searches with users and sessions for personalization - Automatic Cleanup: Cascade deletion and orphaned search cleanup for data integrity</p> <p>Schema Design: <pre><code>class Search &lt; ActiveRecord::Base\n  # Vector similarity support for finding similar searches\n  has_neighbors :query_embedding\n  has_many :search_results, dependent: :destroy\n  has_many :embeddings, through: :search_results\n\n  # Analytics methods\n  def click_through_rate\n    return 0.0 if search_results.count.zero?\n    (search_results.clicked.count / search_results.count.to_f) * 100\n  end\n\n  # Find searches with similar queries\n  def similar_searches(limit: 5)\n    nearest_neighbors(:query_embedding, distance: :cosine).limit(limit)\n  end\nend\n\nclass SearchResult &lt; ActiveRecord::Base\n  belongs_to :search\n  belongs_to :embedding\n\n  # User engagement tracking\n  def mark_as_clicked!\n    update!(clicked: true, clicked_at: Time.current)\n  end\n\n  # Automatic cleanup when search becomes empty\n  after_destroy :cleanup_empty_search\n\n  private\n\n  def cleanup_empty_search\n    search.destroy if search.search_results.count == 0\n  end\nend\n</code></pre></p> <p>Key Features: - Automatic Recording: All searches tracked unless explicitly disabled - Vector Similarity: Query embeddings enable finding similar searches - Performance Metrics: Execution time, result counts, and similarity scores - User Engagement: Click tracking and analytics - Data Integrity: Cascade deletion and automatic cleanup</p>"},{"location":"development/architecture/#background-processing-approach","title":"Background Processing Approach","text":"<p>Decision: Use ActiveJob with asynchronous embedding generation</p> <p>Rationale: - User Experience: Non-blocking document upload and immediate response - Resource Management: Controlled concurrency for expensive LLM operations - Reliability: Retry logic and error handling for API failures - Scalability: Horizontal scaling through job queue workers</p> <p>Error Handling Strategy: <pre><code>class GenerateEmbeddings &lt; ActiveJob::Base\n  retry_on EmbeddingService::APIError, wait: :exponentially_longer, attempts: 3\n  discard_on EmbeddingService::InvalidInputError\n\n  def perform(document_id)\n    # Robust error handling with fallback strategies\n  end\nend\n</code></pre></p>"},{"location":"development/architecture/#vector-database-integration-strategy","title":"Vector Database Integration Strategy","text":"<p>Decision: Use PostgreSQL + pgvector instead of dedicated vector databases</p> <p>Rationale: - Simplicity: Single database system reduces operational complexity - Performance: Native PostgreSQL optimizations with pgvector extension - ACID Compliance: Transactional consistency between documents and embeddings - Ecosystem: Leverages existing PostgreSQL tooling and expertise</p> <p>Performance Characteristics: - IVFFlat Indexing: Sub-linear search performance O(log n) - Concurrent Access: PostgreSQL's MVCC for high concurrency - Memory Efficiency: Configurable vector dimensions and precision - Backup/Recovery: Standard PostgreSQL tools work seamlessly</p>"},{"location":"development/architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"development/architecture/#database-indexing-strategy","title":"Database Indexing Strategy","text":"<p>Vector Indexes: <pre><code>-- IVFFlat index for similarity search\nCREATE INDEX idx_embeddings_vector ON ragdoll_embeddings \nUSING ivfflat (vector vector_cosine_ops) WITH (lists = 100);\n\n-- Composite index for filtered searches\nCREATE INDEX idx_embeddings_content_vector ON ragdoll_embeddings \nUSING ivfflat (embeddable_type, vector vector_cosine_ops);\n</code></pre></p> <p>Text Search Indexes: <pre><code>-- GIN index for full-text search\nCREATE INDEX idx_documents_content_gin ON ragdoll_documents \nUSING gin(to_tsvector('english', title || ' ' || coalesce(content, '')));\n\n-- Metadata search optimization\nCREATE INDEX idx_documents_metadata_gin ON ragdoll_documents \nUSING gin(metadata);\n</code></pre></p> <p>Performance Optimizations:</p> <pre><code>graph TD\n    subgraph \"Query Optimization\"\n        A[Query Input] --&gt; B{Query Type}\n        B --&gt;|Vector Similarity| C[IVFFlat Index]\n        B --&gt;|Full-text Search| D[GIN Index]\n        B --&gt;|Metadata Filter| E[JSON Index]\n\n        C --&gt; F[pgvector Cosine Distance]\n        D --&gt; G[PostgreSQL Text Ranking]\n        E --&gt; H[JSON Path Queries]\n    end\n\n    subgraph \"Caching Strategy\"\n        I[Frequently Used Embeddings] --&gt; J[In-Memory Cache]\n        K[Search Results] --&gt; L[TTL-based Cache]\n        M[Configuration] --&gt; N[Memoization]\n    end\n\n    subgraph \"Connection Management\"\n        O[Multiple Clients] --&gt; P[Connection Pool]\n        P --&gt; Q[Load Balancing]\n        Q --&gt; R[Health Monitoring]\n    end\n\n    F --&gt; S[Result Ranking]\n    G --&gt; S\n    H --&gt; S\n    S --&gt; T[Analytics Update]\n    T --&gt; U[Optimized Results]\n\n    style A fill:#e1f5fe\n    style U fill:#e8f5e8\n    style J fill:#fff3e0\n    style P fill:#f3e5f5</code></pre> <p>Implementation Details: - Query Planning: Use of <code>EXPLAIN ANALYZE</code> for query optimization - Index Maintenance: Regular <code>VACUUM</code> and <code>ANALYZE</code> operations - Connection Pooling: ActiveRecord pool configuration for concurrency - Prepared Statements: Automatic statement caching for repeated queries</p>"},{"location":"development/architecture/#async-processing-design","title":"Async Processing Design","text":"<p>Batch Processing: - Embedding generation in configurable batch sizes (default: 10) - Parallel processing of independent content items - Memory management for large document processing</p> <p>Queue Management: <pre><code># Configuration for different job priorities\nclass GenerateEmbeddings &lt; ActiveJob::Base\n  queue_as :embeddings\n\n  # High priority for interactive operations\n  def self.perform_now_if_small(document_id)\n    document = Ragdoll::Document.find(document_id)\n    if document.estimated_processing_time &lt; 5.seconds\n      perform_now(document_id)\n    else\n      perform_later(document_id)\n    end\n  end\nend\n</code></pre></p>"},{"location":"development/architecture/#caching-strategy","title":"Caching Strategy","text":"<p>Application-Level Caching: - Configuration memoization for frequently accessed settings - Embedding result caching for repeated queries - Search result caching with TTL-based invalidation</p> <p>Database-Level Optimization: - Query result caching through PostgreSQL's shared buffers - Materialized views for complex analytics queries - Partial indexes for status-based filtering</p> <p>Memory Management: - Streaming file processing for large documents - Configurable chunk sizes balancing quality vs. performance - Connection pool sizing based on concurrency requirements</p>"},{"location":"development/architecture/#monitoring-and-observability","title":"Monitoring and Observability","text":"<p>Built-in Analytics: - Search frequency and result quality tracking - Embedding generation performance metrics - Document processing success/failure rates - API response time monitoring</p> <p>Health Checks: <pre><code>def healthy?\n  checks = {\n    database: Database.healthy?,\n    embedding_service: @embedding_service.healthy?,\n    job_queue: ActiveJob::Base.queue_adapter.healthy?\n  }\n\n  checks.all? { |_name, status| status }\nend\n</code></pre></p> <p>Performance Monitoring: - Query execution time tracking - Memory usage monitoring during document processing - Background job queue depth monitoring - LLM API latency and error rate tracking</p> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"development/contributing/","title":"Contributing","text":""},{"location":"development/contributing/#guidelines-for-contributing-to-the-project","title":"Guidelines for Contributing to the Project","text":"<p>Welcome to Ragdoll! We appreciate your interest in contributing to this PostgreSQL-focused RAG system. This guide provides comprehensive information for all types of contributions.</p>"},{"location":"development/contributing/#quick-start-for-contributors","title":"Quick Start for Contributors","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally</li> <li>Set up the development environment</li> <li>Create a feature branch</li> <li>Make your changes with tests</li> <li>Submit a pull request</li> </ol>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#development-setup","title":"Development Setup","text":""},{"location":"development/contributing/#fork-and-clone-process","title":"Fork and Clone Process","text":"<pre><code># 1. Fork the repository on GitHub (click Fork button)\n\n# 2. Clone your fork\ngit clone https://github.com/YOUR_USERNAME/ragdoll.git\ncd ragdoll\n\n# 3. Add upstream remote\ngit remote add upstream https://github.com/original-org/ragdoll.git\n\n# 4. Verify remotes\ngit remote -v\n# origin    https://github.com/YOUR_USERNAME/ragdoll.git (fetch)\n# origin    https://github.com/YOUR_USERNAME/ragdoll.git (push)\n# upstream  https://github.com/original-org/ragdoll.git (fetch)\n# upstream  https://github.com/original-org/ragdoll.git (push)\n</code></pre>"},{"location":"development/contributing/#development-environment-setup","title":"Development Environment Setup","text":"<p>Prerequisites: - Ruby 3.0+ (recommended: 3.2+) - PostgreSQL 12+ with pgvector extension - Git 2.0+ - Basic development tools (gcc, make, etc.)</p> <pre><code># Install dependencies\nbundle install\n\n# Set up environment variables\ncp .env.example .env\n# Edit .env with your API keys and database credentials\n\n# Set up PostgreSQL database\n./bin/setup_postgresql.rb\n\n# Run tests to verify setup\nbundle exec rake test\n\n# Check code style\nbundle exec rubocop\n</code></pre>"},{"location":"development/contributing/#initial-configuration","title":"Initial Configuration","text":"<pre><code># config/development.rb (create if needed)\nRagdoll::Core.configure do |config|\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_development',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true,\n    logger: Logger.new(STDOUT, level: Logger::INFO)\n  }\n\n  # Set up API keys for testing\n  config.ruby_llm_config[:openai][:api_key] = ENV['OPENAI_API_KEY']\nend\n</code></pre>"},{"location":"development/contributing/#understanding-the-codebase","title":"Understanding the Codebase","text":""},{"location":"development/contributing/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart TD\n    A[Client Layer] --&gt; B[Service Layer]\n    B --&gt; C[Model Layer] \n    C --&gt; D[Database Layer]\n\n    B --&gt; E[Jobs Layer]\n    E --&gt; F[External APIs]\n\n    D --&gt; G[PostgreSQL + pgvector]\n    F --&gt; H[LLM Providers]\n\n    subgraph \"Core Services\"\n        I[DocumentProcessor]\n        J[EmbeddingService]\n        K[SearchEngine]\n        L[TextGenerationService]\n    end\n\n    B --&gt; I\n    B --&gt; J\n    B --&gt; K\n    B --&gt; L</code></pre>"},{"location":"development/contributing/#code-organization","title":"Code Organization","text":"<pre><code>lib/ragdoll/core/\n\u251c\u2500\u2500 client.rb              # Main public API\n\u251c\u2500\u2500 configuration.rb       # System configuration\n\u251c\u2500\u2500 database.rb           # Database setup and migrations\n\u251c\u2500\u2500 document_processor.rb # Multi-format parsing\n\u251c\u2500\u2500 embedding_service.rb  # Vector generation\n\u251c\u2500\u2500 search_engine.rb      # Semantic search\n\u251c\u2500\u2500 text_chunker.rb       # Content segmentation\n\u251c\u2500\u2500 text_generation_service.rb # LLM integration\n\u251c\u2500\u2500 jobs/                 # Background processing\n\u2502   \u251c\u2500\u2500 generate_embeddings.rb\n\u2502   \u251c\u2500\u2500 extract_keywords.rb\n\u2502   \u2514\u2500\u2500 generate_summary.rb\n\u251c\u2500\u2500 models/               # ActiveRecord models\n\u2502   \u251c\u2500\u2500 document.rb\n\u2502   \u251c\u2500\u2500 embedding.rb\n\u2502   \u2514\u2500\u2500 content.rb        # STI base class\n\u2514\u2500\u2500 services/             # Specialized services\n    \u251c\u2500\u2500 metadata_generator.rb\n    \u2514\u2500\u2500 image_description_service.rb\n</code></pre>"},{"location":"development/contributing/#design-patterns","title":"Design Patterns","text":"<ul> <li>Service Layer Pattern: Business logic in dedicated service classes</li> <li>Repository Pattern: Database access through ActiveRecord models</li> <li>Factory Pattern: Document creation through DocumentProcessor</li> <li>Strategy Pattern: Multiple LLM providers via configuration</li> <li>Observer Pattern: Background jobs triggered by model callbacks</li> </ul>"},{"location":"development/contributing/#coding-conventions","title":"Coding Conventions","text":"<ul> <li>Ruby Style: Follow Ruby community conventions</li> <li>RuboCop: Automated style enforcement</li> <li>Naming: Descriptive names over comments</li> <li>Methods: Single responsibility, max 20 lines</li> <li>Classes: Max 200 lines, extract services for complex logic</li> </ul>"},{"location":"development/contributing/#contribution-types","title":"Contribution Types","text":""},{"location":"development/contributing/#code-contributions","title":"Code Contributions","text":""},{"location":"development/contributing/#bug-fixes","title":"Bug Fixes","text":"<pre><code># Create bug fix branch\ngit checkout -b fix/document-parsing-error\n\n# Write failing test first\n# test/core/document_processor_test.rb\ndef test_handles_corrupted_pdf\n  assert_raises(DocumentProcessor::ParseError) do\n    DocumentProcessor.parse('test/fixtures/corrupted.pdf')\n  end\nend\n\n# Implement fix\n# lib/ragdoll/core/document_processor.rb\ndef parse_pdf\n  # Add error handling\nrescue PDF::Reader::MalformedPDFError =&gt; e\n  raise ParseError, \"Corrupted PDF: #{e.message}\"\nend\n\n# Verify fix works\nbundle exec rake test\n</code></pre>"},{"location":"development/contributing/#feature-implementations","title":"Feature Implementations","text":"<pre><code># Create feature branch\ngit checkout -b feature/add-excel-support\n\n# Plan implementation\n# 1. Add Excel gem dependency\n# 2. Implement parse_excel method\n# 3. Add Excel to supported formats\n# 4. Write comprehensive tests\n# 5. Update documentation\n\n# Implementation example\n# Gemfile\ngem 'roo', '~&gt; 2.9'\n\n# lib/ragdoll/core/document_processor.rb\nwhen '.xlsx', '.xls'\n  parse_excel\n\nprivate\n\ndef parse_excel\n  workbook = Roo::Spreadsheet.open(@file_path)\n  content = extract_excel_content(workbook)\n  metadata = extract_excel_metadata(workbook)\n\n  {\n    content: content,\n    metadata: metadata,\n    document_type: 'excel'\n  }\nend\n</code></pre>"},{"location":"development/contributing/#performance-improvements","title":"Performance Improvements","text":"<pre><code># Example: Optimize batch embedding generation\nclass EmbeddingService\n  def generate_embeddings_batch_optimized(texts, batch_size: 50)\n    # Process in smaller batches to reduce memory usage\n    texts.each_slice(batch_size).flat_map do |batch|\n      generate_embeddings_batch(batch)\n    end\n  end\nend\n\n# Add benchmark test\nclass EmbeddingServicePerformanceTest &lt; Minitest::Test\n  def test_batch_processing_performance\n    texts = Array.new(1000) { \"Sample text #{rand(1000)}\" }\n\n    time = Benchmark.measure do\n      service.generate_embeddings_batch_optimized(texts)\n    end\n\n    assert time.real &lt; 30, \"Batch processing too slow: #{time.real}s\"\n  end\nend\n</code></pre>"},{"location":"development/contributing/#documentation-contributions","title":"Documentation Contributions","text":""},{"location":"development/contributing/#documentation-updates","title":"Documentation Updates","text":"<pre><code># Always include code examples\n## New Feature Documentation\n\n### Usage\n\n```ruby\n# Basic usage\nclient = Ragdoll::Core.client\nresult = client.new_feature(param: 'value')\n\n# Advanced usage with options\nresult = client.new_feature(\n  param: 'value',\n  advanced_option: true,\n  callback: -&gt;(data) { puts data }\n)\n</code></pre>"},{"location":"development/contributing/#configuration","title":"Configuration","text":"<p><pre><code>Ragdoll::Core.configure do |config|\n  config.new_feature_config = {\n    enabled: true,\n    timeout: 30\n  }\nend\n</code></pre> <pre><code>#### API Documentation\n\n```ruby\n# Use YARD-style documentation\nclass NewService\n  # Process documents with advanced filtering\n  #\n  # @param documents [Array&lt;Document&gt;] Documents to process\n  # @param options [Hash] Processing options\n  # @option options [String] :filter_type ('all') Filter criteria\n  # @option options [Integer] :batch_size (100) Batch processing size\n  # @return [Array&lt;Hash&gt;] Processed document results\n  # @raise [ProcessingError] When processing fails\n  #\n  # @example Basic usage\n  #   service = NewService.new\n  #   results = service.process(documents)\n  #\n  # @example With options\n  #   results = service.process(documents, \n  #     filter_type: 'academic',\n  #     batch_size: 50\n  #   )\n  def process(documents, options = {})\n    # Implementation\n  end\nend\n</code></pre></p>"},{"location":"development/contributing/#testing-contributions","title":"Testing Contributions","text":""},{"location":"development/contributing/#test-coverage-improvements","title":"Test Coverage Improvements","text":"<pre><code># Always test edge cases\nclass DocumentProcessorTest &lt; Minitest::Test\n  def test_handles_empty_file\n    File.write('test/fixtures/empty.txt', '')\n    result = DocumentProcessor.parse('test/fixtures/empty.txt')\n    assert_equal '', result[:content]\n  end\n\n  def test_handles_binary_file\n    assert_raises(DocumentProcessor::UnsupportedFormatError) do\n      DocumentProcessor.parse('test/fixtures/binary.exe')\n    end\n  end\n\n  def test_handles_very_large_file\n    # Create 100MB test file\n    large_content = 'x' * (100 * 1024 * 1024)\n    File.write('test/fixtures/large.txt', large_content)\n\n    result = DocumentProcessor.parse('test/fixtures/large.txt')\n    assert result[:content].length &gt; 0\n  ensure\n    File.delete('test/fixtures/large.txt') if File.exist?('test/fixtures/large.txt')\n  end\nend\n</code></pre>"},{"location":"development/contributing/#development-process","title":"Development Process","text":""},{"location":"development/contributing/#branch-management","title":"Branch Management","text":""},{"location":"development/contributing/#branch-naming-conventions","title":"Branch Naming Conventions","text":"<ul> <li>Features: <code>feature/short-description</code></li> <li>Bug fixes: <code>fix/issue-description</code></li> <li>Documentation: <code>docs/section-being-updated</code></li> <li>Refactoring: <code>refactor/component-name</code></li> <li>Performance: <code>perf/optimization-area</code></li> </ul>"},{"location":"development/contributing/#commit-message-format","title":"Commit Message Format","text":"<pre><code>Type: Brief description (50 characters max)\n\nDetailed explanation of what changed and why. Include:\n- What problem this solves\n- How it was implemented\n- Any breaking changes\n- References to issues (#123)\n\nTypes: feat, fix, docs, style, refactor, test, chore\n</code></pre> <p>Examples: <pre><code>feat: Add Excel document processing support\n\n- Implement parse_excel method using roo gem\n- Support .xlsx and .xls file formats\n- Extract cell values, formulas, and metadata\n- Add comprehensive test coverage\n- Update documentation with usage examples\n\nResolves #145\n</code></pre></p> <pre><code>fix: Handle corrupted PDF files gracefully\n\n- Add proper error handling for PDF::Reader exceptions\n- Return informative error messages\n- Add test cases for various corruption scenarios\n- Prevent application crashes during parsing\n\nFixes #178\n</code></pre>"},{"location":"development/contributing/#code-quality","title":"Code Quality","text":""},{"location":"development/contributing/#pre-commit-checklist","title":"Pre-commit Checklist","text":"<pre><code># Run before every commit\n\n# 1. Code style check\nbundle exec rubocop\n\n# 2. Run all tests\nbundle exec rake test\n\n# 3. Check test coverage\nopen coverage/index.html\n# Ensure coverage &gt;= 85%\n\n# 4. Run specific tests for changed code\nbundle exec rake test test/core/your_changed_test.rb\n\n# 5. Manual testing of changes\n./bin/console\n# Test your changes interactively\n</code></pre>"},{"location":"development/contributing/#code-review-preparation","title":"Code Review Preparation","text":"<pre><code># Before creating PR\n\n# 1. Rebase on latest main\ngit fetch upstream\ngit rebase upstream/main\n\n# 2. Squash commits if needed\ngit rebase -i HEAD~3\n\n# 3. Update documentation\n# Update relevant .md files\n# Add code examples\n# Update CHANGELOG.md\n\n# 4. Self-review your changes\ngit diff upstream/main..HEAD\n</code></pre>"},{"location":"development/contributing/#submission-process","title":"Submission Process","text":""},{"location":"development/contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"development/contributing/#pr-creation-guidelines","title":"PR Creation Guidelines","text":"<ol> <li>Title: Clear, concise description of changes</li> <li>Description: Use the PR template</li> <li>Labels: Add appropriate labels (bug, feature, documentation)</li> <li>Reviewers: Request reviews from maintainers</li> <li>Linked Issues: Reference related issues</li> </ol>"},{"location":"development/contributing/#pr-template","title":"PR Template","text":"<pre><code>## Summary\n\nBrief description of changes and motivation.\n\n## Changes Made\n\n- [ ] Added new feature X\n- [ ] Fixed bug in component Y\n- [ ] Updated documentation\n- [ ] Added test coverage\n\n## Testing\n\n- [ ] All existing tests pass\n- [ ] Added new tests for changes\n- [ ] Manual testing completed\n- [ ] Performance impact assessed\n\n## Documentation\n\n- [ ] Updated relevant documentation\n- [ ] Added code examples\n- [ ] Updated CHANGELOG.md\n\n## Breaking Changes\n\n- [ ] No breaking changes\n- [ ] Breaking changes documented below\n\n## Checklist\n\n- [ ] Code follows style guidelines\n- [ ] Self-review completed\n- [ ] Code is commented where needed\n- [ ] Tests cover edge cases\n</code></pre>"},{"location":"development/contributing/#review-process","title":"Review Process","text":"<ol> <li>Automated Checks: CI must pass</li> <li>Code Review: At least one maintainer approval</li> <li>Testing: Manual testing by reviewer</li> <li>Documentation: Verify docs are complete</li> <li>Merge: Squash and merge after approval</li> </ol>"},{"location":"development/contributing/#code-review-guidelines","title":"Code Review Guidelines","text":""},{"location":"development/contributing/#for-contributors","title":"For Contributors","text":"<ul> <li>Respond promptly to review feedback</li> <li>Ask questions if feedback is unclear</li> <li>Test suggested changes before implementing</li> <li>Update PR description if scope changes</li> <li>Be patient - thorough reviews take time</li> </ul>"},{"location":"development/contributing/#for-reviewers","title":"For Reviewers","text":"<ul> <li>Be constructive and specific in feedback</li> <li>Explain the why behind suggestions</li> <li>Acknowledge good code when you see it</li> <li>Test the changes locally when possible</li> <li>Consider backward compatibility impact</li> </ul>"},{"location":"development/contributing/#bug-reports","title":"Bug Reports","text":""},{"location":"development/contributing/#issue-templates","title":"Issue Templates","text":""},{"location":"development/contributing/#bug-report-template","title":"Bug Report Template","text":"<pre><code>**Bug Description**\nA clear description of what the bug is.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Configure Ragdoll with...\n2. Process document...\n3. Call search method...\n4. See error\n\n**Expected Behavior**\nWhat you expected to happen.\n\n**Actual Behavior**\nWhat actually happened, including error messages.\n\n**Environment**\n- Ruby version: [e.g. 3.2.0]\n- Ragdoll version: [e.g. 0.1.0]\n- PostgreSQL version: [e.g. 14.2]\n- Operating System: [e.g. macOS 12.0]\n\n**Additional Context**\n- Configuration details\n- Log output\n- Sample files (if applicable)\n- Stack trace\n\n**Possible Solution**\n(Optional) Suggest a fix or workaround\n</code></pre>"},{"location":"development/contributing/#information-requirements","title":"Information Requirements","text":"<p>Always include: - Reproduction steps: Minimal code to reproduce - Environment details: Ruby, PostgreSQL, OS versions - Configuration: Relevant Ragdoll configuration - Error messages: Complete stack traces - Expected vs actual: What should happen vs what happens</p> <p>For document processing issues: - File format and size - Sample file (if not sensitive) - Processing configuration</p> <p>For search issues: - Query text - Search configuration - Database state (document count, embedding count)</p>"},{"location":"development/contributing/#feature-requests","title":"Feature Requests","text":""},{"location":"development/contributing/#request-format","title":"Request Format","text":""},{"location":"development/contributing/#feature-request-template","title":"Feature Request Template","text":"<pre><code>**Feature Summary**\nOne-line summary of the requested feature.\n\n**Problem Statement**\nWhat problem does this solve? What use case does it enable?\n\n**Proposed Solution**\nDetailed description of the proposed implementation.\n\n**Alternative Solutions**\nOther approaches you've considered.\n\n**Use Cases**\nSpecific scenarios where this would be useful:\n1. Scenario 1: ...\n2. Scenario 2: ...\n\n**Implementation Considerations**\n- Database schema changes needed\n- API changes required\n- Backward compatibility impact\n- Performance implications\n\n**Priority**\n- [ ] Critical - Blocking current work\n- [ ] High - Important for upcoming release\n- [ ] Medium - Would be nice to have\n- [ ] Low - Future consideration\n\n**Would you be willing to implement this?**\n- [ ] Yes, I can submit a PR\n- [ ] Yes, with guidance\n- [ ] No, but I can test\n- [ ] No\n</code></pre>"},{"location":"development/contributing/#evaluation-process","title":"Evaluation Process","text":"<ol> <li>Initial Review: Maintainers assess fit with project goals</li> <li>Community Discussion: Gather feedback from users</li> <li>Technical Design: Plan implementation approach</li> <li>Priority Assignment: Based on impact and complexity</li> <li>Implementation: Either by maintainers or community</li> </ol>"},{"location":"development/contributing/#community-guidelines","title":"Community Guidelines","text":""},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":""},{"location":"development/contributing/#behavioral-expectations","title":"Behavioral Expectations","text":"<ul> <li>Be respectful in all interactions</li> <li>Be inclusive and welcoming to newcomers</li> <li>Be constructive in criticism and feedback</li> <li>Be patient with questions and learning</li> <li>Be professional in all communications</li> </ul>"},{"location":"development/contributing/#communication-guidelines","title":"Communication Guidelines","text":"<ul> <li>Use clear, descriptive titles for issues and PRs</li> <li>Provide context when asking questions</li> <li>Search existing issues before creating new ones</li> <li>Stay on topic in discussions</li> <li>Use appropriate channels for different types of communication</li> </ul>"},{"location":"development/contributing/#communication-channels","title":"Communication Channels","text":"<ul> <li>GitHub Issues: Bug reports, feature requests</li> <li>GitHub Discussions: General questions, ideas</li> <li>Pull Requests: Code review and discussion</li> <li>Documentation: Inline comments and suggestions</li> </ul>"},{"location":"development/contributing/#release-process","title":"Release Process","text":""},{"location":"development/contributing/#version-management","title":"Version Management","text":"<p>Ragdoll follows Semantic Versioning (SemVer):</p> <ul> <li>MAJOR.MINOR.PATCH (e.g., 1.2.3)</li> <li>MAJOR: Breaking changes</li> <li>MINOR: New features (backward compatible)</li> <li>PATCH: Bug fixes (backward compatible)</li> </ul>"},{"location":"development/contributing/#release-cycle","title":"Release Cycle","text":"<ul> <li>Patch releases: As needed for critical bugs</li> <li>Minor releases: Monthly with new features</li> <li>Major releases: Quarterly with breaking changes</li> </ul>"},{"location":"development/contributing/#changelog-maintenance","title":"Changelog Maintenance","text":"<pre><code># Changelog\n\n## [Unreleased]\n\n### Added\n- New feature descriptions\n\n### Changed\n- Modified behavior descriptions\n\n### Fixed\n- Bug fix descriptions\n\n### Removed\n- Deprecated feature removals\n\n## [1.2.0] - 2024-01-15\n\n### Added\n- Excel document processing support\n- Batch embedding optimization\n\n### Fixed\n- PDF parsing error handling\n- Memory leak in background jobs\n</code></pre>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":""},{"location":"development/contributing/#before-contributing","title":"Before Contributing","text":"<ol> <li>Read the documentation - Check existing docs first</li> <li>Search issues - Your question might already be answered</li> <li>Try the troubleshooting guide - Common issues and solutions</li> <li>Check the development guide - Setup and workflow information</li> </ol>"},{"location":"development/contributing/#need-assistance","title":"Need Assistance?","text":"<ul> <li>Questions: Use GitHub Discussions</li> <li>Bugs: Create detailed issue reports</li> <li>Features: Submit feature request with use cases</li> <li>Code: Start with small contributions and ask for guidance</li> </ul>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - README.md - Contributor section - CHANGELOG.md - Release notes - GitHub - Automatic contribution tracking - Documentation - Author attribution where appropriate</p> <p>Thank you for contributing to Ragdoll! \ud83c\udf89</p> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"development/development/","title":"Development Setup","text":""},{"location":"development/development/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<p>This guide covers the complete development environment setup for Ragdoll, including prerequisites, configuration, workflows, and development tools.</p>"},{"location":"development/development/#quick-start-for-contributors","title":"Quick Start for Contributors","text":"<pre><code># Clone the repository\ngit clone https://github.com/your-org/ragdoll.git\ncd ragdoll\n\n# Install dependencies\nbundle install\n\n# Set up database\n./bin/setup_postgresql.rb\n\n# Run tests to verify setup\nbundle exec rake test\n</code></pre>"},{"location":"development/development/#prerequisites","title":"Prerequisites","text":""},{"location":"development/development/#system-requirements","title":"System Requirements","text":""},{"location":"development/development/#ruby-version-requirements","title":"Ruby Version Requirements","text":"<ul> <li>Ruby 3.0+ (recommended: Ruby 3.2+)</li> <li>Bundler 2.0+ for dependency management</li> <li>Git 2.0+ for version control</li> </ul> <pre><code># Check versions\nruby --version    # Should be &gt;= 3.0.0\nbundle --version  # Should be &gt;= 2.0.0\ngit --version     # Should be &gt;= 2.0.0\n</code></pre>"},{"location":"development/development/#database-requirements","title":"Database Requirements","text":"<ul> <li>PostgreSQL 12+ (required - no SQLite support)</li> <li>pgvector extension for vector similarity search</li> <li>Development and test databases</li> </ul> <pre><code># macOS with Homebrew\nbrew install postgresql pgvector\nbrew services start postgresql\n\n# Ubuntu/Debian\nsudo apt-get install postgresql-14 postgresql-14-pgvector\nsudo systemctl start postgresql\nsudo systemctl enable postgresql\n</code></pre>"},{"location":"development/development/#system-dependencies","title":"System Dependencies","text":"<pre><code># macOS\nbrew install libpq imagemagick\n\n# Ubuntu/Debian\nsudo apt-get install libpq-dev imagemagick libmagickwand-dev\n\n# CentOS/RHEL\nsudo yum install postgresql-devel ImageMagick-devel\n</code></pre>"},{"location":"development/development/#environment-setup","title":"Environment Setup","text":""},{"location":"development/development/#repository-cloning-and-setup","title":"Repository Cloning and Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/your-org/ragdoll.git\ncd ragdoll\n\n# Create and switch to development branch\ngit checkout -b feature/your-feature-name\n\n# Install Ruby dependencies\nbundle install\n\n# Set up git hooks (optional)\ncp .githooks/* .git/hooks/\nchmod +x .git/hooks/*\n</code></pre>"},{"location":"development/development/#environment-variables","title":"Environment Variables","text":"<p>Create <code>.env</code> file for development: <pre><code># .env (not committed to git)\n# LLM Provider API Keys\nOPENAI_API_KEY=your_openai_api_key\nANTHROPIC_API_KEY=your_anthropic_api_key\nGOOGLE_API_KEY=your_google_api_key\n\n# Database Configuration\nDATABASE_PASSWORD=your_secure_password\nTEST_DATABASE_PASSWORD=test_password\n\n# Development Settings\nRAGDOLL_LOG_LEVEL=debug\nRAGDOLL_DEVELOPMENT=true\n</code></pre></p>"},{"location":"development/development/#database-initialization","title":"Database Initialization","text":"<pre><code># Run the setup script\n./bin/setup_postgresql.rb\n\n# Or manual setup\n# Create databases\nsudo -u postgres createdb ragdoll_development\nsudo -u postgres createdb ragdoll_test\n\n# Create user\nsudo -u postgres psql &lt;&lt;EOF\nCREATE USER ragdoll WITH PASSWORD 'secure_password';\nALTER USER ragdoll CREATEDB;\nGRANT ALL PRIVILEGES ON DATABASE ragdoll_development TO ragdoll;\nGRANT ALL PRIVILEGES ON DATABASE ragdoll_test TO ragdoll;\n\\q\nEOF\n\n# Enable pgvector extension\npsql -U ragdoll -d ragdoll_development -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\npsql -U ragdoll -d ragdoll_test -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\n</code></pre>"},{"location":"development/development/#development-configuration","title":"Development Configuration","text":""},{"location":"development/development/#development-specific-configuration","title":"Development-Specific Configuration","text":"<pre><code># config/development.rb (create if needed)\nRagdoll::Core.configure do |config|\n  # Database configuration for development\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_development',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true,\n    logger: Logger.new(STDOUT, level: Logger::DEBUG)\n  }\n\n  # Logging configuration\n  config.logging_config = {\n    level: :debug,\n    filepath: './logs/development.log'\n  }\n\n  # LLM provider configuration\n  config.ruby_llm_config[:openai][:api_key] = ENV['OPENAI_API_KEY']\n  config.ruby_llm_config[:anthropic][:api_key] = ENV['ANTHROPIC_API_KEY']\n\n  # Development-friendly settings\n  config.search[:similarity_threshold] = 0.5  # Lower threshold for testing\n  config.embedding_config[:cache_embeddings] = false  # Fresh embeddings\nend\n</code></pre>"},{"location":"development/development/#test-environment-configuration","title":"Test Environment Configuration","text":"<pre><code># test/test_helper.rb configuration\nRagdoll::Core.configure do |config|\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_test',\n    username: 'ragdoll',\n    password: ENV['TEST_DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true,\n    logger: Logger.new('/dev/null')  # Suppress SQL logs in tests\n  }\n\n  # Use inline job processing for tests\n  ActiveJob::Base.queue_adapter = :test\n\n  # Mock LLM providers in tests\n  config.ruby_llm_config[:openai][:api_key] = 'test_key'\nend\n</code></pre>"},{"location":"development/development/#local-model-setup-ollama","title":"Local Model Setup (Ollama)","text":"<pre><code># Install Ollama for local development\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Pull models for development\nollama pull llama2\nollama pull nomic-embed-text\n\n# Start Ollama server\nollama serve\n</code></pre> <pre><code># Configure Ollama in development\nRagdoll::Core.configure do |config|\n  config.ruby_llm_config[:ollama] = {\n    endpoint: 'http://localhost:11434/v1'\n  }\n\n  # Use local models\n  config.models[:default] = 'ollama/llama2'\n  config.models[:embedding][:text] = 'ollama/nomic-embed-text'\nend\n</code></pre>"},{"location":"development/development/#development-workflow","title":"Development Workflow","text":""},{"location":"development/development/#code-organization","title":"Code Organization","text":"<pre><code>lib/ragdoll/core/\n\u251c\u2500\u2500 client.rb              # Main client interface\n\u251c\u2500\u2500 configuration.rb       # Configuration management\n\u251c\u2500\u2500 database.rb           # Database connection and migrations\n\u251c\u2500\u2500 document_processor.rb # Multi-format document parsing\n\u251c\u2500\u2500 embedding_service.rb  # Vector embedding generation\n\u251c\u2500\u2500 search_engine.rb      # Semantic search implementation\n\u251c\u2500\u2500 text_chunker.rb       # Intelligent text segmentation\n\u251c\u2500\u2500 jobs/                 # ActiveJob background jobs\n\u2502   \u251c\u2500\u2500 generate_embeddings.rb\n\u2502   \u251c\u2500\u2500 extract_keywords.rb\n\u2502   \u2514\u2500\u2500 generate_summary.rb\n\u251c\u2500\u2500 models/               # ActiveRecord models\n\u2502   \u251c\u2500\u2500 document.rb\n\u2502   \u251c\u2500\u2500 embedding.rb\n\u2502   \u2514\u2500\u2500 content/          # STI content models\n\u2514\u2500\u2500 services/             # Business logic services\n    \u251c\u2500\u2500 metadata_generator.rb\n    \u2514\u2500\u2500 image_description_service.rb\n</code></pre>"},{"location":"development/development/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Classes: <code>PascalCase</code> (e.g., <code>DocumentProcessor</code>)</li> <li>Methods: <code>snake_case</code> (e.g., <code>generate_embedding</code>)</li> <li>Constants: <code>SCREAMING_SNAKE_CASE</code> (e.g., <code>DEFAULT_CONFIG</code>)</li> <li>Files: <code>snake_case.rb</code> (e.g., <code>document_processor.rb</code>)</li> <li>Database tables: <code>ragdoll_</code> prefix (e.g., <code>ragdoll_documents</code>)</li> </ul>"},{"location":"development/development/#git-workflow","title":"Git Workflow","text":"<pre><code># Feature development workflow\ngit checkout main\ngit pull origin main\ngit checkout -b feature/add-new-processor\n\n# Make changes\ngit add .\ngit commit -m \"Add support for Excel file processing\n\n- Implement ExcelProcessor class\n- Add Excel MIME type detection\n- Include tests for Excel parsing\n- Update documentation\"\n\n# Push feature branch\ngit push origin feature/add-new-processor\n\n# Create pull request (via GitHub UI)\n</code></pre>"},{"location":"development/development/#commit-message-format","title":"Commit Message Format","text":"<pre><code>Type: Brief description (50 chars max)\n\n- Detailed bullet point 1\n- Detailed bullet point 2\n- Reference to issue #123\n\nTypes: feat, fix, docs, style, refactor, test, chore\n</code></pre>"},{"location":"development/development/#testing-setup","title":"Testing Setup","text":""},{"location":"development/development/#test-framework-configuration","title":"Test Framework Configuration","text":"<p>Ragdoll uses Minitest with custom helpers:</p> <pre><code># test/test_helper.rb\nrequire 'minitest/autorun'\nrequire 'minitest/reporters'\nrequire 'simplecov'\n\n# Start SimpleCov for coverage analysis\nSimpleCov.start do\n  add_filter '/test/'\n  add_filter '/vendor/'\n  minimum_coverage 85\nend\n\nrequire 'ragdoll'\n\n# Test database setup\nclass Minitest::Test\n  def setup\n    # Clean database before each test\n    Ragdoll::Core::Database.reset!\n  end\n\n  def teardown\n    # Clean up after each test\n    ActiveRecord::Base.clear_all_connections!\n  end\nend\n\n# Custom assertions\nmodule TestHelpers\n  def assert_embedding_generated(text)\n    service = Ragdoll::EmbeddingService.new\n    embedding = service.generate_embedding(text)\n    assert embedding.is_a?(Array), \"Expected array, got #{embedding.class}\"\n    assert embedding.length &gt; 0, \"Expected non-empty embedding\"\n  end\n\n  def assert_document_processed(document_id)\n    document = Ragdoll::Document.find(document_id)\n    assert_equal 'processed', document.status\n    assert document.content.present?\n  end\nend\n\nMinitest::Test.include(TestHelpers)\n</code></pre>"},{"location":"development/development/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nbundle exec rake test\n\n# Run specific test file\nbundle exec rake test test/core/client_test.rb\n\n# Run tests with coverage report\nRAILS_ENV=test bundle exec rake test\nopen coverage/index.html\n\n# Run tests with verbose output\nbundle exec rake test TESTOPTS=\"-v\"\n\n# Run performance tests (if implemented)\nbundle exec rake test:performance\n</code></pre>"},{"location":"development/development/#test-categories","title":"Test Categories","text":""},{"location":"development/development/#unit-tests","title":"Unit Tests","text":"<pre><code># test/core/embedding_service_test.rb\nclass EmbeddingServiceTest &lt; Minitest::Test\n  def setup\n    @service = Ragdoll::EmbeddingService.new\n  end\n\n  def test_generates_embedding_for_text\n    text = \"This is a test document\"\n    embedding = @service.generate_embedding(text)\n\n    assert_instance_of Array, embedding\n    assert embedding.length &gt; 0\n    assert embedding.all? { |val| val.is_a?(Numeric) }\n  end\nend\n</code></pre>"},{"location":"development/development/#integration-tests","title":"Integration Tests","text":"<pre><code># test/integration/document_processing_test.rb\nclass DocumentProcessingTest &lt; Minitest::Test\n  def test_complete_document_workflow\n    # Add document\n    client = Ragdoll::Core.client\n    result = client.add_document(path: 'test/fixtures/sample.pdf')\n\n    assert result[:success]\n    document_id = result[:document_id]\n\n    # Verify processing (background jobs run inline in test)\n    document = Ragdoll::Document.find(document_id)\n    assert_equal 'processed', document.status\n    assert document.embeddings.any?\n\n    # Test search\n    search_results = client.search(query: \"sample document\")\n    assert search_results[:results].any?\n  end\nend\n</code></pre>"},{"location":"development/development/#development-tools","title":"Development Tools","text":""},{"location":"development/development/#code-quality-tools","title":"Code Quality Tools","text":""},{"location":"development/development/#rubocop-configuration","title":"RuboCop Configuration","text":"<pre><code># .rubocop.yml\nAllCops:\n  TargetRubyVersion: 3.0\n  NewCops: enable\n  Exclude:\n    - 'vendor/**/*'\n    - 'db/migrate/*'\n    - 'coverage/**/*'\n    - 'pkg/**/*'\n    - 'tmp/**/*'\n\nMetrics/LineLength:\n  Max: 120\n  AllowedPatterns: ['^\\s*#']\n\nStyle/Documentation:\n  Enabled: false  # Disable for now, enable later\n\nStyle/FrozenStringLiteralComment:\n  Enabled: true\n  EnforcedStyle: always\n</code></pre> <pre><code># Run RuboCop\nbundle exec rubocop\n\n# Auto-fix issues\nbundle exec rubocop -a\n\n# Check specific files\nbundle exec rubocop lib/ragdoll/core/client.rb\n</code></pre>"},{"location":"development/development/#simplecov-configuration","title":"SimpleCov Configuration","text":"<pre><code># .simplecov\nSimpleCov.configure do\n  load_profile 'test_frameworks'\n\n  add_filter '/test/'\n  add_filter '/vendor/'\n  add_filter 'version.rb'\n\n  add_group 'Models', 'lib/ragdoll/core/models'\n  add_group 'Services', 'lib/ragdoll/core/services'\n  add_group 'Jobs', 'lib/ragdoll/core/jobs'\n\n  minimum_coverage 85\n  minimum_coverage_by_file 70\nend\n</code></pre>"},{"location":"development/development/#debugging-tools","title":"Debugging Tools","text":""},{"location":"development/development/#console-development","title":"Console Development","text":"<pre><code># bin/console (create executable script)\n#!/usr/bin/env ruby\n\nrequire 'bundler/setup'\nrequire 'ragdoll'\nrequire 'irb'\n\n# Load development configuration\nRagdoll::Core.configure do |config|\n  # Development-friendly settings\nend\n\n# Helper methods for console\ndef reload!\n  load 'lib/ragdoll.rb'\nend\n\ndef test_client\n  @test_client ||= Ragdoll::Core.client\nend\n\ndef sample_document\n  test_client.add_document(path: 'test/fixtures/sample.pdf')\nend\n\nputs \"Ragdoll development console\"\nputs \"Available helpers: reload!, test_client, sample_document\"\n\nIRB.start\n</code></pre> <pre><code># Make console executable and run\nchmod +x bin/console\n./bin/console\n</code></pre>"},{"location":"development/development/#memory-profiling","title":"Memory Profiling","text":"<pre><code># Development memory profiler\nrequire 'memory_profiler'\n\nreport = MemoryProfiler.report do\n  # Code to profile\n  client = Ragdoll::Core.client\n  client.add_document(path: 'large_document.pdf')\nend\n\nreport.pretty_print\n</code></pre>"},{"location":"development/development/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code># lib/ragdoll/core/development/profiler.rb\nmodule Ragdoll\n  module Core\n    module Development\n      class Profiler\n        def self.benchmark(description, &amp;block)\n          puts \"Benchmarking: #{description}\"\n          start_time = Time.current\n          start_memory = get_memory_usage\n\n          result = yield\n\n          end_time = Time.current\n          end_memory = get_memory_usage\n\n          puts \"  Time: #{((end_time - start_time) * 1000).round(2)}ms\"\n          puts \"  Memory: #{(end_memory - start_memory).round(2)}MB\"\n\n          result\n        end\n\n        private\n\n        def self.get_memory_usage\n          `ps -o rss= -p #{Process.pid}`.to_i / 1024.0\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"development/development/#local-development-features","title":"Local Development Features","text":""},{"location":"development/development/#development-scripts","title":"Development Scripts","text":"<pre><code># bin/dev_server (development server script)\n#!/bin/bash\necho \"Starting Ragdoll development environment...\"\n\n# Start PostgreSQL if not running\nif ! pgrep -x \"postgres\" &gt; /dev/null; then\n    echo \"Starting PostgreSQL...\"\n    brew services start postgresql\nfi\n\n# Start Ollama if configured\nif command -v ollama &amp;&gt; /dev/null; then\n    echo \"Starting Ollama server...\"\n    ollama serve &amp;\nfi\n\n# Run tests to verify setup\necho \"Running quick health check...\"\nbundle exec rake test:quick\n\necho \"Development environment ready!\"\necho \"Run './bin/console' to start interactive session\"\n</code></pre>"},{"location":"development/development/#auto-reloading-development","title":"Auto-reloading Development","text":"<pre><code># Development auto-reloader using Listen gem\nrequire 'listen'\n\nlistener = Listen.to('lib') do |modified, added, removed|\n  puts \"Files changed: #{modified + added + removed}\"\n  puts \"Reloading Ragdoll...\"\n\n  # Reload the library\n  Object.send(:remove_const, :Ragdoll) if defined?(Ragdoll)\n  load 'lib/ragdoll.rb'\n\n  puts \"Reload complete!\"\nend\n\nlistener.start\nsleep\n</code></pre>"},{"location":"development/development/#contributing-workflow","title":"Contributing Workflow","text":""},{"location":"development/development/#setting-up-for-contribution","title":"Setting Up for Contribution","text":"<ol> <li> <p>Fork the Repository <pre><code># Fork on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/ragdoll.git\ncd ragdoll\n\n# Add upstream remote\ngit remote add upstream https://github.com/original-org/ragdoll.git\n</code></pre></p> </li> <li> <p>Create Feature Branch <pre><code>git checkout -b feature/your-awesome-feature\n</code></pre></p> </li> <li> <p>Development Process</p> </li> <li>Write tests first (TDD approach)</li> <li>Implement feature</li> <li>Ensure all tests pass</li> <li>Run RuboCop for style compliance</li> <li> <p>Update documentation</p> </li> <li> <p>Testing Requirements <pre><code># All tests must pass\nbundle exec rake test\n\n# Code coverage must be &gt;= 85%\n# RuboCop must pass\nbundle exec rubocop\n\n# Documentation must be updated\n</code></pre></p> </li> </ol>"},{"location":"development/development/#submission-process","title":"Submission Process","text":"<ol> <li>Pre-submission Checklist</li> <li> All tests pass</li> <li> Code coverage maintained</li> <li> RuboCop passes</li> <li> Documentation updated</li> <li> <p> Commit messages follow conventions</p> </li> <li> <p>Create Pull Request <pre><code>git push origin feature/your-awesome-feature\n# Create PR via GitHub UI\n</code></pre></p> </li> <li> <p>PR Requirements</p> </li> <li>Clear description of changes</li> <li>Reference to related issues</li> <li>Before/after examples if applicable</li> <li>Migration notes if needed</li> </ol>"},{"location":"development/development/#development-best-practices","title":"Development Best Practices","text":""},{"location":"development/development/#code-style-guidelines","title":"Code Style Guidelines","text":"<ul> <li>Follow Ruby community conventions</li> <li>Use meaningful variable and method names</li> <li>Keep methods focused and small</li> <li>Write self-documenting code</li> <li>Add comments for complex business logic</li> </ul>"},{"location":"development/development/#testing-philosophy","title":"Testing Philosophy","text":"<ul> <li>Write tests first (TDD)</li> <li>Test behavior not implementation</li> <li>Use descriptive test names</li> <li>Keep tests isolated and fast</li> <li>Mock external services</li> </ul>"},{"location":"development/development/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Profile code changes with large datasets</li> <li>Monitor memory usage in background jobs</li> <li>Use database indexes appropriately</li> <li>Cache expensive operations when possible</li> <li>Consider batch processing for large operations</li> </ul>"},{"location":"development/development/#documentation-standards","title":"Documentation Standards","text":"<ul> <li>Update documentation with code changes</li> <li>Include code examples in documentation</li> <li>Use Mermaid diagrams for complex workflows</li> <li>Document breaking changes clearly</li> <li>Maintain accurate API references</li> </ul> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"development/extending/","title":"Extending the System","text":""},{"location":"development/extending/#adding-new-content-types-and-processors","title":"Adding New Content Types and Processors","text":"<p>Ragdoll is designed for extensibility with clear patterns for adding new content types, document processors, embedding providers, and search algorithms. This guide shows how to extend the system while maintaining PostgreSQL-first architecture.</p>"},{"location":"development/extending/#extension-architecture-overview","title":"Extension Architecture Overview","text":"<pre><code>flowchart TD\n    A[Custom Extension] --&gt; B[Extension Registry]\n    B --&gt; C[Core System]\n\n    subgraph \"Extension Points\"\n        D[Document Processors]\n        E[Content Models]\n        F[Embedding Providers]\n        G[Search Algorithms]\n        H[Metadata Schemas]\n    end\n\n    C --&gt; D\n    C --&gt; E\n    C --&gt; F\n    C --&gt; G\n    C --&gt; H\n\n    subgraph \"Database Layer\"\n        I[PostgreSQL + pgvector]\n        J[STI Content Models]\n        K[Polymorphic Associations]\n    end\n\n    E --&gt; I\n    E --&gt; J\n    E --&gt; K</code></pre>"},{"location":"development/extending/#content-type-extensions","title":"Content Type Extensions","text":""},{"location":"development/extending/#custom-content-models","title":"Custom Content Models","text":"<p>Extend the STI (Single Table Inheritance) content model system:</p> <pre><code># lib/ragdoll/core/models/video_content.rb\nmodule Ragdoll\n  module Core\n    module Models\n      class VideoContent &lt; Content\n        # STI automatically handled by ActiveRecord\n\n        validates :content, presence: true  # content field stores transcript\n\n        # Video-specific methods\n        def transcript\n          content  # STI content field stores transcript\n        end\n\n        def transcript=(value)\n          self.content = value\n        end\n\n        def duration\n          metadata['duration']\n        end\n\n        def resolution\n          \"#{metadata['width']}x#{metadata['height']}\"\n        end\n\n        def video_codec\n          metadata['codec']\n        end\n\n        # Override embedding generation for video-specific handling\n        def generate_embeddings!\n          return unless transcript.present?\n\n          # Use video-specific embedding model\n          service = EmbeddingService.new\n          chunks = TextChunker.chunk(transcript, \n            chunk_size: Ragdoll.config.chunking[:audio][:max_tokens],\n            chunk_overlap: Ragdoll.config.chunking[:audio][:overlap]\n          )\n\n          chunks.each_with_index do |chunk, index|\n            embedding_vector = service.generate_embedding(chunk)\n            next unless embedding_vector\n\n            embeddings.create!(\n              chunk_index: index,\n              embedding_vector: embedding_vector,\n              content: chunk,\n              metadata: { content_type: 'video_transcript' }\n            )\n          end\n        end\n\n        # Custom search relevance for video content\n        def search_boost_factor\n          # Boost based on video quality and duration\n          duration_boost = [duration.to_f / 3600, 2.0].min  # Max 2x for longer videos\n          quality_boost = resolution.include?('1080') ? 1.2 : 1.0\n\n          duration_boost * quality_boost\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"development/extending/#document-model-integration","title":"Document Model Integration","text":"<p>Extend the Document model to handle new content types:</p> <pre><code># Add to existing Document model\nmodule Ragdoll\n  module Core\n    module Models\n      class Document\n        # Add video content relationship\n        has_many :video_contents,\n                 -&gt; { where(type: \"Ragdoll::VideoContent\") },\n                 class_name: \"Ragdoll::VideoContent\",\n                 foreign_key: \"document_id\"\n\n        has_many :video_embeddings, through: :video_contents, source: :embeddings\n\n        # Update content_types method\n        def content_types\n          types = []\n          types &lt;&lt; \"text\" if text_contents.any?\n          types &lt;&lt; \"image\" if image_contents.any?\n          types &lt;&lt; \"audio\" if audio_contents.any?\n          types &lt;&lt; \"video\" if video_contents.any?  # Add this line\n          types\n        end\n\n        # Update content method for video support\n        def content\n          case primary_content_type\n          when \"text\"\n            text_contents.pluck(:content).compact.join(\"\\n\\n\")\n          when \"image\"\n            image_contents.pluck(:content).compact.join(\"\\n\\n\")\n          when \"audio\"\n            audio_contents.pluck(:content).compact.join(\"\\n\\n\")\n          when \"video\"\n            video_contents.pluck(:content).compact.join(\"\\n\\n\")  # Add this\n          else\n            contents.pluck(:content).compact.join(\"\\n\\n\")\n          end\n        end\n\n        # Update all_embeddings method\n        def all_embeddings(content_type: nil)\n          content_ids = []\n\n          if content_type\n            case content_type.to_s\n            when 'text'\n              content_ids.concat(text_contents.pluck(:id))\n            when 'image'\n              content_ids.concat(image_contents.pluck(:id))\n            when 'audio'\n              content_ids.concat(audio_contents.pluck(:id))\n            when 'video'  # Add this\n              content_ids.concat(video_contents.pluck(:id))\n            end\n          else\n            content_ids.concat(text_contents.pluck(:id))\n            content_ids.concat(image_contents.pluck(:id))\n            content_ids.concat(audio_contents.pluck(:id))\n            content_ids.concat(video_contents.pluck(:id))  # Add this\n          end\n\n          return Embedding.none if content_ids.empty?\n\n          Embedding.where(\n            embeddable_type: 'Ragdoll::Content',\n            embeddable_id: content_ids\n          )\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"development/extending/#document-processor-extensions","title":"Document Processor Extensions","text":""},{"location":"development/extending/#custom-file-processors","title":"Custom File Processors","text":"<p>Extend DocumentProcessor to handle new file formats:</p> <pre><code># Extend DocumentProcessor class\nmodule Ragdoll\n  module Core\n    class DocumentProcessor\n      # Add video support to parse method\n      def parse\n        case @file_extension\n        when \".pdf\"\n          parse_pdf\n        when \".docx\"\n          parse_docx\n        when \".txt\", \".md\", \".markdown\"\n          parse_text\n        when \".html\", \".htm\"\n          parse_html\n        when \".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".webp\", \".svg\", \".ico\", \".tiff\", \".tif\"\n          parse_image\n        when \".mp4\", \".avi\", \".mov\", \".wmv\", \".flv\", \".webm\"  # Add video formats\n          parse_video\n        when \".xlsx\", \".xls\"  # Add Excel support\n          parse_excel\n        else\n          parse_text # Default fallback\n        end\n      rescue StandardError =&gt; e\n        raise ParseError, \"Failed to parse #{@file_path}: #{e.message}\"\n      end\n\n      private\n\n      # New video parser\n      def parse_video\n        require 'streamio-ffmpeg'  # Add to Gemfile\n\n        metadata = {\n          file_size: File.size(@file_path),\n          file_type: @file_extension.sub(\".\", \"\"),\n          original_filename: File.basename(@file_path)\n        }\n\n        begin\n          # Extract video metadata using FFmpeg\n          movie = FFMPEG::Movie.new(@file_path)\n\n          metadata.merge!({\n            duration: movie.duration,\n            width: movie.width,\n            height: movie.height,\n            frame_rate: movie.frame_rate,\n            codec: movie.video_codec,\n            audio_codec: movie.audio_codec,\n            bitrate: movie.bitrate\n          })\n\n          # Extract transcript if available (placeholder)\n          # In production, integrate with speech-to-text service\n          transcript = extract_video_transcript(movie)\n\n        rescue StandardError =&gt; e\n          puts \"Warning: Could not extract video metadata: #{e.message}\"\n          transcript = \"Video file: #{File.basename(@file_path)}\"\n        end\n\n        {\n          content: transcript,\n          metadata: metadata,\n          document_type: \"video\"\n        }\n      end\n\n      # New Excel parser\n      def parse_excel\n        require 'roo'  # Add to Gemfile\n\n        content = \"\"\n        metadata = {\n          file_size: File.size(@file_path),\n          file_type: @file_extension.sub(\".\", \"\"),\n          original_filename: File.basename(@file_path)\n        }\n\n        begin\n          workbook = Roo::Spreadsheet.open(@file_path)\n\n          # Process each sheet\n          workbook.sheets.each_with_index do |sheet_name, index|\n            workbook.sheet(sheet_name)\n\n            content += \"\\n\\n--- Sheet: #{sheet_name} ---\\n\\n\" if index &gt; 0\n\n            # Process rows\n            (workbook.first_row..workbook.last_row).each do |row_num|\n              row_data = (workbook.first_column..workbook.last_column).map do |col|\n                workbook.cell(row_num, col)&amp;.to_s&amp;.strip\n              end.compact.reject(&amp;:empty?)\n\n              next if row_data.empty?\n\n              content += row_data.join(\" | \") + \"\\n\"\n            end\n          end\n\n          metadata.merge!({\n            sheet_count: workbook.sheets.count,\n            sheets: workbook.sheets\n          })\n\n        rescue StandardError =&gt; e\n          raise ParseError, \"Failed to parse Excel file: #{e.message}\"\n        end\n\n        {\n          content: content.strip,\n          metadata: metadata,\n          document_type: \"excel\"\n        }\n      end\n\n      def extract_video_transcript(movie)\n        # Placeholder for speech-to-text integration\n        # In production, integrate with:\n        # - OpenAI Whisper API\n        # - Google Speech-to-Text\n        # - Azure Speech Services\n\n        \"Video transcript extraction not yet implemented. \"\n        \"Duration: #{movie.duration}s, Resolution: #{movie.width}x#{movie.height}\"\n      end\n\n      # Update helper methods\n      def self.determine_document_type(file_path)\n        case File.extname(file_path).downcase\n        when \".pdf\" then \"pdf\"\n        when \".docx\" then \"docx\"\n        when \".txt\" then \"text\"\n        when \".md\", \".markdown\" then \"markdown\"\n        when \".html\", \".htm\" then \"html\"\n        when \".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".webp\", \".svg\", \".ico\", \".tiff\", \".tif\" then \"image\"\n        when \".mp4\", \".avi\", \".mov\", \".wmv\", \".flv\", \".webm\" then \"video\"  # Add this\n        when \".xlsx\", \".xls\" then \"excel\"  # Add this\n        else \"text\"\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"development/extending/#embedding-provider-extensions","title":"Embedding Provider Extensions","text":""},{"location":"development/extending/#custom-embedding-providers","title":"Custom Embedding Providers","text":"<p>Add support for new embedding providers:</p> <pre><code># lib/ragdoll/core/embedding_providers/custom_provider.rb\nmodule Ragdoll\n  module Core\n    module EmbeddingProviders\n      class CustomProvider\n        def initialize(config = {})\n          @api_key = config[:api_key] || ENV['CUSTOM_PROVIDER_API_KEY']\n          @endpoint = config[:endpoint] || 'https://api.customprovider.com'\n          @model = config[:model] || 'custom-embedding-v1'\n        end\n\n        def generate_embedding(text)\n          response = make_request('/embeddings', {\n            input: text,\n            model: @model\n          })\n\n          parse_embedding_response(response)\n        end\n\n        def generate_embeddings_batch(texts)\n          response = make_request('/embeddings/batch', {\n            inputs: texts,\n            model: @model\n          })\n\n          parse_batch_response(response)\n        end\n\n        def supports_batch?\n          true\n        end\n\n        def max_batch_size\n          100\n        end\n\n        def embedding_dimensions\n          1536  # Return the dimension count for this provider\n        end\n\n        private\n\n        def make_request(path, data)\n          require 'faraday'\n\n          conn = Faraday.new(url: @endpoint) do |f|\n            f.request :json\n            f.response :json\n          end\n\n          response = conn.post(path) do |req|\n            req.headers['Authorization'] = \"Bearer #{@api_key}\"\n            req.headers['Content-Type'] = 'application/json'\n            req.body = data\n          end\n\n          handle_response(response)\n        end\n\n        def handle_response(response)\n          case response.status\n          when 200..299\n            response.body\n          when 429\n            raise EmbeddingError, \"Rate limit exceeded for custom provider\"\n          when 401\n            raise EmbeddingError, \"Authentication failed for custom provider\"\n          else\n            raise EmbeddingError, \"Custom provider error: #{response.status}\"\n          end\n        end\n\n        def parse_embedding_response(response)\n          response.dig('data', 0, 'embedding') || \n            response['embedding'] ||\n            raise(EmbeddingError, \"Invalid response format from custom provider\")\n        end\n\n        def parse_batch_response(response)\n          if response['data']\n            response['data'].map { |item| item['embedding'] }\n          elsif response['embeddings']\n            response['embeddings']\n          else\n            raise EmbeddingError, \"Invalid batch response format from custom provider\"\n          end\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"development/extending/#provider-integration","title":"Provider Integration","text":"<p>Integrate the custom provider with the main EmbeddingService:</p> <pre><code># Extend EmbeddingService configuration\nmodule Ragdoll\n  module Core\n    class EmbeddingService\n      private\n\n      def configure_ruby_llm\n        # Existing provider configuration...\n\n        # Add custom provider support\n        if Ragdoll.config.embedding_config[:provider] == :custom\n          @custom_provider = EmbeddingProviders::CustomProvider.new(\n            Ragdoll.config.ruby_llm_config[:custom] || {}\n          )\n          return\n        end\n\n        # Existing RubyLLM configuration...\n      end\n\n      def generate_embedding(text)\n        return nil if text.nil? || text.strip.empty?\n\n        cleaned_text = clean_text(text)\n\n        # Use custom provider if configured\n        if @custom_provider\n          return @custom_provider.generate_embedding(cleaned_text)\n        end\n\n        # Existing implementation...\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"development/extending/#search-engine-extensions","title":"Search Engine Extensions","text":""},{"location":"development/extending/#custom-search-algorithms","title":"Custom Search Algorithms","text":"<p>Implement specialized search algorithms:</p> <pre><code># lib/ragdoll/core/search_algorithms/semantic_hybrid.rb\nmodule Ragdoll\n  module Core\n    module SearchAlgorithms\n      class SemanticHybrid\n        def initialize(embedding_service, options = {})\n          @embedding_service = embedding_service\n          @semantic_weight = options[:semantic_weight] || 0.7\n          @keyword_weight = options[:keyword_weight] || 0.3\n          @boost_recent = options[:boost_recent] || false\n        end\n\n        def search(query, **options)\n          # Generate query embedding\n          query_embedding = @embedding_service.generate_embedding(query)\n\n          # Perform semantic search\n          semantic_results = semantic_search(query_embedding, options)\n\n          # Perform keyword search\n          keyword_results = keyword_search(query, options)\n\n          # Combine and rank results\n          combined_results = combine_results(semantic_results, keyword_results)\n\n          # Apply additional ranking factors\n          ranked_results = apply_ranking_factors(combined_results, options)\n\n          ranked_results.take(options[:limit] || 10)\n        end\n\n        private\n\n        def semantic_search(query_embedding, options)\n          Ragdoll::Embedding.search_similar(\n            query_embedding,\n            limit: options[:limit] || 50,\n            threshold: options[:threshold] || 0.5,\n            filters: options[:filters] || {}\n          ).map do |result|\n            result.merge(\n              search_type: 'semantic',\n              base_score: result[:similarity]\n            )\n          end\n        end\n\n        def keyword_search(query, options)\n          # Use PostgreSQL full-text search\n          documents = Ragdoll::Document.search_content(\n            query,\n            limit: options[:limit] || 50\n          )\n\n          documents.map.with_index do |doc, index|\n            # Calculate keyword relevance score\n            relevance = calculate_keyword_relevance(doc, query)\n\n            {\n              document_id: doc.id.to_s,\n              document_title: doc.title,\n              document_location: doc.location,\n              content: doc.content[0..500],\n              search_type: 'keyword',\n              base_score: relevance,\n              similarity: relevance\n            }\n          end\n        end\n\n        def combine_results(semantic_results, keyword_results)\n          # Merge results by document_id\n          combined = {}\n\n          semantic_results.each do |result|\n            doc_id = result[:document_id]\n            combined[doc_id] = result.merge(\n              semantic_score: result[:base_score] * @semantic_weight\n            )\n          end\n\n          keyword_results.each do |result|\n            doc_id = result[:document_id]\n\n            if combined[doc_id]\n              # Combine scores\n              combined[doc_id][:keyword_score] = result[:base_score] * @keyword_weight\n              combined[doc_id][:combined_score] = \n                combined[doc_id][:semantic_score] + (result[:base_score] * @keyword_weight)\n              combined[doc_id][:search_types] = ['semantic', 'keyword']\n            else\n              # Keyword-only result\n              combined[doc_id] = result.merge(\n                keyword_score: result[:base_score] * @keyword_weight,\n                combined_score: result[:base_score] * @keyword_weight,\n                search_types: ['keyword']\n              )\n            end\n          end\n\n          combined.values\n        end\n\n        def apply_ranking_factors(results, options)\n          results.map do |result|\n            score = result[:combined_score] || result[:similarity]\n\n            # Apply recency boost if enabled\n            if @boost_recent\n              doc = Ragdoll::Document.find(result[:document_id])\n              days_old = (Time.current - doc.created_at) / 1.day\n              recency_boost = [1.0 - (days_old / 365), 0.1].max  # Decay over a year\n              score *= (1.0 + recency_boost * 0.2)  # Up to 20% boost for recent docs\n            end\n\n            # Apply content type boost\n            if options[:boost_content_types]\n              content_boost = calculate_content_type_boost(result, options[:boost_content_types])\n              score *= content_boost\n            end\n\n            result.merge(final_score: score)\n          end.sort_by { |r| -r[:final_score] }\n        end\n\n        def calculate_keyword_relevance(document, query)\n          # Simple TF-IDF-like calculation\n          query_terms = query.downcase.split(/\\W+/).reject(&amp;:empty?)\n          content = document.content.downcase\n\n          term_frequencies = query_terms.map do |term|\n            content.scan(/\\b#{Regexp.escape(term)}\\b/).count\n          end\n\n          # Normalize by content length and query terms\n          total_matches = term_frequencies.sum\n          content_length = content.split(/\\W+/).length\n\n          return 0.0 if content_length == 0\n\n          (total_matches.to_f / content_length) * query_terms.length\n        end\n\n        def calculate_content_type_boost(result, boost_config)\n          doc = Ragdoll::Document.find(result[:document_id])\n          boost_config[doc.document_type&amp;.to_sym] || 1.0\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"development/extending/#metadata-schema-extensions","title":"Metadata Schema Extensions","text":""},{"location":"development/extending/#custom-schema-types","title":"Custom Schema Types","text":"<p>Extend metadata schemas for new content types:</p> <pre><code># lib/ragdoll/core/metadata_schemas/video_schema.rb\nmodule Ragdoll\n  module Core\n    module MetadataSchemas\n      class VideoSchema &lt; BaseSchema\n        SCHEMA = {\n          type: 'object',\n          properties: {\n            # Basic video properties\n            duration: {\n              type: 'number',\n              description: 'Video duration in seconds',\n              minimum: 0\n            },\n            resolution: {\n              type: 'string',\n              description: 'Video resolution (e.g., 1920x1080)',\n              pattern: '^\\\\d+x\\\\d+$'\n            },\n            codec: {\n              type: 'string',\n              description: 'Video codec used',\n              enum: ['h264', 'h265', 'vp8', 'vp9', 'av1']\n            },\n\n            # Content classification\n            category: {\n              type: 'string',\n              description: 'Video category',\n              enum: ['educational', 'entertainment', 'documentary', 'tutorial', 'presentation']\n            },\n            topics: {\n              type: 'array',\n              description: 'Main topics covered in the video',\n              items: { type: 'string' },\n              maxItems: 10\n            },\n\n            # Quality indicators\n            transcript_quality: {\n              type: 'string',\n              description: 'Quality of extracted transcript',\n              enum: ['high', 'medium', 'low', 'none']\n            },\n            audio_quality: {\n              type: 'string',\n              description: 'Audio quality assessment',\n              enum: ['excellent', 'good', 'fair', 'poor']\n            },\n\n            # Accessibility\n            has_captions: {\n              type: 'boolean',\n              description: 'Whether video has captions/subtitles'\n            },\n            language: {\n              type: 'string',\n              description: 'Primary language of the video content'\n            }\n          },\n          required: ['duration']\n        }.freeze\n\n        def self.generate_metadata(document)\n          return {} unless document.document_type == 'video'\n\n          video_content = document.video_contents.first\n          return {} unless video_content\n\n          metadata = {\n            duration: video_content.duration,\n            resolution: video_content.resolution,\n            codec: video_content.video_codec\n          }\n\n          # AI-powered content analysis\n          if video_content.transcript.present?\n            ai_analysis = analyze_video_content(video_content.transcript)\n            metadata.merge!(ai_analysis)\n          end\n\n          metadata\n        end\n\n        private\n\n        def self.analyze_video_content(transcript)\n          # Use TextGenerationService for content analysis\n          generator = TextGenerationService.new\n\n          # Extract topics\n          topics = generator.extract_keywords(transcript, max_keywords: 5)\n\n          # Classify category (simplified)\n          category = classify_video_category(transcript)\n\n          # Assess transcript quality\n          transcript_quality = assess_transcript_quality(transcript)\n\n          {\n            topics: topics,\n            category: category,\n            transcript_quality: transcript_quality,\n            language: detect_language(transcript)\n          }\n        end\n\n        def self.classify_video_category(transcript)\n          # Simple keyword-based classification\n          case transcript.downcase\n          when /tutorial|how to|step by step/\n            'tutorial'\n          when /education|learn|study|course/\n            'educational'\n          when /documentary|history|science/\n            'documentary'\n          when /presentation|meeting|conference/\n            'presentation'\n          else\n            'entertainment'\n          end\n        end\n\n        def self.assess_transcript_quality(transcript)\n          # Assess quality based on various factors\n          return 'none' if transcript.blank?\n\n          word_count = transcript.split.length\n          return 'low' if word_count &lt; 50\n\n          # Check for transcript markers (poor quality indicators)\n          poor_quality_markers = ['[inaudible]', '[unclear]', '???', '[music]']\n          marker_count = poor_quality_markers.sum { |marker| transcript.scan(marker).count }\n\n          marker_ratio = marker_count.to_f / word_count\n\n          case marker_ratio\n          when 0..0.01\n            'high'\n          when 0.01..0.05\n            'medium'\n          else\n            'low'\n          end\n        end\n\n        def self.detect_language(transcript)\n          # Simplified language detection\n          # In production, use proper language detection library\n          'en'  # Default to English\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"development/extending/#plugin-architecture","title":"Plugin Architecture","text":""},{"location":"development/extending/#plugin-development-framework","title":"Plugin Development Framework","text":"<pre><code># lib/ragdoll/core/plugin_system.rb\nmodule Ragdoll\n  module Core\n    class PluginSystem\n      @plugins = {}\n      @hooks = Hash.new { |h, k| h[k] = [] }\n\n      class &lt;&lt; self\n        attr_reader :plugins, :hooks\n\n        def register_plugin(name, plugin_class)\n          @plugins[name] = plugin_class\n          plugin_class.new.setup if plugin_class.respond_to?(:setup)\n        end\n\n        def register_hook(event, callback)\n          @hooks[event] &lt;&lt; callback\n        end\n\n        def execute_hooks(event, *args)\n          @hooks[event].each do |callback|\n            callback.call(*args)\n          end\n        end\n\n        def load_plugin(plugin_path)\n          require plugin_path\n        end\n\n        def active_plugins\n          @plugins.keys\n        end\n      end\n    end\n\n    # Base plugin class\n    class BasePlugin\n      def setup\n        # Override in subclasses\n      end\n\n      def teardown\n        # Override in subclasses\n      end\n\n      protected\n\n      def register_hook(event, &amp;block)\n        PluginSystem.register_hook(event, block)\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"development/extending/#example-plugin-implementation","title":"Example Plugin Implementation","text":"<pre><code># lib/ragdoll_plugins/content_analytics_plugin.rb\nclass ContentAnalyticsPlugin &lt; Ragdoll::Core::BasePlugin\n  def setup\n    puts \"Setting up Content Analytics Plugin\"\n\n    # Register hooks for document processing events\n    register_hook(:document_processed) do |document|\n      track_document_metrics(document)\n    end\n\n    register_hook(:search_performed) do |query, results|\n      track_search_metrics(query, results)\n    end\n  end\n\n  private\n\n  def track_document_metrics(document)\n    # Collect document processing metrics\n    metrics = {\n      document_id: document.id,\n      document_type: document.document_type,\n      content_length: document.content&amp;.length || 0,\n      embedding_count: document.all_embeddings.count,\n      processed_at: Time.current\n    }\n\n    # Send to analytics service\n    send_to_analytics('document_processed', metrics)\n  end\n\n  def track_search_metrics(query, results)\n    metrics = {\n      query: query,\n      result_count: results.length,\n      average_similarity: results.map { |r| r[:similarity] }.sum / results.length,\n      searched_at: Time.current\n    }\n\n    send_to_analytics('search_performed', metrics)\n  end\n\n  def send_to_analytics(event, data)\n    # Implementation depends on your analytics service\n    puts \"Analytics: #{event} - #{data}\"\n  end\nend\n\n# Register the plugin\nRagdoll::Core::PluginSystem.register_plugin(:content_analytics, ContentAnalyticsPlugin)\n</code></pre>"},{"location":"development/extending/#integration-best-practices","title":"Integration Best Practices","text":""},{"location":"development/extending/#extension-development-guidelines","title":"Extension Development Guidelines","text":"<ol> <li>Follow STI Patterns: Use Single Table Inheritance for content models</li> <li>Maintain Database Compatibility: Ensure PostgreSQL + pgvector compatibility</li> <li>Implement Proper Error Handling: Use Ragdoll's error classes</li> <li>Add Comprehensive Tests: Follow existing test patterns</li> <li>Document Extensions: Provide clear usage examples</li> </ol>"},{"location":"development/extending/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Database Indexes: Add appropriate indexes for new queries</li> <li>Embedding Dimensions: Ensure consistency across providers</li> <li>Memory Usage: Monitor memory usage with large files</li> <li>Batch Processing: Implement batch operations where possible</li> </ul>"},{"location":"development/extending/#testing-extensions","title":"Testing Extensions","text":"<pre><code># test/extensions/video_content_test.rb\nclass VideoContentTest &lt; Minitest::Test\n  def test_video_content_creation\n    document = create_test_document\n\n    video_content = document.video_contents.create!(\n      content: \"This is a test video transcript\",\n      embedding_model: \"test-model\",\n      metadata: {\n        duration: 120,\n        width: 1920,\n        height: 1080,\n        codec: \"h264\"\n      }\n    )\n\n    assert_equal \"This is a test video transcript\", video_content.transcript\n    assert_equal 120, video_content.duration\n    assert_equal \"1920x1080\", video_content.resolution\n  end\n\n  def test_video_embedding_generation\n    document = create_test_document\n    video_content = document.video_contents.create!(\n      content: \"Test transcript content\",\n      embedding_model: \"test-model\"\n    )\n\n    # Mock embedding service\n    mock_service = MockEmbeddingService.new\n    EmbeddingService.stub(:new, mock_service) do\n      video_content.generate_embeddings!\n    end\n\n    assert video_content.embeddings.any?\n    assert_equal \"Test transcript content\", video_content.embeddings.first.content\n  end\nend\n</code></pre>"},{"location":"development/extending/#extension-packaging","title":"Extension Packaging","text":"<pre><code># ragdoll_video_extension.gemspec\nGem::Specification.new do |spec|\n  spec.name          = \"ragdoll-video-extension\"\n  spec.version       = \"1.0.0\"\n  spec.authors       = [\"Your Name\"]\n  spec.email         = [\"your.email@example.com\"]\n\n  spec.summary       = \"Video content support for Ragdoll\"\n  spec.description   = \"Adds video processing and transcript extraction to Ragdoll\"\n  spec.homepage      = \"https://github.com/yourorg/ragdoll-video-extension\"\n\n  spec.files         = Dir[\"lib/**/*\", \"README.md\"]\n  spec.require_paths = [\"lib\"]\n\n  spec.add_dependency \"ragdoll\", \"~&gt; 1.0\"\n  spec.add_dependency \"streamio-ffmpeg\", \"~&gt; 3.0\"\n\n  spec.add_development_dependency \"minitest\", \"~&gt; 5.0\"\n  spec.add_development_dependency \"simplecov\", \"~&gt; 0.21\"\nend\n</code></pre> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"development/testing/","title":"Testing Guide","text":""},{"location":"development/testing/#running-tests-and-coverage-analysis","title":"Running Tests and Coverage Analysis","text":"<p>Ragdoll uses Minitest as its primary testing framework with comprehensive coverage analysis via SimpleCov. All tests require PostgreSQL with pgvector extension and focus on real database integration rather than mocking.</p>"},{"location":"development/testing/#quick-test-commands","title":"Quick Test Commands","text":"<pre><code># Run all tests\nbundle exec rake test\n\n# Run specific test file\nbundle exec rake test test/core/client_test.rb\n\n# Run with coverage report\nRAILS_ENV=test bundle exec rake test\nopen coverage/index.html\n\n# Run tests with verbose output\nbundle exec rake test TESTOPTS=\"-v\"\n\n# Run specific test method\nbundle exec rake test TESTOPTS=\"--name test_method_name\"\n</code></pre>"},{"location":"development/testing/#test-framework","title":"Test Framework","text":""},{"location":"development/testing/#minitest-configuration","title":"Minitest Configuration","text":"<p>Ragdoll uses Minitest with custom reporting and database management:</p> <pre><code># test/test_helper.rb highlights\nrequire \"simplecov\"\nrequire \"minitest/autorun\"\nrequire \"minitest/reporters\"\n\n# Custom reporter for better test output\nclass CompactTestReporter &lt; Minitest::Reporters::BaseReporter\n  def record(result)\n    status = case result.result_code\n             when \".\" then \"\\e[32mPASS\\e[0m\"\n             when \"F\" then \"\\e[31mFAIL\\e[0m\"\n             when \"E\" then \"\\e[31mERROR\\e[0m\"\n             when \"S\" then \"\\e[33mSKIP\\e[0m\"\n             end\n\n    time_str = result.time &gt;= 1.0 ? \n      \"\\e[31m(#{result.time.round(2)}s)\\e[0m\" : \n      \"(#{result.time.round(3)}s)\"\n\n    puts \"#{result.klass}##{result.name} ... #{status} #{time_str}\"\n  end\nend\n</code></pre>"},{"location":"development/testing/#test-database-configuration","title":"Test Database Configuration","text":"<p>Each test gets a clean PostgreSQL database state:</p> <pre><code>module Minitest\n  class Test\n    def setup\n      Ragdoll::Core.reset_configuration!\n\n      # Setup PostgreSQL test database\n      Ragdoll::Core::Database.setup({\n        adapter: \"postgresql\",\n        database: \"ragdoll_test\",\n        username: ENV[\"POSTGRES_USER\"] || \"postgres\",\n        password: ENV[\"POSTGRES_PASSWORD\"] || \"\",\n        host: ENV[\"POSTGRES_HOST\"] || \"localhost\",\n        port: ENV[\"POSTGRES_PORT\"] || 5432,\n        auto_migrate: true,\n        logger: nil\n      })\n    end\n\n    def teardown\n      # Clean database in foreign key order\n      %w[ragdoll_embeddings ragdoll_contents ragdoll_documents].each do |table|\n        ActiveRecord::Base.connection.execute(\"DELETE FROM #{table}\")\n      end\n\n      Ragdoll::Core.reset_configuration!\n    end\n  end\nend\n</code></pre>"},{"location":"development/testing/#custom-test-helpers","title":"Custom Test Helpers","text":"<pre><code># Custom assertions for Ragdoll-specific testing\nmodule RagdollTestHelpers\n  def assert_embedding_generated(text)\n    service = Ragdoll::EmbeddingService.new\n    embedding = service.generate_embedding(text)\n    assert embedding.is_a?(Array), \"Expected array, got #{embedding.class}\"\n    assert embedding.length &gt; 0, \"Expected non-empty embedding\"\n    assert embedding.all?(Numeric), \"All embedding values must be numeric\"\n  end\n\n  def assert_document_processed(document_id)\n    document = Ragdoll::Document.find(document_id)\n    assert_equal 'processed', document.status\n    assert document.content.present?\n    refute_empty document.all_embeddings\n  end\n\n  def assert_search_results_valid(results)\n    assert_instance_of Array, results\n    results.each do |result|\n      assert result.key?(:similarity)\n      assert result.key?(:content)\n      assert result.key?(:document_id)\n      assert result[:similarity].between?(0.0, 1.0)\n    end\n  end\n\n  def create_test_document(content: \"Test content\", title: \"Test Document\")\n    doc_id = Ragdoll::DocumentManagement.add_document(\n      \"test://#{title}\", content, { title: title }\n    )\n\n    # Process immediately for tests (no background jobs)\n    document = Ragdoll::Document.find(doc_id)\n    document.update!(status: 'processed')\n    doc_id\n  end\nend\n\n# Include helpers in all tests\nMinitest::Test.include(RagdollTestHelpers)\n</code></pre>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":""},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<p>Unit tests focus on individual classes and methods:</p> <pre><code># test/core/embedding_service_test.rb\nclass EmbeddingServiceTest &lt; Minitest::Test\n  def setup\n    super\n    @service = Ragdoll::EmbeddingService.new\n  end\n\n  def test_generates_embedding_for_text\n    # Mock LLM API to avoid external dependencies\n    mock_client = Minitest::Mock.new\n    mock_client.expect(:embed, \n      { \"embeddings\" =&gt; [Array.new(1536) { rand }] },\n      [Hash]\n    )\n\n    service = Ragdoll::EmbeddingService.new(client: mock_client)\n    embedding = service.generate_embedding(\"test text\")\n\n    assert_instance_of Array, embedding\n    assert_equal 1536, embedding.length\n    assert embedding.all?(Numeric)\n\n    mock_client.verify\n  end\n\n  def test_handles_empty_text\n    embedding = @service.generate_embedding(\"\")\n    assert_nil embedding\n  end\n\n  def test_handles_very_long_text\n    long_text = \"word \" * 10_000\n    embedding = @service.generate_embedding(long_text)\n\n    # Should truncate and still generate embedding\n    assert_instance_of Array, embedding\n  end\nend\n</code></pre> <pre><code># test/core/models/document_test.rb\nclass DocumentTest &lt; Minitest::Test\n  def test_creates_document_with_required_fields\n    document = Ragdoll::Document.create!(\n      location: \"/test/path.txt\",\n      title: \"Test Document\",\n      document_type: \"text\",\n      status: \"pending\",\n      file_modified_at: Time.current\n    )\n\n    assert document.persisted?\n    assert_equal \"text\", document.document_type\n    assert_equal \"pending\", document.status\n  end\n\n  def test_validates_required_fields\n    document = Ragdoll::Document.new\n\n    refute document.valid?\n    assert document.errors[:location].present?\n    assert document.errors[:title].present?\n  end\n\n  def test_normalizes_file_paths\n    document = Ragdoll::Document.create!(\n      location: \"relative/path.txt\",\n      title: \"Test\",\n      document_type: \"text\",\n      status: \"pending\",\n      file_modified_at: Time.current\n    )\n\n    # Should convert to absolute path\n    assert document.location.start_with?(\"/\")\n    assert document.location.include?(\"relative/path.txt\")\n  end\nend\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<p>Integration tests verify component interactions:</p> <pre><code># test/core/client_integration_test.rb\nclass ClientIntegrationTest &lt; Minitest::Test\n  def setup\n    super\n    @client = Ragdoll::Core.client\n  end\n\n  def test_complete_document_workflow\n    # Create temporary test file\n    Tempfile.create(['test', '.txt']) do |file|\n      file.write(\"This is test content about machine learning algorithms.\")\n      file.rewind\n\n      # Add document\n      result = @client.add_document(path: file.path)\n      assert result[:success]\n\n      document_id = result[:document_id]\n      assert document_id.present?\n\n      # Verify document was created\n      document = Ragdoll::Document.find(document_id)\n      assert_equal \"text\", document.document_type\n\n      # Simulate background job processing\n      document.generate_embeddings_for_all_content!\n\n      # Verify embeddings were created\n      assert document.all_embeddings.any?\n\n      # Test search functionality\n      search_results = @client.search(query: \"machine learning\")\n      assert search_results[:results].any?\n\n      # Verify search result structure\n      result = search_results[:results].first\n      assert result[:document_id] == document_id\n      assert result[:similarity] &gt; 0.5\n    end\n  end\n\n  def test_document_management_operations\n    # Add document\n    doc_id = create_test_document(\n      content: \"Integration test content\",\n      title: \"Integration Test Doc\"\n    )\n\n    # Test retrieval\n    document = @client.get_document(id: doc_id)\n    assert_equal \"Integration Test Doc\", document[:title]\n    assert_equal \"Integration test content\", document[:content]\n\n    # Test update\n    updated = @client.update_document(\n      id: doc_id,\n      metadata: { category: \"test\" }\n    )\n    assert_equal \"test\", updated[:metadata][\"category\"]\n\n    # Test deletion\n    assert @client.delete_document(id: doc_id)\n    assert_nil @client.get_document(id: doc_id)\n  end\nend\n</code></pre>"},{"location":"development/testing/#system-tests","title":"System Tests","text":"<p>System tests verify end-to-end workflows:</p> <pre><code># test/system/rag_workflow_test.rb\nclass RAGWorkflowTest &lt; Minitest::Test\n  def test_complete_rag_system\n    client = Ragdoll::Core.client\n\n    # Add multiple documents\n    documents = [\n      { content: \"Ruby is a programming language\", title: \"Ruby Intro\" },\n      { content: \"Python is used for data science\", title: \"Python Guide\" },\n      { content: \"JavaScript runs in browsers\", title: \"JS Basics\" }\n    ]\n\n    doc_ids = documents.map do |doc|\n      create_test_document(content: doc[:content], title: doc[:title])\n    end\n\n    # Process all documents\n    doc_ids.each do |doc_id|\n      document = Ragdoll::Document.find(doc_id)\n      document.generate_embeddings_for_all_content!\n    end\n\n    # Test semantic search\n    results = client.search(query: \"programming languages\")\n    assert results[:results].length &gt;= 2\n\n    # Results should be ordered by relevance\n    first_result = results[:results].first\n    assert first_result[:similarity] &gt; 0.3\n\n    # Test context enhancement\n    enhanced = client.enhance_prompt(\n      prompt: \"What programming languages are mentioned?\",\n      context_limit: 3\n    )\n\n    assert enhanced[:context_count] &gt; 0\n    assert enhanced[:enhanced_prompt].include?(\"programming\")\n    assert enhanced[:context_sources].any?\n  end\nend\n</code></pre>"},{"location":"development/testing/#test-execution","title":"Test Execution","text":""},{"location":"development/testing/#running-specific-tests","title":"Running Specific Tests","text":"<pre><code># Run all tests in a directory\nbundle exec rake test test/core/\n\n# Run specific test class\nbundle exec rake test test/core/client_test.rb\n\n# Run specific test method\nbundle exec rake test TESTOPTS=\"--name test_specific_method\"\n\n# Run tests matching pattern\nbundle exec rake test TESTOPTS=\"--name /embedding/\"\n\n# Run with seed for reproducible randomization\nbundle exec rake test TESTOPTS=\"--seed 12345\"\n</code></pre>"},{"location":"development/testing/#test-configuration","title":"Test Configuration","text":""},{"location":"development/testing/#environment-variables","title":"Environment Variables","text":"<pre><code># Database configuration\nexport POSTGRES_USER=ragdoll_test\nexport POSTGRES_PASSWORD=test_password\nexport POSTGRES_HOST=localhost\nexport POSTGRES_PORT=5432\n\n# Test-specific settings\nexport RAGDOLL_LOG_LEVEL=error\nexport COVERAGE_UNDERCOVER=true\n\n# LLM API keys for integration tests\nexport OPENAI_API_KEY=test_key\nexport ANTHROPIC_API_KEY=test_key\n</code></pre>"},{"location":"development/testing/#mock-service-setup","title":"Mock Service Setup","text":"<pre><code># test/support/mock_services.rb\nclass MockLLMClient\n  def embed(input:, model:)\n    # Return consistent mock embeddings\n    embeddings = Array.new(input.is_a?(Array) ? input.length : 1) do\n      Array.new(1536) { rand(-1.0..1.0) }\n    end\n\n    { \"embeddings\" =&gt; embeddings }\n  end\n\n  def chat(model:, messages:, **options)\n    # Return mock chat completion\n    {\n      \"choices\" =&gt; [{\n        \"message\" =&gt; {\n          \"content\" =&gt; \"This is a mock response to: #{messages.last[:content][0..50]}...\"\n        }\n      }]\n    }\n  end\nend\n\n# Use in tests\nclass ServiceTest &lt; Minitest::Test\n  def setup\n    super\n    @mock_client = MockLLMClient.new\n    @service = Ragdoll::EmbeddingService.new(client: @mock_client)\n  end\nend\n</code></pre>"},{"location":"development/testing/#coverage-analysis","title":"Coverage Analysis","text":""},{"location":"development/testing/#simplecov-configuration","title":"SimpleCov Configuration","text":"<pre><code># test/test_helper.rb - SimpleCov setup\nSimpleCov.start do\n  add_filter \"/test/\"\n  track_files \"lib/**/*.rb\"\n\n  # Coverage groups for better reporting\n  add_group \"Core\", \"lib/ragdoll/core\"\n  add_group \"Models\", \"lib/ragdoll/core/models\"\n  add_group \"Services\", \"lib/ragdoll/core/services\"\n  add_group \"Jobs\", \"lib/ragdoll/core/jobs\"\n\n  # Coverage thresholds\n  minimum_coverage 85\n  minimum_coverage_by_file 70\n\n  # Exclude version file and generated files\n  add_filter \"version.rb\"\n  add_filter \"migrate/\"\nend\n</code></pre>"},{"location":"development/testing/#coverage-metrics-and-reporting","title":"Coverage Metrics and Reporting","text":"<pre><code># Generate coverage report\nbundle exec rake test\n\n# View HTML coverage report\nopen coverage/index.html\n\n# Check coverage percentage\ngrep -A 5 \"covered at\" coverage/index.html\n\n# Coverage by file type\ngrep \"LOC:\" coverage/.resultset.json\n</code></pre>"},{"location":"development/testing/#coverage-analysis-tools","title":"Coverage Analysis Tools","text":"<pre><code># Custom coverage analysis\nclass CoverageAnalyzer\n  def self.analyze_uncovered_lines\n    if defined?(SimpleCov)\n      SimpleCov.result.files.each do |file|\n        uncovered = file.missed_lines\n        if uncovered.any?\n          puts \"#{file.filename}: #{uncovered.length} uncovered lines\"\n          uncovered.first(5).each do |line_num|\n            puts \"  Line #{line_num}: #{file.src[line_num - 1].strip}\"\n          end\n        end\n      end\n    end\n  end\nend\n\n# Run after tests\nCoverageAnalyzer.analyze_uncovered_lines\n</code></pre>"},{"location":"development/testing/#mocking-and-stubbing","title":"Mocking and Stubbing","text":""},{"location":"development/testing/#external-service-mocking","title":"External Service Mocking","text":""},{"location":"development/testing/#llm-provider-mocking","title":"LLM Provider Mocking","text":"<pre><code># Mock OpenAI API responses\nclass MockOpenAIService\n  def initialize(responses = {})\n    @responses = responses\n    @call_count = Hash.new(0)\n  end\n\n  def embed(input:, model:)\n    @call_count[:embed] += 1\n\n    if @responses[:embed_error]\n      raise StandardError, @responses[:embed_error]\n    end\n\n    # Return mock embedding\n    {\n      \"embeddings\" =&gt; [Array.new(1536) { rand(-1.0..1.0) }]\n    }\n  end\n\n  def call_count(method)\n    @call_count[method]\n  end\nend\n\n# Use in tests\ndef test_handles_api_failures\n  mock_service = MockOpenAIService.new(embed_error: \"Rate limit exceeded\")\n  service = Ragdoll::EmbeddingService.new(client: mock_service)\n\n  assert_raises(Ragdoll::Core::EmbeddingError) do\n    service.generate_embedding(\"test text\")\n  end\n\n  assert_equal 1, mock_service.call_count(:embed)\nend\n</code></pre>"},{"location":"development/testing/#database-mocking-advanced","title":"Database Mocking (Advanced)","text":"<pre><code># Mock specific database operations\nclass DatabaseMockTest &lt; Minitest::Test\n  def test_handles_database_connection_failure\n    # Temporarily break database connection\n    original_connection = ActiveRecord::Base.connection\n\n    ActiveRecord::Base.stub(:connection, nil) do\n      client = Ragdoll::Core.client\n\n      assert_raises(ActiveRecord::ConnectionNotEstablished) do\n        client.add_document(path: \"test.txt\")\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"development/testing/#test-doubles-and-stubs","title":"Test Doubles and Stubs","text":"<pre><code># Minitest stub examples\ndef test_file_processing_with_stub\n  # Stub File.read to return controlled content\n  File.stub(:read, \"mocked file content\") do\n    result = Ragdoll::DocumentProcessor.parse(\"any_path.txt\")\n    assert_equal \"mocked file content\", result[:content]\n  end\nend\n\n# Class method stubbing\ndef test_with_time_stub\n  fixed_time = Time.parse(\"2024-01-01 00:00:00 UTC\")\n\n  Time.stub(:current, fixed_time) do\n    document = create_test_document\n    doc = Ragdoll::Document.find(document)\n    assert_equal fixed_time.to_i, doc.created_at.to_i\n  end\nend\n</code></pre>"},{"location":"development/testing/#performance-testing","title":"Performance Testing","text":""},{"location":"development/testing/#benchmark-tests","title":"Benchmark Tests","text":"<pre><code># test/performance/embedding_benchmark_test.rb\nrequire 'benchmark'\n\nclass EmbeddingBenchmarkTest &lt; Minitest::Test\n  def test_embedding_generation_performance\n    service = Ragdoll::EmbeddingService.new\n    texts = Array.new(100) { \"Sample text #{rand(1000)}\" }\n\n    time = Benchmark.measure do\n      service.generate_embeddings_batch(texts)\n    end\n\n    # Should process 100 embeddings in under 30 seconds\n    assert time.real &lt; 30, \"Batch embedding too slow: #{time.real}s\"\n\n    puts \"Processed #{texts.length} embeddings in #{time.real.round(2)}s\"\n    puts \"Average: #{(time.real / texts.length * 1000).round(2)}ms per embedding\"\n  end\n\n  def test_search_performance\n    # Create test dataset\n    10.times do |i|\n      create_test_document(\n        content: \"Test document #{i} with unique content about topic #{i}\",\n        title: \"Doc #{i}\"\n      )\n    end\n\n    client = Ragdoll::Core.client\n\n    # Benchmark search performance\n    time = Benchmark.measure do\n      100.times do\n        client.search(query: \"test content\")\n      end\n    end\n\n    avg_time = time.real / 100\n    assert avg_time &lt; 0.5, \"Search too slow: #{avg_time}s per query\"\n\n    puts \"Average search time: #{(avg_time * 1000).round(2)}ms\"\n  end\nend\n</code></pre>"},{"location":"development/testing/#memory-usage-testing","title":"Memory Usage Testing","text":"<pre><code># test/performance/memory_test.rb\nclass MemoryTest &lt; Minitest::Test\n  def test_memory_usage_during_processing\n    start_memory = get_memory_usage\n\n    # Process multiple large documents\n    10.times do |i|\n      large_content = \"Large content \" * 10_000\n      create_test_document(\n        content: large_content,\n        title: \"Large Doc #{i}\"\n      )\n    end\n\n    end_memory = get_memory_usage\n    memory_increase = end_memory - start_memory\n\n    # Should not use more than 500MB\n    assert memory_increase &lt; 500, \"Memory usage too high: #{memory_increase}MB\"\n\n    puts \"Memory usage increased by #{memory_increase}MB\"\n  end\n\n  private\n\n  def get_memory_usage\n    `ps -o rss= -p #{Process.pid}`.to_i / 1024.0  # Convert to MB\n  end\nend\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions-configuration","title":"GitHub Actions Configuration","text":"<pre><code># .github/workflows/test.yml\nname: Test Suite\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    services:\n      postgres:\n        image: pgvector/pgvector:pg14\n        env:\n          POSTGRES_USER: postgres\n          POSTGRES_PASSWORD: postgres\n          POSTGRES_DB: ragdoll_test\n        options: &gt;\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n        ports:\n          - 5432:5432\n\n    strategy:\n      matrix:\n        ruby-version: ['3.0', '3.1', '3.2']\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Ruby\n      uses: ruby/setup-ruby@v1\n      with:\n        ruby-version: ${{ matrix.ruby-version }}\n        bundler-cache: true\n\n    - name: Set up database\n      env:\n        POSTGRES_USER: postgres\n        POSTGRES_PASSWORD: postgres\n        POSTGRES_HOST: localhost\n        POSTGRES_PORT: 5432\n      run: |\n        sudo apt-get update\n        sudo apt-get install -y postgresql-client\n        createdb -h localhost -U postgres ragdoll_test\n        psql -h localhost -U postgres -d ragdoll_test -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\n\n    - name: Run tests\n      env:\n        POSTGRES_USER: postgres\n        POSTGRES_PASSWORD: postgres\n        POSTGRES_HOST: localhost\n        POSTGRES_PORT: 5432\n      run: bundle exec rake test\n\n    - name: Upload coverage reports\n      uses: codecov/codecov-action@v3\n      with:\n        files: ./coverage/.resultset.json\n        flags: unittests\n        name: codecov-umbrella\n</code></pre>"},{"location":"development/testing/#ci-specific-test-configuration","title":"CI-Specific Test Configuration","text":"<pre><code># test/support/ci_configuration.rb\nmodule CIConfiguration\n  def self.setup\n    if ENV['CI']\n      # CI-specific settings\n      Ragdoll::Core.configure do |config|\n        config.logging_config[:level] = :error\n        config.embedding_config[:cache_embeddings] = false\n      end\n\n      # Use faster, less accurate models for CI\n      ENV['OPENAI_API_KEY'] = 'test_key_for_ci'\n    end\n  end\nend\n\n# Include in test_helper.rb\nCIConfiguration.setup if ENV['CI']\n</code></pre>"},{"location":"development/testing/#testing-best-practices","title":"Testing Best Practices","text":""},{"location":"development/testing/#test-organization","title":"Test Organization","text":"<pre><code>test/\n\u251c\u2500\u2500 test_helper.rb         # Global test setup\n\u251c\u2500\u2500 support/               # Test utilities\n\u2502   \u251c\u2500\u2500 mock_services.rb\n\u2502   \u2517\u2500\u2500 test_helpers.rb\n\u251c\u2500\u2500 core/                  # Unit tests\n\u2502   \u251c\u2500\u2500 client_test.rb\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2514\u2500\u2500 services/\n\u251c\u2500\u2500 integration/           # Integration tests\n\u2502   \u2517\u2500\u2500 workflow_test.rb\n\u251c\u2500\u2500 performance/           # Performance tests\n\u2502   \u2517\u2500\u2500 benchmark_test.rb\n\u251c\u2500\u2500 system/                # End-to-end tests\n\u2502   \u2517\u2500\u2500 rag_system_test.rb\n\u2514\u2500\u2500 fixtures/              # Test data\n    \u251c\u2500\u2500 sample.pdf\n    \u251c\u2500\u2500 test_image.png\n    \u2514\u2500\u2500 documents/\n</code></pre>"},{"location":"development/testing/#test-naming-conventions","title":"Test Naming Conventions","text":"<pre><code># Test class naming: [ClassName]Test\nclass DocumentProcessorTest &lt; Minitest::Test\n  # Test method naming: test_[action]_[condition]_[expected_result]\n  def test_parse_pdf_with_valid_file_returns_content\n    # Test implementation\n  end\n\n  def test_parse_pdf_with_corrupted_file_raises_error\n    # Test implementation\n  end\n\n  def test_parse_pdf_with_empty_file_returns_empty_content\n    # Test implementation\n  end\nend\n</code></pre>"},{"location":"development/testing/#quality-assurance","title":"Quality Assurance","text":""},{"location":"development/testing/#test-coverage-goals","title":"Test Coverage Goals","text":"<ul> <li>Overall Coverage: \u2265 85%</li> <li>Critical Paths: \u2265 95% (search, embedding, document processing)</li> <li>New Code: 100% (enforced in CI)</li> <li>Integration Points: \u2265 90%</li> </ul>"},{"location":"development/testing/#code-quality-in-tests","title":"Code Quality in Tests","text":"<pre><code># Good test structure\nclass WellStructuredTest &lt; Minitest::Test\n  def test_descriptive_name_following_convention\n    # Arrange - Set up test data\n    document = create_test_document(content: \"test content\")\n    client = Ragdoll::Core.client\n\n    # Act - Perform the action being tested\n    result = client.search(query: \"test\")\n\n    # Assert - Verify the expected outcome\n    assert result[:results].any?\n    assert_search_results_valid(result[:results])\n\n    # Cleanup (if needed beyond teardown)\n    # Usually handled by teardown method\n  end\nend\n</code></pre>"},{"location":"development/testing/#test-debugging","title":"Test Debugging","text":"<pre><code># Debug failing tests\nclass DebugTest &lt; Minitest::Test\n  def test_with_debugging\n    # Use pry for interactive debugging\n    require 'pry'; binding.pry if ENV['DEBUG']\n\n    # Add detailed logging\n    puts \"Starting test with data: #{@test_data.inspect}\" if ENV['VERBOSE']\n\n    # Your test code here\n  end\nend\n</code></pre> <pre><code># Run tests with debugging\nDEBUG=true VERBOSE=true bundle exec rake test TESTOPTS=\"--name test_specific_method\"\n</code></pre>"},{"location":"development/testing/#test-checklist","title":"Test Checklist","text":""},{"location":"development/testing/#before-committing","title":"Before Committing","text":"<ul> <li> All tests pass locally</li> <li> Coverage is \u2265 85%</li> <li> New code has 100% test coverage</li> <li> Tests follow naming conventions</li> <li> No hardcoded values or paths</li> <li> Tests are isolated and repeatable</li> <li> Performance tests run within limits</li> <li> Integration tests cover happy path and error cases</li> <li> Mock services are used appropriately</li> <li> Database is properly cleaned between tests</li> </ul>"},{"location":"development/testing/#for-new-features","title":"For New Features","text":"<ul> <li> Unit tests for all new classes/methods</li> <li> Integration tests for feature workflows</li> <li> Error handling tests for edge cases</li> <li> Performance tests for critical paths</li> <li> Documentation tests for examples</li> <li> Backward compatibility tests</li> </ul> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"getting-started/configuration/","title":"Configuration Guide","text":"<p>Ragdoll features a comprehensive configuration system that supports enterprise-grade deployments with fine-grained control over all aspects of the system. The configuration supports multiple LLM providers, database adapters, processing parameters, and operational settings.</p>"},{"location":"getting-started/configuration/#overview","title":"Overview","text":"<p>The configuration system provides 25+ configurable options organized into logical groups:</p> <ul> <li>LLM Provider Configuration: Multiple provider support with API key management</li> <li>Database Configuration: PostgreSQL with pgvector extension (REQUIRED)</li> <li>Processing Parameters: Chunking, embedding, and content analysis settings</li> <li>Search Configuration: Similarity thresholds, ranking weights, and result limits</li> <li>Operational Settings: Logging, monitoring, background processing, and performance tuning</li> <li>Feature Toggles: Enable/disable optional features and advanced capabilities</li> </ul>"},{"location":"getting-started/configuration/#basic-configuration","title":"Basic Configuration","text":""},{"location":"getting-started/configuration/#simple-setup","title":"Simple Setup","text":"<pre><code>require 'ragdoll-core'\n\nRagdoll::Core.configure do |config|\n  # Configure LLM providers (new structure)\n  config.llm_providers[:openai][:api_key] = ENV['OPENAI_API_KEY']\n\n  # Configure models (new structure)\n  config.models[:embedding][:text] = 'openai/text-embedding-3-small'\n  config.models[:text_generation][:default] = 'openai/gpt-4o-mini'\n\n  # Database (PostgreSQL REQUIRED)\n  config.database = {\n    adapter: 'postgresql',\n    database: 'ragdoll_development',\n    username: 'ragdoll',\n    password: ENV['RAGDOLL_DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true\n  }\nend\n</code></pre>"},{"location":"getting-started/configuration/#production-setup","title":"Production Setup","text":"<pre><code>Ragdoll::Core.configure do |config|\n  # Configure LLM providers (new structure)\n  config.llm_providers[:openai][:api_key] = ENV['OPENAI_API_KEY']\n  config.llm_providers[:openai][:organization] = ENV['OPENAI_ORGANIZATION']\n  config.llm_providers[:openai][:project] = ENV['OPENAI_PROJECT']\n\n  # Configure models (new structure)\n  config.models[:embedding][:text] = 'openai/text-embedding-3-small'\n  config.models[:text_generation][:default] = 'openai/gpt-4o-mini'\n  config.models[:text_generation][:summary] = 'openai/gpt-4o'\n  config.models[:text_generation][:keywords] = 'openai/gpt-4o-mini'\n\n  # Production PostgreSQL with pgvector\n  config.database = {\n    adapter: 'postgresql',\n    database: ENV['DATABASE_NAME'] || 'ragdoll_production',\n    username: ENV['DATABASE_USERNAME'] || 'ragdoll',\n    password: ENV['RAGDOLL_DATABASE_PASSWORD'],\n    host: ENV['DATABASE_HOST'] || 'localhost',\n    port: ENV['DATABASE_PORT'] || 5432,\n    auto_migrate: false  # Handle migrations separately in production\n  }\n\n  # Production logging\n  config.logging[:level] = :info\n  config.logging[:filepath] = '/var/log/ragdoll/ragdoll.log'\n\n  # Performance settings\n  config.processing[:text][:chunking][:max_tokens] = 1000\n  config.processing[:text][:chunking][:overlap] = 200\n  config.processing[:search][:similarity_threshold] = 0.75\n  config.processing[:search][:max_results] = 50\n\n  # Enable summarization\n  config.summarization[:enable] = true\n  config.summarization[:max_length] = 300\nend\n</code></pre>"},{"location":"getting-started/configuration/#llm-provider-configuration","title":"LLM Provider Configuration","text":""},{"location":"getting-started/configuration/#supported-providers","title":"Supported Providers","text":"<p>Ragdoll supports 7 LLM providers through the ruby_llm integration:</p> <pre><code># OpenAI (recommended for production)\nconfig.llm_providers[:openai][:api_key] = ENV['OPENAI_API_KEY']\nconfig.llm_providers[:openai][:organization] = ENV['OPENAI_ORGANIZATION']\nconfig.llm_providers[:openai][:project] = ENV['OPENAI_PROJECT']\nconfig.models[:embedding][:text] = 'openai/text-embedding-3-small'  # or text-embedding-3-large\nconfig.models[:text_generation][:default] = 'openai/gpt-4o'\nconfig.models[:text_generation][:summary] = 'openai/gpt-4o'\nconfig.models[:text_generation][:keywords] = 'openai/gpt-4o-mini'\n\n# Anthropic Claude\nconfig.llm_providers[:anthropic][:api_key] = ENV['ANTHROPIC_API_KEY']\nconfig.llm_providers[:openai][:api_key] = ENV['OPENAI_API_KEY']  # Still needed for embeddings\nconfig.models[:embedding][:text] = 'openai/text-embedding-3-small'  # OpenAI embeddings\nconfig.models[:text_generation][:default] = 'anthropic/claude-3-sonnet-20240229'\nconfig.models[:text_generation][:summary] = 'anthropic/claude-3-sonnet-20240229'\nconfig.models[:text_generation][:keywords] = 'anthropic/claude-3-haiku-20240307'\n\n# Google Gemini\nconfig.llm_providers[:google][:api_key] = ENV['GOOGLE_API_KEY']\nconfig.llm_providers[:google][:project_id] = ENV['GOOGLE_PROJECT_ID']\nconfig.models[:text_generation][:default] = 'google/gemini-1.5-pro'\nconfig.models[:text_generation][:summary] = 'google/gemini-1.5-pro'\nconfig.models[:text_generation][:keywords] = 'google/gemini-1.5-flash'\n\n# Azure OpenAI\nconfig.llm_providers[:azure][:api_key] = ENV['AZURE_OPENAI_API_KEY']\nconfig.llm_providers[:azure][:endpoint] = ENV['AZURE_OPENAI_ENDPOINT']\nconfig.llm_providers[:azure][:api_version] = ENV['AZURE_OPENAI_API_VERSION']\nconfig.models[:embedding][:text] = 'text-embedding-ada-002'\nconfig.models[:text_generation][:default] = 'azure/gpt-4'\n\n# Ollama (local deployment)\nconfig.llm_providers[:ollama][:endpoint] = ENV['OLLAMA_ENDPOINT'] || 'http://localhost:11434'\nconfig.models[:text_generation][:default] = 'ollama/llama3:8b'\nconfig.models[:text_generation][:summary] = 'ollama/llama3:8b'\nconfig.models[:text_generation][:keywords] = 'ollama/llama3:8b'\nconfig.models[:embedding][:text] = 'nomic-embed-text'\n\n# HuggingFace\nconfig.llm_providers[:huggingface][:api_key] = ENV['HUGGINGFACE_API_KEY']\nconfig.models[:text_generation][:default] = 'huggingface/microsoft/DialoGPT-medium'\nconfig.models[:embedding][:text] = 'sentence-transformers/all-MiniLM-L6-v2'\n\n# OpenRouter (access to multiple models)\nconfig.llm_providers[:openrouter][:api_key] = ENV['OPENROUTER_API_KEY']\nconfig.models[:text_generation][:default] = 'openrouter/anthropic/claude-3-sonnet'\nconfig.models[:text_generation][:summary] = 'openrouter/anthropic/claude-3-sonnet'\nconfig.models[:text_generation][:keywords] = 'openrouter/openai/gpt-3.5-turbo'\n</code></pre>"},{"location":"getting-started/configuration/#multi-provider-configuration","title":"Multi-Provider Configuration","text":"<pre><code># Use different providers for different tasks\nRagdoll::Core.configure do |config|\n  # Configure multiple LLM providers\n  config.llm_providers[:openai][:api_key] = ENV['OPENAI_API_KEY']\n  config.llm_providers[:anthropic][:api_key] = ENV['ANTHROPIC_API_KEY']\n  config.llm_providers[:ollama][:endpoint] = 'http://localhost:11434'\n\n  # Configure models for different tasks\n  config.models[:embedding][:text] = 'openai/text-embedding-3-small'  # OpenAI embeddings\n  config.models[:text_generation][:default] = 'openai/gpt-4o-mini'    # OpenAI for general tasks\n  config.models[:text_generation][:summary] = 'anthropic/claude-3-sonnet-20240229'  # Claude for summarization\n  config.models[:text_generation][:keywords] = 'ollama/llama3:8b'     # Local Ollama for keywords\nend\n</code></pre>"},{"location":"getting-started/configuration/#database-configuration","title":"Database Configuration","text":"<p>Important Note: Ragdoll ONLY supports PostgreSQL. The system requires the pgvector extension for vector similarity search and uses PostgreSQL-specific features. SQLite is not supported.</p>"},{"location":"getting-started/configuration/#postgresql-with-pgvector-recommended-for-production","title":"PostgreSQL with pgvector (Recommended for Production)","text":"<pre><code>config.database_config = {\n  adapter: 'postgresql',\n  database: 'ragdoll_production',\n  username: 'ragdoll_user',\n  password: ENV['DATABASE_PASSWORD'],\n  host: 'postgres.example.com',\n  port: 5432,\n  pool: 25,                    # Connection pool size\n  timeout: 5000,               # Connection timeout (ms)\n  auto_migrate: false,         # Handle migrations separately\n  sslmode: 'require',          # SSL configuration\n\n  # pgvector specific settings\n  extensions: ['vector'],\n  search_path: ['public', 'vector'],\n\n  # Performance tuning\n  prepared_statements: true,\n  advisory_locks: true,\n  variables: {\n    'shared_preload_libraries' =&gt; 'vector',\n    'max_connections' =&gt; '200',\n    'shared_buffers' =&gt; '256MB',\n    'effective_cache_size' =&gt; '1GB'\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#important-postgresql-only","title":"Important: PostgreSQL Only","text":"<p>Ragdoll exclusively supports PostgreSQL with the pgvector extension. This is required for:</p> <ul> <li>Vector similarity search operations  </li> <li>Advanced PostgreSQL features like GIN indexes for full-text search</li> <li>JSON field operations and queries</li> <li>Optimized performance for large document collections</li> </ul> <p>No other database adapters are supported.</p>"},{"location":"getting-started/configuration/#database-environment-configuration","title":"Database Environment Configuration","text":"<pre><code># Environment-specific database configuration (PostgreSQL for all environments)\ncase Rails.env\nwhen 'development'\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_development',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true\n  }\nwhen 'test'\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_test',\n    username: 'ragdoll',  \n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true\n  }\nwhen 'production'\n  config.database_config = {\n    adapter: 'postgresql',\n    url: ENV['DATABASE_URL'],  # Use connection URL\n    pool: ENV.fetch('RAILS_MAX_THREADS', 25).to_i,\n    auto_migrate: false\n  }\nend\n</code></pre>"},{"location":"getting-started/configuration/#processing-configuration","title":"Processing Configuration","text":""},{"location":"getting-started/configuration/#text-processing","title":"Text Processing","text":"<p>Schema Note: The <code>embedding_model</code> is now stored per content type (text, image, audio) rather than duplicated in individual embeddings. This provides better data normalization while maintaining functionality through polymorphic relationships.</p> <pre><code>config.chunk_size = 1000              # Characters per text chunk\nconfig.chunk_overlap = 200            # Overlap between chunks\nconfig.max_chunk_size = 2000          # Maximum chunk size limit\nconfig.min_chunk_size = 100           # Minimum chunk size limit\n\n# Language and encoding\nconfig.default_language = 'en'       # Default language for processing\nconfig.auto_detect_language = true   # Enable language detection\nconfig.encoding_fallbacks = ['UTF-8', 'ISO-8859-1', 'Windows-1252']\n\n# Text cleaning\nconfig.remove_extra_whitespace = true\nconfig.normalize_unicode = true\nconfig.strip_html_tags = true\nconfig.preserve_code_blocks = true\n</code></pre>"},{"location":"getting-started/configuration/#document-processing","title":"Document Processing","text":"<pre><code># PDF processing\nconfig.pdf_max_pages = 1000           # Maximum pages to process\nconfig.pdf_extract_images = true     # Extract images from PDFs\nconfig.pdf_extract_tables = true     # Extract table content\nconfig.pdf_ocr_fallback = true       # Use OCR for scanned PDFs\n\n# Image processing\nconfig.image_max_size = 10.megabytes # Maximum image file size\nconfig.image_formats = %w[jpg jpeg png gif webp bmp tiff]\nconfig.generate_image_descriptions = true\nconfig.image_description_model = 'gpt-4-vision-preview'\n\n# Audio processing\nconfig.audio_max_duration = 3600     # Maximum audio duration (seconds)\nconfig.audio_formats = %w[mp3 wav flac m4a ogg]\nconfig.speech_to_text_provider = :openai\nconfig.audio_chunk_duration = 300    # Chunk audio files (seconds)\n</code></pre>"},{"location":"getting-started/configuration/#search-configuration","title":"Search Configuration","text":""},{"location":"getting-started/configuration/#basic-search-settings","title":"Basic Search Settings","text":"<pre><code>config.search_similarity_threshold = 0.7    # Minimum similarity score\nconfig.max_search_results = 20              # Maximum results per search\nconfig.enable_usage_analytics = true       # Track search usage\nconfig.search_cache_ttl = 300.seconds      # Cache search results\n\n# Cross-modal search\nconfig.enable_cross_modal_search = true\nconfig.content_type_weights = {\n  'text' =&gt; 1.0,\n  'image' =&gt; 0.8,\n  'audio' =&gt; 0.7\n}\n</code></pre>"},{"location":"getting-started/configuration/#advanced-search-configuration","title":"Advanced Search Configuration","text":"<pre><code># Ranking algorithm weights\nconfig.ranking_weights = {\n  similarity: 0.6,                    # Semantic similarity weight\n  usage: 0.3,                         # Usage frequency weight\n  recency: 0.1                        # Document recency weight\n}\n\n# Hybrid search settings\nconfig.enable_fulltext_search = true\nconfig.fulltext_search_weight = 0.3\nconfig.semantic_search_weight = 0.7\n\n# Search suggestions\nconfig.enable_search_suggestions = true\nconfig.suggestion_cache_ttl = 3600.seconds\nconfig.max_suggestions = 10\n\n# Performance settings\nconfig.vector_index_lists = 100      # IVFFlat index lists\nconfig.search_timeout = 30.seconds   # Search operation timeout\nconfig.embedding_cache_size = 1000   # LRU cache for embeddings\n</code></pre>"},{"location":"getting-started/configuration/#background-processing-configuration","title":"Background Processing Configuration","text":""},{"location":"getting-started/configuration/#job-queue-settings","title":"Job Queue Settings","text":"<pre><code>config.enable_background_processing = true\nconfig.job_queue_prefix = 'ragdoll'\nconfig.job_timeout = 300.seconds\nconfig.max_retry_attempts = 3\n\n# Queue priorities\nconfig.embedding_queue_priority = 10    # High priority\nconfig.processing_queue_priority = 5    # Medium priority\nconfig.analysis_queue_priority = 1      # Low priority\n\n# Batch processing\nconfig.batch_processing_size = 100\nconfig.batch_processing_delay = 5.seconds\n</code></pre>"},{"location":"getting-started/configuration/#feature-toggles","title":"Feature Toggles","text":"<pre><code># Optional features\nconfig.enable_keyword_extraction = true\nconfig.enable_document_summarization = true\nconfig.enable_summary_embeddings = true\nconfig.enable_image_descriptions = true\nconfig.enable_audio_transcription = true\n\n# Analytics and monitoring\nconfig.enable_usage_analytics = true\nconfig.enable_performance_monitoring = true\nconfig.enable_search_analytics = true\nconfig.analytics_batch_size = 100\nconfig.analytics_flush_interval = 60.seconds\n</code></pre>"},{"location":"getting-started/configuration/#logging-configuration","title":"Logging Configuration","text":""},{"location":"getting-started/configuration/#log-levels-and-output","title":"Log Levels and Output","text":"<pre><code># Log level configuration\nconfig.log_level = :info              # :debug, :info, :warn, :error, :fatal\nconfig.log_file = '/var/log/ragdoll/ragdoll.log'\nconfig.log_rotation = 'daily'         # 'daily', 'weekly', 'monthly'\nconfig.log_max_size = 100.megabytes   # Maximum log file size\n\n# Structured logging\nconfig.log_format = :json             # :json, :logfmt, :plain\nconfig.log_timestamp = true\nconfig.log_correlation_id = true      # Include correlation IDs\n\n# Component-specific logging\nconfig.log_levels = {\n  'SearchEngine' =&gt; :debug,\n  'EmbeddingService' =&gt; :info,\n  'DocumentProcessor' =&gt; :warn,\n  'BackgroundJobs' =&gt; :info\n}\n</code></pre>"},{"location":"getting-started/configuration/#development-logging","title":"Development Logging","text":"<pre><code># Development-specific logging\nif Rails.env.development?\n  config.log_level = :debug\n  config.log_file = nil              # Log to stdout\n  config.log_format = :plain\n  config.log_sql_queries = true\n  config.log_embedding_requests = true\n  config.log_search_queries = true\nend\n</code></pre>"},{"location":"getting-started/configuration/#performance-configuration","title":"Performance Configuration","text":""},{"location":"getting-started/configuration/#memory-and-caching","title":"Memory and Caching","text":"<pre><code># Cache configuration\nconfig.enable_query_cache = true\nconfig.embedding_cache_ttl = 3600.seconds\nconfig.search_cache_ttl = 300.seconds\nconfig.metadata_cache_ttl = 1800.seconds\n\n# Memory management\nconfig.max_memory_usage = 2.gigabytes\nconfig.garbage_collection_frequency = 1000  # Requests between GC\nconfig.connection_pool_size = 25\nconfig.connection_checkout_timeout = 5.seconds\n</code></pre>"},{"location":"getting-started/configuration/#database-optimization","title":"Database Optimization","text":"<pre><code># Query optimization\nconfig.use_prepared_statements = true\nconfig.enable_query_logging = false   # Disable in production\nconfig.statement_timeout = 30.seconds\nconfig.idle_transaction_timeout = 60.seconds\n\n# Index configuration\nconfig.auto_create_indexes = true\nconfig.vector_index_type = 'ivfflat'  # 'ivfflat' or 'hnsw'\nconfig.index_maintenance_interval = 1.day\n</code></pre>"},{"location":"getting-started/configuration/#environment-specific-configuration","title":"Environment-Specific Configuration","text":""},{"location":"getting-started/configuration/#development-configuration","title":"Development Configuration","text":"<pre><code># config/environments/development.rb\nRagdoll::Core.configure do |config|\n  # Configure Ruby LLM providers\n  config.ruby_llm_config[:openai][:api_key] = ENV['OPENAI_API_KEY']\n\n  # Configure models\n  config.models[:embedding][:text] = 'text-embedding-3-small'\n  config.models[:default] = 'openai/gpt-4o-mini'\n\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_development',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true\n  }\n\n  config.logging_config[:log_level] = :debug\n  config.logging_config[:log_filepath] = nil  # stdout\n\n  # Disable expensive features in development\n  config.summarization_config[:enable] = false\n  config.keywords_config[:enable] = false\n  config.background_processing_config[:enable] = false\n\n  # Fast development settings\n  config.chunking[:text][:max_tokens] = 500\n  config.search[:max_results] = 10\n  config.search[:cache_ttl] = 60\nend\n</code></pre>"},{"location":"getting-started/configuration/#test-configuration","title":"Test Configuration","text":"<pre><code># config/environments/test.rb\nRagdoll::Core.configure do |config|\n  # Use test doubles for LLM services\n  config.ruby_llm_config[:test][:api_key] = 'test-key'\n\n  # Configure test models\n  config.models[:embedding][:text] = 'test-embedding-model'\n  config.models[:default] = 'test/test-model'\n\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_test',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true\n  }\n\n  config.logging_config[:log_level] = :fatal  # Suppress logs in tests\n\n  # Disable external services\n  config.background_processing_config[:enable] = false\n  config.analytics_config[:enable] = false\n  config.summarization_config[:enable] = false\n\n  # Fast test settings\n  config.chunking[:text][:max_tokens] = 100\n  config.search[:max_results] = 5\n  config.search[:similarity_threshold] = 0.5\nend\n</code></pre>"},{"location":"getting-started/configuration/#production-configuration","title":"Production Configuration","text":"<pre><code># config/environments/production.rb\nRagdoll::Core.configure do |config|\n  # Configure Ruby LLM providers\n  config.ruby_llm_config[:openai][:api_key] = ENV['OPENAI_API_KEY']\n\n  # Configure production models\n  config.models[:embedding][:text] = 'text-embedding-3-small'\n  config.models[:default] = 'openai/gpt-4o-mini'\n\n  config.database_config = {\n    adapter: 'postgresql',\n    url: ENV['DATABASE_URL'],\n    pool: ENV.fetch('RAILS_MAX_THREADS', 25).to_i,\n    auto_migrate: false\n  }\n\n  config.logging_config[:log_level] = :info\n  config.logging_config[:log_filepath] = '/var/log/ragdoll/ragdoll.log'\n  config.logging_config[:format] = :json\n\n  # Enable all production features\n  config.background_processing_config[:enable] = true\n  config.analytics_config[:enable] = true\n  config.summarization_config[:enable] = true\n  config.keywords_config[:enable] = true\n\n  # Production performance settings\n  config.chunking[:text][:max_tokens] = 1000\n  config.chunking[:text][:overlap] = 200\n  config.search[:max_results] = 50\n  config.search[:similarity_threshold] = 0.75\n\n  # Cache settings\n  config.search[:cache_ttl] = 300\n  config.embedding_cache[:ttl] = 3600\n\n  # Security settings\n  config.logging_config[:log_requests] = false  # May contain sensitive data\n  config.logging_config[:log_embedding_content] = false   # Don't log actual content\nend\n</code></pre>"},{"location":"getting-started/configuration/#configuration-validation","title":"Configuration Validation","text":""},{"location":"getting-started/configuration/#validation-rules","title":"Validation Rules","text":"<pre><code>class ConfigurationValidator\n  def self.validate!(config)\n    validate_required_settings!(config)\n    validate_llm_provider!(config)\n    validate_database_config!(config)\n    validate_numeric_ranges!(config)\n    validate_feature_dependencies!(config)\n  end\n\n  private\n\n  def self.validate_required_settings!(config)\n    required = [:llm_provider, :embedding_model, :database_config]\n    missing = required.select { |setting| config.send(setting).nil? }\n\n    unless missing.empty?\n      raise ConfigurationError, \"Missing required settings: #{missing.join(', ')}\"\n    end\n  end\n\n  def self.validate_llm_provider!(config)\n    valid_providers = [:openai, :anthropic, :google, :azure, :ollama, :huggingface, :openrouter]\n\n    unless valid_providers.include?(config.llm_provider)\n      raise ConfigurationError, \"Invalid LLM provider: #{config.llm_provider}\"\n    end\n\n    # Validate provider-specific settings\n    case config.llm_provider\n    when :openai\n      raise ConfigurationError, \"OpenAI API key required\" if config.openai_api_key.blank?\n    when :anthropic\n      raise ConfigurationError, \"Anthropic API key required\" if config.anthropic_api_key.blank?\n    # ... other providers\n    end\n  end\nend\n</code></pre>"},{"location":"getting-started/configuration/#dynamic-configuration","title":"Dynamic Configuration","text":""},{"location":"getting-started/configuration/#runtime-configuration-updates","title":"Runtime Configuration Updates","text":"<pre><code># Update configuration at runtime\nRagdoll::Core.configure do |config|\n  config.search_similarity_threshold = 0.8\n  config.max_search_results = 30\nend\n\n# Temporary configuration for specific operations\nRagdoll::Core.with_configuration(llm_provider: :anthropic) do\n  # Use Anthropic for this operation\n  result = Ragdoll::Core.search(\"complex query requiring Claude\")\nend\n</code></pre>"},{"location":"getting-started/configuration/#configuration-monitoring","title":"Configuration Monitoring","text":"<pre><code># Monitor configuration changes\nclass ConfigurationMonitor\n  def self.track_changes\n    Ragdoll::Core.configuration.on_change do |setting, old_value, new_value|\n      Rails.logger.info \"Configuration changed: #{setting} = #{new_value} (was #{old_value})\"\n\n      # Trigger cache invalidation if needed\n      if [:search_similarity_threshold, :max_search_results].include?(setting)\n        SearchCache.clear\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"getting-started/configuration/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/configuration/#1-environment-management","title":"1. Environment Management","text":"<ul> <li>Use environment variables for sensitive settings (API keys, database passwords)</li> <li>Create environment-specific configuration files</li> <li>Validate configuration on application startup</li> <li>Document all configuration options for your team</li> </ul>"},{"location":"getting-started/configuration/#2-security-considerations","title":"2. Security Considerations","text":"<ul> <li>Never commit API keys or passwords to version control</li> <li>Use encrypted credential management in production</li> <li>Disable verbose logging of sensitive data in production</li> <li>Implement configuration validation and sanitization</li> </ul>"},{"location":"getting-started/configuration/#3-performance-optimization","title":"3. Performance Optimization","text":"<ul> <li>Tune chunk sizes based on your content characteristics</li> <li>Adjust similarity thresholds based on search quality requirements</li> <li>Configure appropriate cache TTLs for your usage patterns</li> <li>Monitor and adjust database connection pool sizes</li> </ul>"},{"location":"getting-started/configuration/#4-operational-excellence","title":"4. Operational Excellence","text":"<ul> <li>Implement configuration monitoring and alerting</li> <li>Use structured logging for better observability</li> <li>Plan for configuration changes without service restarts</li> <li>Document configuration dependencies and interactions</li> </ul> <p>The configuration system in Ragdoll provides the flexibility needed for both development simplicity and production sophistication, enabling you to tune the system precisely for your specific use case and operational requirements.</p>"},{"location":"getting-started/installation/","title":"Installation &amp; Setup","text":"<p>This comprehensive guide covers installing Ragdoll in various environments, from development setup to production deployment with all dependencies and configuration options.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>Ruby: 3.0 or higher</li> <li>Database: PostgreSQL 12+ (with pgvector extension) - REQUIRED</li> <li>Memory: 2 GB RAM minimum, 4 GB recommended</li> <li>Storage: 1 GB free space (plus document storage)</li> <li>Network: Internet access for LLM API calls</li> </ul>"},{"location":"getting-started/installation/#recommended-production-requirements","title":"Recommended Production Requirements","text":"<ul> <li>Ruby: 3.2+ </li> <li>Database: PostgreSQL 14+ with pgvector extension</li> <li>Memory: 8 GB RAM or more</li> <li>Storage: 10 GB+ SSD storage</li> <li>CPU: 4+ cores for background processing</li> <li>Network: Stable internet with low latency to LLM providers</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-gem-installation-recommended","title":"Method 1: Gem Installation (Recommended)","text":"<pre><code># Install the gem\ngem install ragdoll\n\n# Verify installation\nruby -e \"require 'ragdoll'; puts Ragdoll::Core::VERSION\"\n</code></pre>"},{"location":"getting-started/installation/#method-2-bundler-for-applications","title":"Method 2: Bundler (For Applications)","text":"<p>Add to your <code>Gemfile</code>:</p> <pre><code># Gemfile\ngem 'ragdoll', '~&gt; 0.1.0'\n\n# Required: PostgreSQL adapter  \ngem 'pg', '~&gt; 1.5'        # PostgreSQL - REQUIRED\n\n# Optional: background job adapter\ngem 'sidekiq', '~&gt; 7.0'   # For production background processing\n</code></pre> <p>Install dependencies:</p> <pre><code>bundle install\n</code></pre>"},{"location":"getting-started/installation/#method-3-development-installation","title":"Method 3: Development Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/madbomber/ragdoll.git\ncd ragdoll\n\n# Install dependencies\nbundle install\n\n# Run tests to verify installation\nbundle exec rake test\n\n# Install locally\nbundle exec rake install\n</code></pre>"},{"location":"getting-started/installation/#database-setup","title":"Database Setup","text":""},{"location":"getting-started/installation/#postgresql-with-pgvector-recommended-for-production","title":"PostgreSQL with pgvector (Recommended for Production)","text":""},{"location":"getting-started/installation/#installation","title":"Installation","text":"<p>Ubuntu/Debian: <pre><code># Install PostgreSQL\nsudo apt update\nsudo apt install postgresql-14 postgresql-contrib-14\n\n# Install pgvector\nsudo apt install postgresql-14-pgvector\n\n# Start PostgreSQL\nsudo systemctl start postgresql\nsudo systemctl enable postgresql\n</code></pre></p> <p>macOS (Homebrew): <pre><code># Install PostgreSQL\nbrew install postgresql@14\n\n# Install pgvector\nbrew install pgvector\n\n# Start PostgreSQL\nbrew services start postgresql@14\n</code></pre></p> <p>CentOS/RHEL: <pre><code># Install PostgreSQL\nsudo dnf install postgresql14-server postgresql14-contrib\n\n# Install pgvector\nsudo dnf install postgresql14-pgvector\n\n# Initialize and start\nsudo postgresql-setup --initdb\nsudo systemctl start postgresql\nsudo systemctl enable postgresql\n</code></pre></p>"},{"location":"getting-started/installation/#database-configuration","title":"Database Configuration","text":"<pre><code># Switch to postgres user\nsudo -u postgres psql\n\n# Create database and user\nCREATE DATABASE ragdoll_development;\nCREATE DATABASE ragdoll_test;\nCREATE DATABASE ragdoll_production;\n\nCREATE USER ragdoll WITH PASSWORD 'your_secure_password';\nGRANT ALL PRIVILEGES ON DATABASE ragdoll_development TO ragdoll;\nGRANT ALL PRIVILEGES ON DATABASE ragdoll_test TO ragdoll;\nGRANT ALL PRIVILEGES ON DATABASE ragdoll_production TO ragdoll;\n\n# Enable pgvector extension on each database\n\\c ragdoll_development\nCREATE EXTENSION vector;\n\n\\c ragdoll_test  \nCREATE EXTENSION vector;\n\n\\c ragdoll_production\nCREATE EXTENSION vector;\n\n\\q\n</code></pre>"},{"location":"getting-started/installation/#pgvector-verification","title":"pgvector Verification","text":"<pre><code># Test pgvector installation\npsql -U ragdoll -d ragdoll_development -c \"SELECT vector('[1,2,3]') &lt;-&gt; vector('[4,5,6]');\"\n# Should return a distance value\n</code></pre> <p>Note: Ragdoll ONLY supports PostgreSQL. SQLite is not supported due to the requirement for pgvector extension and PostgreSQL-specific features.</p>"},{"location":"getting-started/installation/#database-migration","title":"Database Migration","text":"<pre><code># Create and configure Ragdoll client\nrequire 'ragdoll'\n\nRagdoll::Core.configure do |config|\n  config.database_config = {\n    adapter: 'postgresql',  # PostgreSQL REQUIRED\n    database: 'ragdoll_development',\n    username: 'ragdoll',\n    password: 'your_secure_password',\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true  # Automatically run migrations\n  }\nend\n\n# The schema will be created automatically on first use\nclient = Ragdoll::Core.client\n</code></pre>"},{"location":"getting-started/installation/#llm-provider-setup","title":"LLM Provider Setup","text":""},{"location":"getting-started/installation/#openai-recommended","title":"OpenAI (Recommended)","text":"<pre><code># Set your OpenAI API key\nexport OPENAI_API_KEY='sk-your-openai-api-key-here'\n\n# Add to your shell profile for persistence\necho 'export OPENAI_API_KEY=\"sk-your-openai-api-key-here\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>Configuration: <pre><code>Ragdoll::Core.configure do |config|\n  # Configure Ruby LLM providers\n  config.ruby_llm_config[:openai][:api_key] = ENV['OPENAI_API_KEY']\n\n  # Configure embedding models\n  config.models[:embedding][:text] = 'text-embedding-3-small'  # Recommended\n  # config.models[:embedding][:text] = 'text-embedding-3-large'  # Higher quality\n\n  # Set default model\n  config.models[:default] = 'openai/gpt-4o-mini'\nend\n</code></pre></p>"},{"location":"getting-started/installation/#anthropic-claude","title":"Anthropic Claude","text":"<pre><code># Set your Anthropic API key\nexport ANTHROPIC_API_KEY='sk-ant-your-anthropic-key-here'\n</code></pre> <p>Configuration: <pre><code>Ragdoll::Core.configure do |config|\n  # Configure Ruby LLM providers\n  config.ruby_llm_config[:anthropic][:api_key] = ENV['ANTHROPIC_API_KEY']\n  config.ruby_llm_config[:openai][:api_key] = ENV['OPENAI_API_KEY']  # Still needed for embeddings\n\n  # Configure models\n  config.models[:default] = 'anthropic/claude-3-sonnet-20240229'\n  config.models[:summary] = 'anthropic/claude-3-sonnet-20240229'\n  config.models[:embedding][:text] = 'text-embedding-3-small'  # OpenAI embeddings\nend\n</code></pre></p>"},{"location":"getting-started/installation/#google-gemini","title":"Google Gemini","text":"<pre><code># Set your Google API key\nexport GOOGLE_API_KEY='your-google-api-key-here'\n</code></pre> <p>Configuration: <pre><code>Ragdoll::Core.configure do |config|\n  # Configure Ruby LLM providers\n  config.ruby_llm_config[:google][:api_key] = ENV['GOOGLE_API_KEY']\n\n  # Configure models\n  config.models[:default] = 'google/gemini-1.5-pro'\n  config.models[:summary] = 'google/gemini-1.5-pro'\nend\n</code></pre></p>"},{"location":"getting-started/installation/#ollama-local-llm","title":"Ollama (Local LLM)","text":"<pre><code># Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Pull models\nollama pull llama3:8b\nollama pull nomic-embed-text\n\n# Start Ollama service\nollama serve\n</code></pre> <p>Configuration: <pre><code>Ragdoll::Core.configure do |config|\n  # Configure Ruby LLM providers\n  config.ruby_llm_config[:ollama][:endpoint] = 'http://localhost:11434/v1'\n\n  # Configure models\n  config.models[:default] = 'ollama/llama3:8b'\n  config.models[:summary] = 'ollama/llama3:8b'\n  config.models[:embedding][:text] = 'nomic-embed-text'\nend\n</code></pre></p>"},{"location":"getting-started/installation/#development-environment-setup","title":"Development Environment Setup","text":""},{"location":"getting-started/installation/#basic-development-configuration","title":"Basic Development Configuration","text":"<p>Create a configuration file:</p> <pre><code># config/ragdoll.rb\nrequire 'ragdoll'\n\nRagdoll::Core.configure do |config|\n  # Configure Ruby LLM providers\n  config.ruby_llm_config[:openai][:api_key] = ENV['OPENAI_API_KEY']\n\n  # Configure models\n  config.models[:embedding][:text] = 'text-embedding-3-small'\n  config.models[:default] = 'openai/gpt-4o-mini'\n\n  # Development database (PostgreSQL)\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_development',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true\n  }\n\n  # Development settings\n  config.logging_config[:log_level] = :debug\n  config.chunking[:text][:max_tokens] = 800\n  config.chunking[:text][:overlap] = 100\nend\n</code></pre>"},{"location":"getting-started/installation/#rails-integration","title":"Rails Integration","text":"<p>For Rails applications:</p> <pre><code># config/initializers/ragdoll.rb\nRagdoll::Core.configure do |config|\n  # Configure Ruby LLM providers\n  config.ruby_llm_config[:openai][:api_key] = ENV['OPENAI_API_KEY']\n\n  # Configure models\n  config.models[:embedding][:text] = 'text-embedding-3-small'\n  config.models[:default] = 'openai/gpt-4o-mini'\n\n  # Use Rails database configuration\n  config.database_config = Rails.application.config.database_configuration[Rails.env]\n\n  # Logging configuration\n  config.logging_config[:log_level] = Rails.logger.level\n  config.logging_config[:log_filepath] = Rails.root.join('log', 'ragdoll.log').to_s\nend\n</code></pre>"},{"location":"getting-started/installation/#environment-specific-configuration","title":"Environment-Specific Configuration","text":"<pre><code># config/environments/development.rb\nRagdoll::Core.configure do |config|\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_development',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true\n  }\n  config.logging_config[:log_level] = :debug\nend\n\n# config/environments/test.rb  \nRagdoll::Core.configure do |config|\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_test',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true\n  }\n  config.logging_config[:log_level] = :fatal\nend\n\n# config/environments/production.rb\nRagdoll::Core.configure do |config|\n  config.database_config = {\n    adapter: 'postgresql',\n    url: ENV['DATABASE_URL'],\n    pool: 25,\n    auto_migrate: false\n  }\n  config.logging_config[:log_level] = :info\nend\n</code></pre>"},{"location":"getting-started/installation/#background-processing-setup","title":"Background Processing Setup","text":""},{"location":"getting-started/installation/#sidekiq-recommended-for-production","title":"Sidekiq (Recommended for Production)","text":"<pre><code># Install Redis\n# Ubuntu/Debian\nsudo apt install redis-server\n\n# macOS\nbrew install redis\n\n# Start Redis\nredis-server\n</code></pre> <p>Configuration: <pre><code># Gemfile\ngem 'sidekiq', '~&gt; 7.0'\ngem 'redis', '~&gt; 5.0'\n\n# config/initializers/sidekiq.rb\nSidekiq.configure_server do |config|\n  config.redis = { url: ENV['REDIS_URL'] || 'redis://localhost:6379/0' }\nend\n\nSidekiq.configure_client do |config|\n  config.redis = { url: ENV['REDIS_URL'] || 'redis://localhost:6379/0' }\nend\n\n# Enable background processing in Ragdoll\nRagdoll::Core.configure do |config|\n  # Background processing configuration would go here\n  # (Current implementation handles this automatically)\nend\n</code></pre></p> <p>Start Sidekiq workers: <pre><code># Development\nbundle exec sidekiq\n\n# Production with specific queues\nbundle exec sidekiq -q embeddings:3 -q processing:2 -q analysis:1\n</code></pre></p>"},{"location":"getting-started/installation/#alternative-async-development","title":"Alternative: Async (Development)","text":"<pre><code># For development/testing (processing handled automatically)\nRagdoll::Core.configure do |config|\n  # No special configuration needed for development\nend\n</code></pre>"},{"location":"getting-started/installation/#file-storage-configuration","title":"File Storage Configuration","text":""},{"location":"getting-started/installation/#local-storage-development","title":"Local Storage (Development)","text":"<p>File storage is handled internally by the content models (TextContent, ImageContent, AudioContent). No additional configuration is required for basic file handling.</p>"},{"location":"getting-started/installation/#verification-and-testing","title":"Verification and Testing","text":""},{"location":"getting-started/installation/#basic-verification","title":"Basic Verification","text":"<pre><code># test_installation.rb\nrequire 'ragdoll'\n\n# Configure with minimal settings\nRagdoll::Core.configure do |config|\n  # Configure Ruby LLM providers\n  config.ruby_llm_config[:openai][:api_key] = ENV['OPENAI_API_KEY']\n\n  # Configure models\n  config.models[:embedding][:text] = 'text-embedding-3-small'\n  config.models[:default] = 'openai/gpt-4o-mini'\n\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_test',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true\n  }\nend\n\n# Test basic functionality\nputs \"Testing Ragdoll installation...\"\n\n# Test database connection\nclient = Ragdoll::Core.client\nputs \"\u2713 Database connection successful\"\n\n# Test document addition\nresult = Ragdoll.add_document(\n  path: __FILE__,  # Add this test file as a document\n  title: \"Installation Test\"\n)\nputs \"\u2713 Document addition successful: #{result[:success] ? 'OK' : 'FAILED'}\"\n\n# Test search\nsearch_results = Ragdoll.search(query: \"test document\")\nputs \"\u2713 Search functionality working: #{search_results[:results]&amp;.size || 0} results\"\n\n# Test system health\nhealthy = Ragdoll.healthy?\nputs \"\u2713 System health: #{healthy ? 'OK' : 'ISSUES DETECTED'}\"\n\nputs \"\\nInstallation verification complete! \ud83c\udf89\"\nputs \"Ragdoll is ready to use.\"\n</code></pre> <p>Run the verification: <pre><code>ruby test_installation.rb\n</code></pre></p>"},{"location":"getting-started/installation/#component-testing","title":"Component Testing","text":"<pre><code># Test database connection\nruby -e \"\nrequire 'ragdoll'\nRagdoll::Core.configure do |config|\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_development',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost'\n  }\nend\nputs 'Database connection: ' + (Ragdoll::Core::Database.connection.active? ? 'OK' : 'FAILED')\n\"\n\n# Test LLM provider\nruby -e \"\nrequire 'ragdoll'\nputs 'OpenAI API: ' + (ENV['OPENAI_API_KEY'] ? 'OK' : 'MISSING')\n\"\n\n# Test pgvector\npsql -U ragdoll -d ragdoll_development -c \"SELECT vector('[1,2,3]') &lt;-&gt; vector('[4,5,6]');\"\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-installation-issues","title":"Common Installation Issues","text":""},{"location":"getting-started/installation/#1-database-connection-errors","title":"1. Database Connection Errors","text":"<p>Error: <code>PG::ConnectionBad: could not connect to server</code></p> <p>Solutions: <pre><code># Check PostgreSQL status\nsudo systemctl status postgresql\n\n# Start PostgreSQL if stopped\nsudo systemctl start postgresql\n\n# Check connection parameters\npsql -U ragdoll -d ragdoll_development -h localhost\n\n# Verify pg_hba.conf allows connections\nsudo nano /etc/postgresql/14/main/pg_hba.conf\n# Add: local   all   ragdoll   md5\n</code></pre></p>"},{"location":"getting-started/installation/#2-pgvector-extension-issues","title":"2. pgvector Extension Issues","text":"<p>Error: <code>PG::UndefinedFile: could not open extension control file</code></p> <p>Solutions: <pre><code># Reinstall pgvector\nsudo apt remove postgresql-14-pgvector\nsudo apt install postgresql-14-pgvector\n\n# Manually install extension\nsudo -u postgres psql -d ragdoll_development -c \"CREATE EXTENSION vector;\"\n</code></pre></p>"},{"location":"getting-started/installation/#3-ruby-gem-dependencies","title":"3. Ruby Gem Dependencies","text":"<p>Error: <code>LoadError: cannot load such file</code></p> <p>Solutions: <pre><code># Update bundler\ngem update bundler\n\n# Clean and reinstall\nbundle clean --force\nbundle install\n\n# Check Ruby version\nruby --version  # Should be 3.0+\n</code></pre></p>"},{"location":"getting-started/installation/#4-llm-api-issues","title":"4. LLM API Issues","text":"<p>Error: <code>OpenAI API key not set</code></p> <p>Solutions: <pre><code># Check environment variable\necho $OPENAI_API_KEY\n\n# Set temporarily\nexport OPENAI_API_KEY='sk-your-key-here'\n\n# Add to shell profile permanently\necho 'export OPENAI_API_KEY=\"sk-your-key-here\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre></p>"},{"location":"getting-started/installation/#5-permission-issues","title":"5. Permission Issues","text":"<p>Error: <code>Permission denied</code> for database or file operations</p> <p>Solutions: <pre><code># Fix database permissions\nsudo -u postgres psql -c \"GRANT ALL PRIVILEGES ON DATABASE ragdoll_development TO ragdoll;\"\n\n# Fix file permissions\nsudo chown -R $USER:$USER ~/.ragdoll/\nchmod -R 755 ~/.ragdoll/\n</code></pre></p>"},{"location":"getting-started/installation/#performance-issues","title":"Performance Issues","text":""},{"location":"getting-started/installation/#slow-embedding-generation","title":"Slow Embedding Generation","text":"<pre><code># Optimize configuration for better performance\nRagdoll::Core.configure do |config|\n  config.chunking[:text][:max_tokens] = 800        # Smaller chunks for faster processing\n  config.models[:embedding][:text] = 'text-embedding-3-small'  # Faster model\nend\n</code></pre>"},{"location":"getting-started/installation/#database-performance","title":"Database Performance","text":"<pre><code>-- Add indexes for better query performance\nCREATE INDEX CONCURRENTLY idx_embeddings_vector_search \nON ragdoll_embeddings USING ivfflat (embedding_vector vector_cosine_ops);\n\n-- Update statistics\nANALYZE ragdoll_embeddings;\n</code></pre>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<ol> <li>Check Documentation: Review the complete documentation in <code>docs/</code></li> <li>Enable Debug Logging: Set <code>config.log_level = :debug</code></li> <li>Health Check: Run <code>Ragdoll::Core.health_check</code> to identify issues</li> <li>GitHub Issues: Report bugs and feature requests</li> <li>Community: Join discussions and get help from other users</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Quick Start: Follow the Quick Start Guide for basic usage</li> <li>Configuration: Read the Configuration Guide for advanced setup</li> <li>API Reference: Explore the Client API Reference</li> <li>Production Deployment: Plan your Production Deployment</li> </ol> <p>Congratulations! You now have Ragdoll installed and ready to build sophisticated document intelligence applications.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>Get up and running with Ragdoll in 5 minutes. This guide covers installation, basic configuration, and essential operations to demonstrate the powerful document intelligence capabilities.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ruby 3.2+</li> <li>PostgreSQL database with pgvector extension</li> <li>An OpenAI API key (or other supported LLM provider)</li> </ul>"},{"location":"getting-started/quick-start/#installation","title":"Installation","text":""},{"location":"getting-started/quick-start/#option-1-gem-installation","title":"Option 1: Gem Installation","text":"<pre><code>gem install ragdoll\n</code></pre>"},{"location":"getting-started/quick-start/#option-2-bundler","title":"Option 2: Bundler","text":"<p>Add to your Gemfile:</p> <pre><code>gem 'ragdoll'\n</code></pre> <p>Then run:</p> <pre><code>bundle install\n</code></pre>"},{"location":"getting-started/quick-start/#option-3-development-setup","title":"Option 3: Development Setup","text":"<pre><code>git clone https://github.com/madbomber/ragdoll.git\ncd ragdoll\nbundle install\n</code></pre>"},{"location":"getting-started/quick-start/#basic-configuration","title":"Basic Configuration","text":""},{"location":"getting-started/quick-start/#postgresql-setup","title":"PostgreSQL Setup","text":"<pre><code>require 'ragdoll'\n\nRagdoll::Core.configure do |config|\n  # Configure Ruby LLM providers\n  config.ruby_llm_config[:openai][:api_key] = ENV['OPENAI_API_KEY']  # Set your OpenAI API key\n\n  # Configure models\n  config.embedding_config[:text][:model] = 'openai/text-embedding-3-small'\n  config.embedding_config[:default][:model] = 'openai/gpt-4o-mini'\n\n  # PostgreSQL database\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_development',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true  # Automatically create schema\n  }\n\n  # Basic settings\n  config.logging_config[:log_level] = :info\n  config.chunking[:text][:max_tokens] = 1000\n  config.chunking[:text][:overlap] = 200\nend\n</code></pre>"},{"location":"getting-started/quick-start/#production-postgresql-setup","title":"Production PostgreSQL Setup","text":"<pre><code>Ragdoll::Core.configure do |config|\n  # Configure Ruby LLM providers\n  config.ruby_llm_config[:openai][:api_key] = ENV['OPENAI_API_KEY']\n\n  # Configure models\n  config.embedding_config[:text][:model] = 'openai/text-embedding-3-small'\n  config.embedding_config[:default][:model] = 'openai/gpt-4o-mini'\n\n  # PostgreSQL with pgvector\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_production',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true\n  }\nend\n</code></pre>"},{"location":"getting-started/quick-start/#first-steps","title":"First Steps","text":""},{"location":"getting-started/quick-start/#1-add-your-first-document","title":"1. Add Your First Document","text":"<pre><code># Add a PDF document\nresult = Ragdoll::Core.add_document(path: 'research_paper.pdf')\nputs result[:message]\n# =&gt; \"Document 'research_paper' added successfully with ID 123\"\n\n# Store the document ID for later use\ndoc_id = result[:document_id]\n</code></pre>"},{"location":"getting-started/quick-start/#2-check-processing-status","title":"2. Check Processing Status","text":"<pre><code># Check if document processing is complete\nstatus = Ragdoll::Core.document_status(id: doc_id)\nputs \"Status: #{status[:status]}\"\nputs \"Progress: #{status[:progress]}%\"\nputs \"Embeddings: #{status[:embeddings_count]} ready\"\n\n# Wait for processing to complete (in real applications, use background monitoring)\nwhile status[:status] == 'processing'\n  sleep(2)\n  status = Ragdoll::Core.document_status(id: doc_id)\n  puts \"Processing... #{status[:progress]}%\"\nend\n</code></pre>"},{"location":"getting-started/quick-start/#3-search-your-document","title":"3. Search Your Document","text":"<pre><code># Search for content (automatically tracked)\nresults = Ragdoll::Core.search(\n  query: 'machine learning algorithms',\n  session_id: 123,  # Optional: track user sessions\n  user_id:    456   # Optional: track by user\n)\n\nresults.each do |result|\n  puts \"Score: #{result[:similarity_score]}\"\n  puts \"Content: #{result[:content]}\"\n  puts \"Document: #{result[:document_title]}\"\n  puts \"---\"\nend\n\n# View search analytics\nanalytics = Ragdoll::Search.search_analytics(days: 1)\nputs \"Searches today: #{analytics[:total_searches]}\"\nputs \"Avg execution time: #{analytics[:avg_execution_time]}ms\"\n</code></pre>"},{"location":"getting-started/quick-start/#4-use-rag-enhancement","title":"4. Use RAG Enhancement","text":"<pre><code># Get relevant context for a question\ncontext = Ragdoll::Core.get_context(\n  query: 'What are the key benefits of neural networks?',\n  limit: 3\n)\n\nputs \"Found #{context[:context_items].size} relevant passages\"\n\n# Enhance a prompt with context\nenhanced = Ragdoll::Core.enhance_prompt(\n  prompt: 'Explain neural networks',\n  context_limit: 3\n)\n\nputs \"Enhanced prompt:\"\nputs enhanced[:enhanced_prompt]\n</code></pre>"},{"location":"getting-started/quick-start/#common-use-cases","title":"Common Use Cases","text":""},{"location":"getting-started/quick-start/#document-intelligence-pipeline","title":"Document Intelligence Pipeline","text":"<pre><code># Process a collection of research papers\ndocuments_dir = '/path/to/research/papers'\n\n# Add all PDFs in directory\nresult = Ragdoll::Core.add_directory(\n  path: documents_dir,\n  file_patterns: ['*.pdf'],\n  recursive: true\n)\n\nputs \"Processing #{result[:processed_files]} documents...\"\n\n# Wait for processing to complete\nsleep(30)  # In production, use proper background job monitoring\n\n# Search across all documents\nresults = Ragdoll::Core.search(\n  query: 'deep learning optimization techniques',\n  limit: 10\n)\n\nputs \"Found #{results.size} relevant passages across your research library\"\n</code></pre>"},{"location":"getting-started/quick-start/#multi-modal-content","title":"Multi-Modal Content","text":"<pre><code># Add different types of content\ntext_result = Ragdoll::Core.add_text(\n  content: \"This is important information about AI safety...\",\n  title: \"AI Safety Notes\"\n)\n\n# Add an image with description\nimage_result = Ragdoll::Core.add_image(\n  image_path: 'neural_network_diagram.png',\n  description: 'Diagram showing the architecture of a convolutional neural network'\n)\n\n# Search across all content types\nmixed_results = Ragdoll::Core.search(\n  query: 'neural network architecture',\n  content_types: ['text', 'image']\n)\n\nmixed_results.each do |result|\n  puts \"Type: #{result[:content_type]}\"\n  puts \"Content: #{result[:content]}\"\n  puts \"---\"\nend\n</code></pre>"},{"location":"getting-started/quick-start/#knowledge-base-qa","title":"Knowledge Base Q&amp;A","text":"<pre><code># Build a knowledge base\nknowledge_files = [\n  'company_handbook.pdf',\n  'technical_documentation.docx',\n  'meeting_notes.txt',\n  'presentation_slides.pdf'\n]\n\nknowledge_files.each do |file|\n  result = Ragdoll::Core.add_document(path: file)\n  puts \"Added: #{result[:message]}\"\nend\n\n# Wait for processing\nsleep(60)\n\n# Answer questions using RAG\ndef answer_question(question)\n  enhanced = Ragdoll::Core.enhance_prompt(\n    prompt: question,\n    context_limit: 5,\n    include_sources: true\n  )\n\n  puts \"Question: #{question}\"\n  puts \"Context sources: #{enhanced[:sources].join(', ')}\"\n  puts \"Enhanced prompt ready for LLM\"\n  puts enhanced[:enhanced_prompt]\nend\n\nanswer_question(\"What is our company's policy on remote work?\")\nanswer_question(\"How do I configure the authentication system?\")\n</code></pre>"},{"location":"getting-started/quick-start/#configuration-examples","title":"Configuration Examples","text":""},{"location":"getting-started/quick-start/#multiple-llm-providers","title":"Multiple LLM Providers","text":"<pre><code># Use different providers for different tasks\nRagdoll::Core.configure do |config|\n  # Configure multiple Ruby LLM providers\n  config.ruby_llm_config[:openai][:api_key] = ENV['OPENAI_API_KEY']\n  config.ruby_llm_config[:anthropic][:api_key] = ENV['ANTHROPIC_API_KEY']\n  config.ruby_llm_config[:ollama][:endpoint] = 'http://localhost:11434/v1'\n\n  # Configure models for different tasks\n  config.embedding_config[:text][:model] = 'openai/text-embedding-3-small'  # OpenAI embeddings\n  config.embedding_config[:default][:model] = 'openai/gpt-4o-mini'          # OpenAI for general tasks\n  config.summarization_config[:model] = 'anthropic/claude-3-sonnet-20240229'  # Claude for summarization\n  config.keywords_config[:model] = 'ollama/llama3:8b'                         # Local Ollama for keywords\nend\n</code></pre>"},{"location":"getting-started/quick-start/#performance-tuning","title":"Performance Tuning","text":"<pre><code>Ragdoll::Core.configure do |config|\n  # Optimize for your content\n  config.chunking[:text][:max_tokens] = 1500         # Larger chunks for technical documents\n  config.chunking[:text][:overlap] = 300             # More overlap for better context\n  config.search[:similarity_threshold] = 0.8         # Higher threshold for precision\n  config.search[:max_results] = 20                   # More results for comprehensive search\n\n  # Enable advanced features\n  config.analytics_config[:enable] = true\n  config.summarization_config[:enable] = true\n  config.keywords_config[:enable] = true\nend\n</code></pre>"},{"location":"getting-started/quick-start/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"getting-started/quick-start/#check-system-health","title":"Check System Health","text":"<pre><code># System health check\nhealth = Ragdoll::Core.health_check\nputs \"System status: #{health[:status]}\"\nputs \"Components:\"\nhealth[:components].each do |component, status|\n  puts \"  #{component}: #{status}\"\nend\n</code></pre>"},{"location":"getting-started/quick-start/#get-system-statistics","title":"Get System Statistics","text":"<pre><code># Comprehensive statistics\nstats = Ragdoll::Core.stats\n\nputs \"Documents: #{stats[:documents][:total]}\"\nputs \"Embeddings: #{stats[:embeddings][:total]}\"\nputs \"Searches today: #{stats[:usage][:searches_today]}\"\nputs \"Storage used: #{stats[:usage][:storage_used]}\"\n</code></pre>"},{"location":"getting-started/quick-start/#view-recent-activity","title":"View Recent Activity","text":"<pre><code># List recent documents\nrecent_docs = Ragdoll::Core.list_documents(\n  limit: 10,\n  sort: 'created_at',\n  order: 'desc'\n)\n\nputs \"Recent documents:\"\nrecent_docs[:documents].each do |doc|\n  puts \"  #{doc[:title]} (#{doc[:status]}) - #{doc[:created_at]}\"\nend\n</code></pre>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/quick-start/#api-key-not-set","title":"API Key Not Set","text":"<pre><code># Error: Missing OpenAI API key\n# Solution: Set your API key\nENV['OPENAI_API_KEY'] = 'sk-your-actual-api-key'\n</code></pre>"},{"location":"getting-started/quick-start/#database-connection-issues","title":"Database Connection Issues","text":"<pre><code># Error: Database connection failed\n# Solution: Check database configuration\nconfig.database_config = {\n  adapter: 'postgresql',\n  database: 'ragdoll_development',\n  username: 'ragdoll',\n  password: ENV['DATABASE_PASSWORD'],\n  host: 'localhost',\n  port: 5432,\n  auto_migrate: true\n}\n</code></pre>"},{"location":"getting-started/quick-start/#document-processing-stuck","title":"Document Processing Stuck","text":"<pre><code># Check document status\nstatus = Ragdoll::Core.document_status(id: doc_id)\nputs status\n\n# Check background job status\njob_status = Ragdoll::Core.job_status\nputs \"Active jobs: #{job_status[:active_jobs]}\"\nputs \"Failed jobs: #{job_status[:failed_jobs]}\"\n</code></pre>"},{"location":"getting-started/quick-start/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code>Ragdoll::Core.configure do |config|\n  config.logging_config[:log_level] = :debug\n  config.logging_config[:log_filepath] = nil  # Log to stdout for development\nend\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you have Ragdoll running:</p> <ol> <li>Explore Advanced Features: Read the Multi-Modal Architecture guide</li> <li>Optimize Performance: Check the Configuration Guide</li> <li>Scale to Production: Follow the Deployment Guide</li> <li>Build Applications: Review the API Reference</li> </ol>"},{"location":"getting-started/quick-start/#example-applications","title":"Example Applications","text":"<p>Try building these common applications:</p> <ul> <li>Document Q&amp;A System: Upload PDFs and ask questions</li> <li>Research Assistant: Search across academic papers</li> <li>Knowledge Base: Company documentation with smart search</li> <li>Content Analyzer: Extract insights from large document collections</li> <li>Multi-Modal Search: Find content across text, images, and audio</li> </ul>"},{"location":"getting-started/quick-start/#community-and-support","title":"Community and Support","text":"<ul> <li>Documentation: Complete guides in the <code>docs/</code> directory</li> <li>Examples: Working examples for common use cases</li> <li>Issues: Report issues on GitHub</li> <li>Discussions: Join community discussions</li> </ul> <p>Ragdoll provides enterprise-grade document intelligence capabilities out of the box. This quick start barely scratches the surface of what's possible with the full feature set!</p>"},{"location":"operations/background-processing/","title":"Background Processing","text":"<p>Ragdoll implements a comprehensive background processing system using ActiveJob to handle computationally intensive operations without blocking the main application thread. This system is essential for production deployments where document processing, embedding generation, and content analysis must scale efficiently.</p>"},{"location":"operations/background-processing/#overview","title":"Overview","text":"<p>Schema Note: Following recent schema optimization, the <code>embedding_model</code> field is now stored in content-specific tables (text_contents, image_contents, audio_contents) rather than in individual embeddings. This eliminates field duplication while maintaining full functionality through polymorphic relationships.</p> <p>The background processing architecture consists of four specialized job types that handle different aspects of document intelligence:</p> <ul> <li>Ragdoll::GenerateEmbeddingsJob: Vector embedding creation for search</li> <li>Ragdoll::ExtractTextJob: Content extraction from various file formats  </li> <li>Ragdoll::ExtractKeywordsJob: AI-powered keyword analysis</li> <li>Ragdoll::GenerateSummaryJob: Document summarization</li> </ul> <p>All jobs are designed to work together in a coordinated pipeline while maintaining individual reliability and error handling.</p>"},{"location":"operations/background-processing/#architecture","title":"Architecture","text":""},{"location":"operations/background-processing/#job-inheritance-structure","title":"Job Inheritance Structure","text":"<pre><code># Base job class with shared functionality\nclass ApplicationJob &lt; ActiveJob::Base\n  include Ragdoll::Core::Jobs::SharedMethods\n\n  retry_on StandardError, wait: :exponentially_longer, attempts: 3\n  discard_on ActiveJob::DeserializationError\n\n  around_perform :with_job_logging\n  around_perform :with_error_handling\nend\n\n# Specialized job implementations\nclass Ragdoll::GenerateEmbeddingsJob &lt; ApplicationJob\nclass Ragdoll::ExtractTextJob &lt; ApplicationJob  \nclass Ragdoll::ExtractKeywordsJob &lt; ApplicationJob\nclass Ragdoll::GenerateSummaryJob &lt; ApplicationJob\n</code></pre>"},{"location":"operations/background-processing/#queue-configuration","title":"Queue Configuration","text":"<pre><code># Queue priorities and routing\nclass Ragdoll::GenerateEmbeddingsJob &lt; ApplicationJob\n  queue_as :embeddings\n  queue_with_priority 10  # High priority for search functionality\nend\n\nclass Ragdoll::ExtractTextJob &lt; ApplicationJob\n  queue_as :processing\n  queue_with_priority 5   # Medium priority for content extraction\nend\n\nclass Ragdoll::GenerateSummaryJob &lt; ApplicationJob\n  queue_as :analysis\n  queue_with_priority 1   # Lower priority for enhancement features\nend\n</code></pre>"},{"location":"operations/background-processing/#job-types","title":"Job Types","text":""},{"location":"operations/background-processing/#1-generateembeddingsjob","title":"1. GenerateEmbeddingsJob","text":"<p>Purpose: Creates vector embeddings for semantic search functionality.</p> <p>Implementation: <pre><code>class Ragdoll::GenerateEmbeddingsJob &lt; ApplicationJob\n  def perform(embeddable_id, embeddable_type, options = {})\n    embeddable = embeddable_type.constantize.find(embeddable_id)\n\n    # Extract content based on type\n    content = extract_content_for_embedding(embeddable)\n\n    # Generate vector embedding\n    vector = EmbeddingService.generate_embedding(\n      content,\n      model: options[:model] || Configuration.embedding_model\n    )\n\n    # Store embedding with metadata (embedding_model accessed via polymorphic relationship)\n    embeddable.embeddings.create!(\n      embedding_vector: vector,\n      content: content,\n      chunk_index: options[:chunk_index],\n      metadata: build_embedding_metadata(embeddable, options)\n    )\n\n    # Update document processing status\n    update_document_status(embeddable.document)\n\n  rescue EmbeddingService::Error =&gt; e\n    handle_embedding_error(e, embeddable)\n    raise\n  end\n\n  private\n\n  def extract_content_for_embedding(embeddable)\n    case embeddable\n    when TextContent\n      embeddable.content\n    when ImageContent\n      embeddable.description || embeddable.alt_text\n    when AudioContent\n      embeddable.transcript\n    else\n      raise ArgumentError, \"Unsupported embeddable type: #{embeddable.class}\"\n    end\n  end\nend\n</code></pre></p> <p>Features: - \u2705 Multi-modal content support (text, image descriptions, audio transcripts) - \u2705 Configurable embedding models per job - \u2705 Chunk-aware processing for large documents - \u2705 Automatic retry with exponential backoff - \u2705 Error handling with fallback strategies - \u2705 Progress tracking and status updates</p> <p>Usage: <pre><code># Queue embedding generation for text content\ntext_content = TextContent.find(123)\nRagdoll::GenerateEmbeddingsJob.perform_later(text_content.id, 'TextContent')\n\n# Batch processing with options\nTextContent.where(embeddings_count: 0).find_each do |content|\n  Ragdoll::GenerateEmbeddingsJob.perform_later(\n    content.id, \n    'TextContent',\n    model: 'text-embedding-3-large',\n    chunk_index: content.calculate_chunk_index\n  )\nend\n</code></pre></p>"},{"location":"operations/background-processing/#2-extracttextjob","title":"2. ExtractTextJob","text":"<p>Purpose: Extracts text content from various file formats and prepares it for processing.</p> <p>Implementation: <pre><code>class Ragdoll::ExtractTextJob &lt; ApplicationJob\n  def perform(document_id, options = {})\n    document = Document.find(document_id)\n\n    # Extract text based on document type\n    extracted_text = case document.document_type\n    when 'pdf'\n      extract_from_pdf(document)\n    when 'docx'\n      extract_from_docx(document)\n    when 'audio'\n      extract_from_audio(document)  # Speech-to-text\n    when 'image'\n      extract_from_image(document)  # OCR\n    else\n      extract_from_file(document)\n    end\n\n    # Create text content with chunking and embedding model\n    text_content = document.text_contents.create!(\n      content: extracted_text,\n      language_detected: detect_language(extracted_text),\n      embedding_model: options[:embedding_model] || Configuration.embedding_model,\n      chunk_size: options[:chunk_size] || Configuration.chunk_size,\n      chunk_overlap: options[:chunk_overlap] || Configuration.chunk_overlap,\n      extraction_method: determine_extraction_method(document),\n      extracted_at: Time.current\n    )\n\n    # Queue embedding generation\n    Ragdoll::GenerateEmbeddingsJob.perform_later(text_content.id, 'TextContent')\n\n    # Queue additional analysis\n    if Configuration.enable_keyword_extraction?\n      Ragdoll::ExtractKeywordsJob.perform_later(text_content.id)\n    end\n\n    if Configuration.enable_document_summarization?\n      Ragdoll::GenerateSummaryJob.perform_later(text_content.id)\n    end\n\n    # Update document status\n    document.update!(status: 'processed') if document.processing_complete?\n\n  rescue DocumentProcessor::ExtractionError =&gt; e\n    handle_extraction_error(e, document)\n    raise\n  end\nend\n</code></pre></p> <p>Features: - \u2705 Multi-format support (PDF, DOCX, audio, images) - \u2705 Language detection and encoding handling - \u2705 Intelligent text chunking - \u2705 OCR integration for image-based text - \u2705 Speech-to-text for audio content - \u2705 Automatic downstream job queuing - \u2705 Progress tracking and status management</p>"},{"location":"operations/background-processing/#3-extractkeywordsjob","title":"3. ExtractKeywordsJob","text":"<p>Purpose: Performs AI-powered keyword and topic extraction from text content.</p> <p>Implementation: <pre><code>class Ragdoll::ExtractKeywordsJob &lt; ApplicationJob\n  def perform(text_content_id, options = {})\n    text_content = TextContent.find(text_content_id)\n\n    # Generate keywords using LLM\n    keywords_result = TextGenerationService.extract_keywords(\n      text_content.content,\n      model: options[:model] || Configuration.keywords_model,\n      max_keywords: options[:max_keywords] || 10,\n      include_topics: options[:include_topics] || true\n    )\n\n    # Update document metadata\n    document = text_content.document\n    current_metadata = document.metadata || {}\n\n    document.update!(\n      metadata: current_metadata.merge(\n        keywords: keywords_result[:keywords],\n        topics: keywords_result[:topics],\n        keyword_extraction: {\n          model: keywords_result[:model],\n          extracted_at: Time.current,\n          confidence_scores: keywords_result[:confidence_scores]\n        }\n      )\n    )\n\n    # Trigger re-indexing for search\n    if Configuration.enable_search_indexing?\n      reindex_document_for_search(document)\n    end\n\n  rescue TextGenerationService::Error =&gt; e\n    handle_keyword_extraction_error(e, text_content)\n    raise\n  end\nend\n</code></pre></p> <p>Features: - \u2705 LLM-powered keyword extraction - \u2705 Topic modeling and categorization - \u2705 Confidence scoring for extracted terms - \u2705 Metadata integration with document search - \u2705 Configurable extraction parameters - \u2705 Search index updates</p>"},{"location":"operations/background-processing/#4-generatesummaryjob","title":"4. GenerateSummaryJob","text":"<p>Purpose: Creates AI-generated summaries of document content.</p> <p>Implementation: <pre><code>class Ragdoll::GenerateSummaryJob &lt; ApplicationJob\n  def perform(text_content_id, options = {})\n    text_content = TextContent.find(text_content_id)\n\n    # Generate summary using LLM\n    summary_result = TextGenerationService.generate_summary(\n      text_content.content,\n      model: options[:model] || Configuration.summary_model,\n      max_length: options[:max_length] || 200,\n      style: options[:style] || 'academic'\n    )\n\n    # Update document metadata\n    document = text_content.document\n    current_metadata = document.metadata || {}\n\n    document.update!(\n      metadata: current_metadata.merge(\n        summary: summary_result[:summary],\n        summary_metadata: {\n          model: summary_result[:model],\n          length: summary_result[:summary].length,\n          generated_at: Time.current,\n          style: options[:style],\n          reading_time_minutes: estimate_reading_time(summary_result[:summary])\n        }\n      )\n    )\n\n    # Create searchable summary embedding\n    if Configuration.enable_summary_embeddings?\n      Ragdoll::GenerateEmbeddingsJob.perform_later(\n        document.id,\n        'Document', \n        content_type: 'summary',\n        content: summary_result[:summary]\n      )\n    end\n\n  rescue TextGenerationService::Error =&gt; e\n    handle_summary_generation_error(e, text_content)\n    raise\n  end\nend\n</code></pre></p> <p>Features: - \u2705 AI-powered summarization with style options - \u2705 Configurable summary length and format - \u2705 Reading time estimation - \u2705 Summary embedding generation for search - \u2705 Metadata enrichment - \u2705 Multiple summarization models</p>"},{"location":"operations/background-processing/#job-orchestration","title":"Job Orchestration","text":""},{"location":"operations/background-processing/#processing-pipeline","title":"Processing Pipeline","text":"<pre><code># Complete document processing pipeline\nclass DocumentProcessingOrchestrator\n  def self.process_document(document)\n    # 1. Extract text content\n    Ragdoll::ExtractTextJob.perform_later(document.id)\n\n    # 2. Handle multi-modal content\n    if document.has_images?\n      document.image_contents.each do |image|\n        GenerateImageDescriptionJob.perform_later(image.id)\n      end\n    end\n\n    if document.has_audio?\n      document.audio_contents.each do |audio|\n        Ragdoll::ExtractTextJob.perform_later(audio.id, content_type: 'audio')\n      end\n    end\n\n    # 3. Generate embeddings (triggered by content creation)\n    # 4. Extract keywords (triggered by text content creation)\n    # 5. Generate summary (triggered by text content creation)\n  end\nend\n</code></pre>"},{"location":"operations/background-processing/#batch-processing","title":"Batch Processing","text":"<pre><code># Efficient batch processing for large document collections\nclass BatchProcessor\n  def self.process_document_batch(document_ids)\n    # Group by document type for optimized processing\n    documents = Document.where(id: document_ids).includes(:text_contents)\n\n    documents.group_by(&amp;:document_type).each do |type, docs|\n      case type\n      when 'pdf'\n        queue_pdf_batch(docs)\n      when 'audio'\n        queue_audio_batch(docs)\n      when 'image'\n        queue_image_batch(docs)\n      end\n    end\n  end\n\n  private\n\n  def self.queue_pdf_batch(documents)\n    # Batch PDF processing with staggered job scheduling\n    documents.each_with_index do |doc, index|\n      ExtractTextJob.set(wait: index * 2.seconds).perform_later(doc.id)\n    end\n  end\nend\n</code></pre>"},{"location":"operations/background-processing/#error-handling","title":"Error Handling","text":""},{"location":"operations/background-processing/#retry-strategies","title":"Retry Strategies","text":"<pre><code>class ApplicationJob &lt; ActiveJob::Base\n  # Exponential backoff for transient errors\n  retry_on EmbeddingService::RateLimitError, wait: :exponentially_longer, attempts: 5\n  retry_on Net::TimeoutError, wait: 10.seconds, attempts: 3\n\n  # Discard jobs that can't be retried\n  discard_on ActiveRecord::RecordNotFound\n  discard_on DocumentProcessor::UnsupportedFormatError\n\n  # Custom retry logic for specific errors\n  retry_on TextGenerationService::ModelUnavailableError do |job, error|\n    # Try alternative model\n    job.arguments[1] = { model: 'fallback-model' }\n    job.retry_job(wait: 30.seconds)\n  end\nend\n</code></pre>"},{"location":"operations/background-processing/#error-monitoring","title":"Error Monitoring","text":"<pre><code>class ApplicationJob &lt; ActiveJob::Base\n  around_perform :with_error_tracking\n\n  private\n\n  def with_error_tracking\n    start_time = Time.current\n    yield\n    track_job_success(Time.current - start_time)\n  rescue =&gt; error\n    track_job_failure(error, Time.current - start_time)\n\n    # Update document status on critical failures\n    if respond_to?(:document) &amp;&amp; document.present?\n      document.update!(\n        status: 'error',\n        error_message: error.message,\n        error_details: {\n          job_class: self.class.name,\n          error_class: error.class.name,\n          backtrace: error.backtrace.first(10),\n          occurred_at: Time.current\n        }\n      )\n    end\n\n    raise\n  end\nend\n</code></pre>"},{"location":"operations/background-processing/#monitoring-and-analytics","title":"Monitoring and Analytics","text":""},{"location":"operations/background-processing/#job-status-tracking","title":"Job Status Tracking","text":"<pre><code># Track job progress for user feedback\nclass JobStatusTracker\n  def self.track_document_processing(document)\n    {\n      document_id: document.id,\n      status: document.status,\n      jobs_queued: count_queued_jobs(document),\n      jobs_completed: count_completed_jobs(document),\n      estimated_completion: estimate_completion_time(document),\n      processing_steps: {\n        text_extraction: job_status('Ragdoll::ExtractTextJob', document),\n        embedding_generation: job_status('Ragdoll::GenerateEmbeddingsJob', document),\n        keyword_extraction: job_status('Ragdoll::ExtractKeywordsJob', document),\n        summary_generation: job_status('Ragdoll::GenerateSummaryJob', document)\n      }\n    }\n  end\nend\n\n# Usage in API\nGET /api/documents/123/status\n{\n  \"document_id\": 123,\n  \"status\": \"processing\",\n  \"progress\": 65,\n  \"jobs_queued\": 2,\n  \"jobs_completed\": 6,\n  \"estimated_completion\": \"2024-01-15T10:30:00Z\"\n}\n</code></pre>"},{"location":"operations/background-processing/#performance-metrics","title":"Performance Metrics","text":"<pre><code># Background job performance monitoring\nclass JobMetrics\n  def self.embedding_generation_stats\n    {\n      average_duration: calculate_average_duration('Ragdoll::GenerateEmbeddingsJob'),\n      success_rate: calculate_success_rate('Ragdoll::GenerateEmbeddingsJob'),\n      throughput_per_hour: calculate_throughput('Ragdoll::GenerateEmbeddingsJob'),\n      queue_depth: queue_depth('embeddings'),\n      error_types: error_breakdown('Ragdoll::GenerateEmbeddingsJob')\n    }\n  end\nend\n</code></pre>"},{"location":"operations/background-processing/#configuration","title":"Configuration","text":""},{"location":"operations/background-processing/#queue-adapter-setup","title":"Queue Adapter Setup","text":"<pre><code># Production: Sidekiq configuration\n# config/application.rb\nconfig.active_job.queue_adapter = :sidekiq\n\n# Queue priorities and routing\n# config/schedule.yml\n:queues:\n  - [embeddings, 3]     # High priority, 3 workers\n  - [processing, 2]     # Medium priority, 2 workers  \n  - [analysis, 1]       # Low priority, 1 worker\n\n# Development: Async configuration\n# config/environments/development.rb\nconfig.active_job.queue_adapter = :async\n\n# Test: Inline configuration\n# config/environments/test.rb\nconfig.active_job.queue_adapter = :test\n</code></pre>"},{"location":"operations/background-processing/#ragdoll-configuration","title":"Ragdoll Configuration","text":"<pre><code>Ragdoll::Core.configure do |config|\n  # Background processing settings\n  config.enable_background_processing = true\n  config.job_queue_prefix = 'ragdoll'\n\n  # Job-specific settings\n  config.enable_keyword_extraction = true\n  config.enable_document_summarization = true\n  config.enable_summary_embeddings = true\n\n  # Performance tuning\n  config.batch_size = 100\n  config.job_timeout = 300.seconds\n  config.max_retry_attempts = 3\n\n  # Model configuration for jobs\n  config.embedding_model = 'text-embedding-3-small'\n  config.summary_model = 'gpt-4'\n  config.keywords_model = 'gpt-3.5-turbo'\nend\n</code></pre>"},{"location":"operations/background-processing/#production-deployment","title":"Production Deployment","text":""},{"location":"operations/background-processing/#scaling-strategy","title":"Scaling Strategy","text":"<pre><code># Horizontal scaling with multiple job types\n# docker-compose.yml\nservices:\n  ragdoll-embeddings:\n    image: ragdoll-app\n    command: bundle exec sidekiq -q embeddings -c 3\n\n  ragdoll-processing:\n    image: ragdoll-app  \n    command: bundle exec sidekiq -q processing -c 2\n\n  ragdoll-analysis:\n    image: ragdoll-app\n    command: bundle exec sidekiq -q analysis -c 1\n</code></pre>"},{"location":"operations/background-processing/#health-checks","title":"Health Checks","text":"<pre><code># Job queue health monitoring\nclass JobHealthCheck\n  def self.status\n    {\n      queues: queue_status,\n      workers: worker_status,\n      failed_jobs: failed_job_count,\n      processing_lag: calculate_processing_lag,\n      alerts: generate_alerts\n    }\n  end\n\n  def self.queue_status\n    %w[embeddings processing analysis].map do |queue|\n      {\n        name: queue,\n        size: Sidekiq::Queue.new(queue).size,\n        latency: Sidekiq::Queue.new(queue).latency,\n        busy: Sidekiq::Workers.new.select { |_, _, work| work['queue'] == queue }.size\n      }\n    end\n  end\nend\n</code></pre>"},{"location":"operations/background-processing/#best-practices","title":"Best Practices","text":""},{"location":"operations/background-processing/#1-job-design","title":"1. Job Design","text":"<ul> <li>Keep jobs focused on single responsibilities</li> <li>Use appropriate queue priorities</li> <li>Implement comprehensive error handling</li> <li>Design for idempotency when possible</li> </ul>"},{"location":"operations/background-processing/#2-performance-optimization","title":"2. Performance Optimization","text":"<ul> <li>Batch similar operations when feasible</li> <li>Use appropriate retry strategies</li> <li>Monitor queue depths and processing times</li> <li>Scale workers based on job types and volumes</li> </ul>"},{"location":"operations/background-processing/#3-error-management","title":"3. Error Management","text":"<ul> <li>Implement dead letter queues for failed jobs</li> <li>Log comprehensive error context</li> <li>Provide user feedback on job status</li> <li>Plan for graceful degradation</li> </ul>"},{"location":"operations/background-processing/#4-monitoring","title":"4. Monitoring","text":"<ul> <li>Track job performance metrics</li> <li>Monitor queue health and worker utilization</li> <li>Set up alerts for processing delays</li> <li>Analyze failure patterns for improvements</li> </ul> <p>The background processing system in Ragdoll provides a robust foundation for scalable document intelligence operations, ensuring that computationally intensive tasks don't impact user experience while maintaining reliability and observability in production environments.</p>"},{"location":"operations/database-schema/","title":"Database Schema","text":"<p>Ragdoll implements a sophisticated PostgreSQL database schema designed for high-performance vector similarity search and multi-modal content management. The schema uses polymorphic associations, Single Table Inheritance (STI), and advanced PostgreSQL features including pgvector for embedding storage.</p>"},{"location":"operations/database-schema/#polymorphic-multi-modal-database-design","title":"Polymorphic Multi-Modal Database Design","text":"<p>The database architecture is built around three core principles:</p> <ol> <li>Multi-Modal Content Support: Single schema handles text, images, audio, and mixed content</li> <li>Polymorphic Embeddings: Vector embeddings can be associated with any content type</li> <li>Performance Optimization: Advanced indexing strategies for both vector and full-text search</li> </ol>"},{"location":"operations/database-schema/#schema-architecture-overview","title":"Schema Architecture Overview","text":"<pre><code>erDiagram\n    DOCUMENTS {\n        bigint id PK\n        string location UK\n        string title\n        text summary\n        text keywords\n        string document_type\n        string status\n        json metadata\n        timestamp file_modified_at\n        timestamp created_at\n        timestamp updated_at\n    }\n\n    CONTENTS {\n        bigint id PK\n        string type\n        bigint document_id FK\n        string embedding_model\n        text content\n        text data\n        json metadata\n        float duration\n        integer sample_rate\n        timestamp created_at\n        timestamp updated_at\n    }\n\n    EMBEDDINGS {\n        bigint id PK\n        string embeddable_type\n        bigint embeddable_id\n        text content\n        vector embedding_vector\n        integer chunk_index\n        integer usage_count\n        datetime returned_at\n        json metadata\n        timestamp created_at\n        timestamp updated_at\n    }\n\n    DOCUMENTS ||--o{ CONTENTS : has_many\n    CONTENTS ||--o{ EMBEDDINGS : polymorphic\n    DOCUMENTS ||--o{ EMBEDDINGS : polymorphic</code></pre>"},{"location":"operations/database-schema/#key-design-features","title":"Key Design Features","text":""},{"location":"operations/database-schema/#single-table-inheritance-sti-for-content","title":"Single Table Inheritance (STI) for Content","text":"<pre><code># Content types inherit from base Content model\nclass Content &lt; ActiveRecord::Base\n  # Base functionality for all content types\nend\n\nclass TextContent &lt; Content\n  # Text-specific methods and validations\nend\n\nclass ImageContent &lt; Content\n  # Image-specific methods and validations\nend\n\nclass AudioContent &lt; Content\n  # Audio-specific methods and validations\nend\n</code></pre>"},{"location":"operations/database-schema/#polymorphic-embedding-architecture","title":"Polymorphic Embedding Architecture","text":"<pre><code># Embeddings can belong to any content type or document\nbelongs_to :embeddable, polymorphic: true\n\n# Examples:\nembedding.embeddable = text_content\nembedding.embeddable = image_content\nembedding.embeddable = document\n</code></pre>"},{"location":"operations/database-schema/#dual-metadata-system","title":"Dual Metadata System","text":"<ul> <li>Document.metadata: LLM-generated semantic metadata (classification, topics, etc.)</li> <li>Content.metadata: Technical file metadata (encoding, dimensions, etc.)</li> <li>Embedding.metadata: Vector processing metadata (positions, chunks, etc.)</li> </ul>"},{"location":"operations/database-schema/#core-tables","title":"Core Tables","text":"<p>Ragdoll uses four primary tables for data storage:</p>"},{"location":"operations/database-schema/#documents-table-ragdoll_documents","title":"Documents Table (<code>ragdoll_documents</code>)","text":"<p>The central table for document management and LLM-generated metadata storage.</p> <pre><code>CREATE TABLE ragdoll_documents (\n  id                 BIGINT PRIMARY KEY,\n  location           VARCHAR NOT NULL,           -- File path, URL, or identifier\n  title              VARCHAR NOT NULL,           -- Human-readable title\n  summary            TEXT NOT NULL DEFAULT '',  -- LLM-generated summary\n  keywords           TEXT NOT NULL DEFAULT '',  -- LLM-generated keywords\n  document_type      VARCHAR NOT NULL DEFAULT 'text', -- Document format\n  status             VARCHAR NOT NULL DEFAULT 'pending', -- Processing status\n  metadata           JSON DEFAULT '{}',         -- LLM-generated structured metadata\n  file_modified_at   TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n  created_at         TIMESTAMP NOT NULL,\n  updated_at         TIMESTAMP NOT NULL\n);\n</code></pre>"},{"location":"operations/database-schema/#column-details","title":"Column Details","text":"Column Type Purpose Example <code>location</code> VARCHAR Unique document identifier <code>/path/to/doc.pdf</code>, <code>https://example.com/file</code> <code>title</code> VARCHAR Display name for search results \"Machine Learning Research Paper\" <code>summary</code> TEXT LLM-generated content summary \"This paper explores neural networks...\" <code>keywords</code> TEXT Comma-separated keywords \"machine learning, AI, neural networks\" <code>document_type</code> VARCHAR Content format type <code>pdf</code>, <code>text</code>, <code>image</code>, <code>mixed</code> <code>status</code> VARCHAR Processing state <code>pending</code>, <code>processing</code>, <code>completed</code>, <code>failed</code> <code>metadata</code> JSON Structured LLM metadata <code>{\"classification\": \"research\", \"topics\": [...]}</code> <code>file_modified_at</code> TIMESTAMP Source file modification time Used for change detection"},{"location":"operations/database-schema/#relationships","title":"Relationships","text":"<pre><code># Document model relationships\nhas_many :contents, dependent: :destroy\nhas_many :text_contents, -&gt; { where(type: 'TextContent') }\nhas_many :image_contents, -&gt; { where(type: 'ImageContent') }\nhas_many :audio_contents, -&gt; { where(type: 'AudioContent') }\nhas_many :embeddings, as: :embeddable, dependent: :destroy\n</code></pre>"},{"location":"operations/database-schema/#status-values","title":"Status Values","text":"<ul> <li><code>pending</code>: Document added but not yet processed</li> <li><code>processing</code>: Currently being analyzed by LLM services</li> <li><code>completed</code>: All processing finished successfully</li> <li><code>failed</code>: Processing encountered errors</li> <li><code>outdated</code>: Source file modified since last processing</li> </ul>"},{"location":"operations/database-schema/#contents-table-ragdoll_contents","title":"Contents Table (<code>ragdoll_contents</code>)","text":"<p>Single Table Inheritance (STI) table storing all content types with type-specific fields.</p> <pre><code>CREATE TABLE ragdoll_contents (\n  id               BIGINT PRIMARY KEY,\n  type             VARCHAR NOT NULL,              -- STI discriminator\n  document_id      BIGINT NOT NULL REFERENCES ragdoll_documents(id),\n  embedding_model  VARCHAR NOT NULL,              -- Model for embedding generation\n  content          TEXT,                          -- Text content or description\n  data             TEXT,                          -- Raw file data (base64, etc.)\n  metadata         JSON DEFAULT '{}',            -- Technical file metadata\n  duration         FLOAT,                         -- Audio duration (seconds)\n  sample_rate      INTEGER,                       -- Audio sample rate (Hz)\n  created_at       TIMESTAMP NOT NULL,\n  updated_at       TIMESTAMP NOT NULL\n);\n</code></pre>"},{"location":"operations/database-schema/#content-types-sti","title":"Content Types (STI)","text":"<p>TextContent (<code>type = 'TextContent'</code>) - <code>content</code>: The actual text content - <code>metadata</code>: <code>{\"encoding\": \"UTF-8\", \"line_count\": 150, \"chunk_size\": 1000}</code></p> <p>ImageContent (<code>type = 'ImageContent'</code>) - <code>content</code>: LLM-generated image description - <code>data</code>: Base64 encoded image data or file path - <code>metadata</code>: <code>{\"width\": 1920, \"height\": 1080, \"format\": \"PNG\", \"file_size\": 2048576}</code></p> <p>AudioContent (<code>type = 'AudioContent'</code>) - <code>content</code>: Transcribed text or audio description - <code>data</code>: Audio file path or encoded data - <code>duration</code>: Audio length in seconds - <code>sample_rate</code>: Audio quality (e.g., 44100 Hz) - <code>metadata</code>: <code>{\"format\": \"MP3\", \"bitrate\": 320, \"channels\": 2}</code></p>"},{"location":"operations/database-schema/#technical-metadata-examples","title":"Technical Metadata Examples","text":"<pre><code>// TextContent metadata\n{\n  \"encoding\": \"UTF-8\",\n  \"line_count\": 245,\n  \"word_count\": 1850,\n  \"chunk_size\": 1000,\n  \"overlap\": 200,\n  \"processing_time_ms\": 150\n}\n\n// ImageContent metadata\n{\n  \"width\": 1920,\n  \"height\": 1080,\n  \"format\": \"PNG\",\n  \"file_size\": 2048576,\n  \"color_depth\": 24,\n  \"has_transparency\": false,\n  \"exif_data\": {...}\n}\n\n// AudioContent metadata\n{\n  \"format\": \"MP3\",\n  \"bitrate\": 320,\n  \"channels\": 2,\n  \"codec\": \"LAME\",\n  \"file_size\": 5242880,\n  \"transcription_confidence\": 0.95\n}\n</code></pre>"},{"location":"operations/database-schema/#embeddings-table-ragdoll_embeddings","title":"Embeddings Table (<code>ragdoll_embeddings</code>)","text":"<p>Polymorphic table storing vector embeddings with usage tracking and similarity search optimization.</p> <pre><code>CREATE TABLE ragdoll_embeddings (\n  id                 BIGINT PRIMARY KEY,\n  embeddable_type    VARCHAR NOT NULL,           -- Polymorphic type\n  embeddable_id      BIGINT NOT NULL,            -- Polymorphic ID\n  content            TEXT NOT NULL DEFAULT '',  -- Original text that was embedded\n  embedding_vector   VECTOR(1536) NOT NULL,     -- pgvector embedding\n  chunk_index        INTEGER NOT NULL,          -- Chunk ordering\n  usage_count        INTEGER DEFAULT 0,         -- Search usage tracking\n  returned_at        TIMESTAMP,                 -- Last usage timestamp\n  metadata           JSON DEFAULT '{}',         -- Processing metadata\n  created_at         TIMESTAMP NOT NULL,\n  updated_at         TIMESTAMP NOT NULL\n);\n</code></pre>"},{"location":"operations/database-schema/#vector-storage-details","title":"Vector Storage Details","text":"<p>Embedding Vector (<code>embedding_vector</code>) - Uses pgvector's <code>VECTOR(1536)</code> type for OpenAI embeddings - Supports other dimensions: 768 (sentence-transformers), 4096 (large models) - Stored as compressed binary data for performance</p> <p>Polymorphic Associations <pre><code># Can belong to any content type or document\nbelongs_to :embeddable, polymorphic: true\n\n# Examples:\nembedding.embeddable_type = 'TextContent'\nembedding.embeddable_type = 'Document'\nembedding.embeddable_type = 'ImageContent'\n</code></pre></p> <p>Usage Tracking - <code>usage_count</code>: Incremented each time embedding appears in search results - <code>returned_at</code>: Updated when embedding is returned in search - Used for analytics and caching strategies</p> <p>Chunk Management - <code>chunk_index</code>: Orders chunks within a document (0, 1, 2, ...) - <code>content</code>: Original text chunk that was embedded - <code>metadata</code>: Processing info like start/end positions</p>"},{"location":"operations/database-schema/#embedding-metadata-examples","title":"Embedding Metadata Examples","text":"<pre><code>{\n  \"start_position\": 1250,\n  \"end_position\": 2250,\n  \"chunk_size\": 1000,\n  \"overlap\": 200,\n  \"embedding_model\": \"openai/text-embedding-3-small\",\n  \"embedding_dimensions\": 1536,\n  \"generation_time_ms\": 45,\n  \"token_count\": 180\n}\n</code></pre>"},{"location":"operations/database-schema/#search-tracking-tables","title":"Search Tracking Tables","text":"<p>Ragdoll includes comprehensive search tracking capabilities with two additional tables for analytics and similarity analysis.</p>"},{"location":"operations/database-schema/#searches-table-ragdoll_searches","title":"Searches Table (<code>ragdoll_searches</code>)","text":"<p>Stores all search queries with vector embeddings for similarity analysis and performance tracking.</p> <pre><code>CREATE TABLE ragdoll_searches (\n  id                    BIGINT PRIMARY KEY,\n  query                 TEXT NOT NULL,                    -- Original search query\n  query_embedding       VECTOR(1536) NOT NULL,            -- Query vector for similarity\n  search_type           VARCHAR DEFAULT 'semantic',       -- semantic, hybrid, fulltext\n  results_count         INTEGER DEFAULT 0,                -- Number of results returned\n  max_similarity_score  FLOAT,                            -- Highest similarity score\n  min_similarity_score  FLOAT,                            -- Lowest similarity score\n  avg_similarity_score  FLOAT,                            -- Average similarity score\n  search_filters        JSON DEFAULT '{}',                -- Applied filters\n  search_options        JSON DEFAULT '{}',                -- Search options used\n  execution_time_ms     INTEGER,                          -- Query execution time\n  session_id            VARCHAR,                          -- User session identifier\n  user_id               VARCHAR,                          -- User identifier\n  created_at           TIMESTAMP NOT NULL,\n  updated_at           TIMESTAMP NOT NULL\n);\n</code></pre> <p>Key Features: - Stores query embeddings for finding similar searches - Tracks performance metrics (execution time, result counts) - Records search configuration (filters, options, weights) - Links to user sessions for behavior analysis</p>"},{"location":"operations/database-schema/#search-results-table-ragdoll_search_results","title":"Search Results Table (<code>ragdoll_search_results</code>)","text":"<p>Junction table linking searches to returned embeddings with engagement tracking.</p> <pre><code>CREATE TABLE ragdoll_search_results (\n  id                BIGINT PRIMARY KEY,\n  search_id         BIGINT NOT NULL REFERENCES ragdoll_searches(id) ON DELETE CASCADE,\n  embedding_id      BIGINT NOT NULL REFERENCES ragdoll_embeddings(id) ON DELETE CASCADE,\n  similarity_score  FLOAT NOT NULL,                  -- Similarity score for this result\n  result_rank       INTEGER NOT NULL,                -- Position in result list (1-based)\n  clicked           BOOLEAN DEFAULT FALSE,           -- User engagement tracking\n  clicked_at        TIMESTAMP,                       -- When result was clicked\n  created_at        TIMESTAMP NOT NULL,\n  updated_at        TIMESTAMP NOT NULL,\n\n  UNIQUE(search_id, result_rank)                    -- One rank per search\n);\n</code></pre> <p>Key Features: - Tracks which embeddings were returned for each search - Records similarity scores and ranking positions - Monitors user engagement (click-through tracking) - Automatic cascade deletion when searches or embeddings are removed</p>"},{"location":"operations/database-schema/#search-tracking-indexes","title":"Search Tracking Indexes","text":"<pre><code>-- Vector similarity search for finding similar queries\nCREATE INDEX idx_searches_query_embedding \n  ON ragdoll_searches \n  USING ivfflat (query_embedding vector_cosine_ops) \n  WITH (lists = 100);\n\n-- Performance analysis indexes\nCREATE INDEX idx_searches_execution_time \n  ON ragdoll_searches(execution_time_ms DESC);\nCREATE INDEX idx_searches_created_at \n  ON ragdoll_searches(created_at DESC);\nCREATE INDEX idx_searches_session \n  ON ragdoll_searches(session_id);\nCREATE INDEX idx_searches_user \n  ON ragdoll_searches(user_id);\n\n-- Search results indexes\nCREATE INDEX idx_search_results_search \n  ON ragdoll_search_results(search_id);\nCREATE INDEX idx_search_results_embedding_score \n  ON ragdoll_search_results(embedding_id, similarity_score DESC);\nCREATE INDEX idx_search_results_clicked \n  ON ragdoll_search_results(clicked, clicked_at DESC);\n</code></pre> <pre><code>### PostgreSQL Extensions Table\n\nRagdoll requires several PostgreSQL extensions enabled in the database:\n\n```sql\n-- Vector similarity search (REQUIRED)\nCREATE EXTENSION IF NOT EXISTS vector;\n\n-- Text processing extensions\nCREATE EXTENSION IF NOT EXISTS unaccent;  -- Remove accents\nCREATE EXTENSION IF NOT EXISTS pg_trgm;   -- Trigram fuzzy search\n\n-- UUID support\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n</code></pre>"},{"location":"operations/database-schema/#database-requirements","title":"Database Requirements","text":"<p>Ragdoll has strict database requirements for optimal performance and functionality.</p>"},{"location":"operations/database-schema/#postgresql-version-requirements","title":"PostgreSQL Version Requirements","text":"<p>Minimum Requirements: - PostgreSQL 12+ (for JSON operators and improved indexing) - pgvector 0.4.0+ (for vector similarity search) - Recommended: PostgreSQL 14+ with pgvector 0.5.0+</p> <p>Version Compatibility Matrix:</p> PostgreSQL pgvector Status Notes 12.x 0.4.0+ \u2705 Supported Minimum required version 13.x 0.4.0+ \u2705 Supported Good performance 14.x 0.5.0+ \u2705 Recommended Improved vector performance 15.x 0.5.0+ \u2705 Recommended Best JSON performance 16.x 0.5.0+ \u2705 Latest Latest features"},{"location":"operations/database-schema/#required-extensions","title":"Required Extensions","text":""},{"location":"operations/database-schema/#pgvector-extension-critical","title":"pgvector Extension (CRITICAL)","text":"<pre><code>-- Enable vector similarity search\nCREATE EXTENSION IF NOT EXISTS vector;\n\n-- Verify installation\nSELECT * FROM pg_extension WHERE extname = 'vector';\n</code></pre> <p>Installation Methods:</p> <pre><code># Ubuntu/Debian\nsudo apt-get install postgresql-14-pgvector\n\n# macOS with Homebrew\nbrew install pgvector\n\n# Docker\ndocker run -d --name ragdoll-postgres \\\n  -e POSTGRES_PASSWORD=password \\\n  -p 5432:5432 \\\n  pgvector/pgvector:pg14\n</code></pre>"},{"location":"operations/database-schema/#text-processing-extensions","title":"Text Processing Extensions","text":"<pre><code>-- Remove accents for better text search\nCREATE EXTENSION IF NOT EXISTS unaccent;\n\n-- Trigram similarity for fuzzy matching\nCREATE EXTENSION IF NOT EXISTS pg_trgm;\n\n-- UUID generation\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n</code></pre>"},{"location":"operations/database-schema/#database-configuration","title":"Database Configuration","text":""},{"location":"operations/database-schema/#memory-settings","title":"Memory Settings","text":"<pre><code># postgresql.conf optimizations\nshared_buffers = 256MB                    # 25% of RAM for small instances\neffective_cache_size = 1GB                # Available cache memory\nwork_mem = 64MB                          # Per-operation memory\nmaintenance_work_mem = 256MB              # Index maintenance\n\n# Vector-specific settings\nmax_connections = 100                     # Adjust based on needs\nrandom_page_cost = 1.1                   # SSD optimization\n</code></pre>"},{"location":"operations/database-schema/#connection-settings","title":"Connection Settings","text":"<pre><code># database.yml configuration\nproduction:\n  adapter: postgresql\n  database: ragdoll_production\n  username: ragdoll\n  password: &lt;%= ENV['DATABASE_PASSWORD'] %&gt;\n  host: localhost\n  port: 5432\n  pool: 25                                # Connection pool size\n  timeout: 5000                          # Connection timeout (ms)\n  prepared_statements: true              # Performance optimization\n  advisory_locks: true                   # Enable advisory locking\n</code></pre>"},{"location":"operations/database-schema/#performance-optimization","title":"Performance Optimization","text":""},{"location":"operations/database-schema/#vector-search-optimization","title":"Vector Search Optimization","text":"<pre><code>-- Adjust IVFFlat parameters for your dataset size\nSET ivfflat.probes = 10;                 -- Search accuracy vs speed\n\n-- Monitor vector index performance\nEXPLAIN (ANALYZE, BUFFERS) \nSELECT * FROM ragdoll_embeddings \nORDER BY embedding_vector &lt;=&gt; '[0.1,0.2,...]' \nLIMIT 10;\n</code></pre>"},{"location":"operations/database-schema/#json-indexing","title":"JSON Indexing","text":"<pre><code>-- Create expression indexes for frequently queried JSON fields\nCREATE INDEX idx_metadata_classification \nON ragdoll_documents USING btree ((metadata-&gt;&gt;'classification'));\n\nCREATE INDEX idx_metadata_topics \nON ragdoll_documents USING gin ((metadata-&gt;'topics'));\n</code></pre>"},{"location":"operations/database-schema/#full-text-search-optimization","title":"Full-Text Search Optimization","text":"<pre><code>-- Update full-text search statistics\nANALYZE ragdoll_documents;\nANALYZE ragdoll_contents;\n\n-- Monitor full-text query performance\nEXPLAIN (ANALYZE, BUFFERS)\nSELECT * FROM ragdoll_documents \nWHERE to_tsvector('english', title || ' ' || summary) \n      @@ plainto_tsquery('english', 'machine learning');\n</code></pre>"},{"location":"operations/database-schema/#indexing-strategy","title":"Indexing Strategy","text":"<p>Ragdoll uses a comprehensive indexing strategy optimized for vector similarity search, full-text search, and metadata filtering.</p>"},{"location":"operations/database-schema/#vector-similarity-indexes","title":"Vector Similarity Indexes","text":""},{"location":"operations/database-schema/#ivfflat-index-for-embeddings","title":"IVFFlat Index for Embeddings","text":"<pre><code>-- Primary vector similarity index (cosine distance)\nCREATE INDEX index_ragdoll_embeddings_on_embedding_vector_cosine\nON ragdoll_embeddings \nUSING ivfflat (embedding_vector vector_cosine_ops)\nWITH (lists = 100);\n\n-- Alternative: L2 distance index\nCREATE INDEX index_ragdoll_embeddings_on_embedding_vector_l2\nON ragdoll_embeddings \nUSING ivfflat (embedding_vector vector_l2_ops)\nWITH (lists = 100);\n</code></pre>"},{"location":"operations/database-schema/#ivfflat-configuration","title":"IVFFlat Configuration","text":"<p>Lists Parameter Sizing: - Small datasets (&lt; 100K vectors): <code>lists = 100</code> - Medium datasets (100K - 1M vectors): <code>lists = 1000</code> - Large datasets (&gt; 1M vectors): <code>lists = sqrt(rows)</code></p> <p>Query-Time Parameters: <pre><code>-- Adjust search accuracy vs speed\nSET ivfflat.probes = 10;  -- Default: good balance\nSET ivfflat.probes = 1;   -- Fastest, less accurate\nSET ivfflat.probes = 50;  -- Most accurate, slower\n</code></pre></p>"},{"location":"operations/database-schema/#vector-index-maintenance","title":"Vector Index Maintenance","text":"<pre><code>-- Rebuild vector indexes after significant data changes\nREINDEX INDEX index_ragdoll_embeddings_on_embedding_vector_cosine;\n\n-- Monitor index usage\nSELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch\nFROM pg_stat_user_indexes \nWHERE indexname LIKE '%embedding_vector%';\n</code></pre>"},{"location":"operations/database-schema/#full-text-search-indexes","title":"Full-Text Search Indexes","text":""},{"location":"operations/database-schema/#document-full-text-index","title":"Document Full-Text Index","text":"<pre><code>-- Comprehensive full-text search across multiple fields\nCREATE INDEX index_ragdoll_documents_on_fulltext_search\nON ragdoll_documents \nUSING gin (\n  to_tsvector('english', \n    COALESCE(title, '') || ' ' ||\n    COALESCE(metadata-&gt;&gt;'summary', '') || ' ' ||\n    COALESCE(metadata-&gt;&gt;'keywords', '') || ' ' ||\n    COALESCE(metadata-&gt;&gt;'description', '')\n  )\n);\n</code></pre>"},{"location":"operations/database-schema/#content-full-text-index","title":"Content Full-Text Index","text":"<pre><code>-- Content-specific full-text search\nCREATE INDEX index_ragdoll_contents_on_fulltext_search\nON ragdoll_contents \nUSING gin (to_tsvector('english', COALESCE(content, '')));\n</code></pre>"},{"location":"operations/database-schema/#full-text-search-usage","title":"Full-Text Search Usage","text":"<pre><code>-- Example full-text queries\nSELECT * FROM ragdoll_documents \nWHERE to_tsvector('english', title || ' ' || (metadata-&gt;&gt;'summary'))\n      @@ plainto_tsquery('english', 'machine learning neural networks');\n\n-- Ranked full-text search\nSELECT *, ts_rank(to_tsvector('english', title), query) as rank\nFROM ragdoll_documents, plainto_tsquery('english', 'AI research') query\nWHERE to_tsvector('english', title) @@ query\nORDER BY rank DESC;\n</code></pre>"},{"location":"operations/database-schema/#json-metadata-indexes","title":"JSON Metadata Indexes","text":""},{"location":"operations/database-schema/#classification-and-type-indexes","title":"Classification and Type Indexes","text":"<pre><code>-- Document type filtering\nCREATE INDEX index_ragdoll_documents_on_metadata_type\nON ragdoll_documents ((metadata-&gt;&gt;'document_type'));\n\n-- Content classification filtering\nCREATE INDEX index_ragdoll_documents_on_metadata_classification\nON ragdoll_documents ((metadata-&gt;&gt;'classification'));\n\n-- Language filtering\nCREATE INDEX index_ragdoll_documents_on_metadata_language\nON ragdoll_documents ((metadata-&gt;&gt;'language'));\n</code></pre>"},{"location":"operations/database-schema/#composite-json-indexes","title":"Composite JSON Indexes","text":"<pre><code>-- Multi-field filtering\nCREATE INDEX index_ragdoll_documents_on_type_and_classification\nON ragdoll_documents (\n  (metadata-&gt;&gt;'document_type'),\n  (metadata-&gt;&gt;'classification')\n);\n\n-- Date-based filtering with type\nCREATE INDEX index_ragdoll_documents_on_type_and_date\nON ragdoll_documents (\n  (metadata-&gt;&gt;'document_type'),\n  created_at\n);\n</code></pre>"},{"location":"operations/database-schema/#array-and-complex-json-indexes","title":"Array and Complex JSON Indexes","text":"<pre><code>-- GIN index for array fields (topics, keywords)\nCREATE INDEX index_ragdoll_documents_on_metadata_topics\nON ragdoll_documents USING gin ((metadata-&gt;'topics'));\n\nCREATE INDEX index_ragdoll_documents_on_metadata_keywords_array\nON ragdoll_documents USING gin ((metadata-&gt;'keywords'));\n\n-- JSONB path indexes for nested objects\nCREATE INDEX index_ragdoll_documents_on_author_info\nON ragdoll_documents USING gin ((metadata-&gt;'author_info'));\n</code></pre>"},{"location":"operations/database-schema/#performance-optimized-indexes","title":"Performance-Optimized Indexes","text":""},{"location":"operations/database-schema/#primary-key-and-foreign-key-indexes","title":"Primary Key and Foreign Key Indexes","text":"<pre><code>-- Documents table indexes\nCREATE UNIQUE INDEX index_ragdoll_documents_on_location \nON ragdoll_documents (location);  -- Unique document lookup\n\nCREATE INDEX index_ragdoll_documents_on_status \nON ragdoll_documents (status);    -- Processing status filtering\n\nCREATE INDEX index_ragdoll_documents_on_document_type \nON ragdoll_documents (document_type);  -- Document type filtering\n\n-- Contents table indexes\nCREATE INDEX index_ragdoll_contents_on_document_id \nON ragdoll_contents (document_id);     -- Foreign key performance\n\nCREATE INDEX index_ragdoll_contents_on_type \nON ragdoll_contents (type);            -- STI type filtering\n\nCREATE INDEX index_ragdoll_contents_on_embedding_model \nON ragdoll_contents (embedding_model); -- Model-based filtering\n\n-- Embeddings table indexes\nCREATE INDEX index_ragdoll_embeddings_on_embeddable \nON ragdoll_embeddings (embeddable_type, embeddable_id);  -- Polymorphic lookup\n\nCREATE INDEX index_ragdoll_embeddings_on_usage_tracking \nON ragdoll_embeddings (returned_at DESC, usage_count DESC);  -- Analytics\n</code></pre>"},{"location":"operations/database-schema/#composite-performance-indexes","title":"Composite Performance Indexes","text":"<pre><code>-- Multi-column indexes for common query patterns\nCREATE INDEX index_ragdoll_documents_on_type_and_status \nON ragdoll_documents (document_type, status);\n\nCREATE INDEX index_ragdoll_contents_on_document_and_type \nON ragdoll_contents (document_id, type);\n\nCREATE INDEX index_ragdoll_embeddings_on_type_and_usage \nON ragdoll_embeddings (embeddable_type, usage_count DESC);\n</code></pre>"},{"location":"operations/database-schema/#time-based-indexes","title":"Time-Based Indexes","text":"<pre><code>-- Chronological sorting and filtering\nCREATE INDEX index_ragdoll_documents_on_created_at \nON ragdoll_documents (created_at DESC);\n\nCREATE INDEX index_ragdoll_documents_on_file_modified_at \nON ragdoll_documents (file_modified_at DESC);\n\nCREATE INDEX index_ragdoll_embeddings_on_returned_at \nON ragdoll_embeddings (returned_at DESC) \nWHERE returned_at IS NOT NULL;\n</code></pre>"},{"location":"operations/database-schema/#index-maintenance-and-monitoring","title":"Index Maintenance and Monitoring","text":""},{"location":"operations/database-schema/#index-usage-statistics","title":"Index Usage Statistics","text":"<pre><code>-- Monitor index performance\nSELECT \n  schemaname,\n  tablename,\n  indexname,\n  idx_scan as scans,\n  idx_tup_read as tuples_read,\n  idx_tup_fetch as tuples_fetched,\n  idx_scan::float / NULLIF(seq_scan + idx_scan, 0) as index_usage_ratio\nFROM pg_stat_user_indexes \nWHERE schemaname = 'public'\n  AND tablename LIKE 'ragdoll_%'\nORDER BY idx_scan DESC;\n</code></pre>"},{"location":"operations/database-schema/#index-size-monitoring","title":"Index Size Monitoring","text":"<pre><code>-- Check index sizes\nSELECT \n  indexname,\n  tablename,\n  pg_size_pretty(pg_relation_size(indexname::regclass)) as size\nFROM pg_indexes \nWHERE schemaname = 'public'\n  AND tablename LIKE 'ragdoll_%'\nORDER BY pg_relation_size(indexname::regclass) DESC;\n</code></pre>"},{"location":"operations/database-schema/#index-maintenance-commands","title":"Index Maintenance Commands","text":"<pre><code>-- Rebuild indexes after major data changes\nREINDEX TABLE ragdoll_documents;\nREINDEX TABLE ragdoll_contents;\nREINDEX TABLE ragdoll_embeddings;\n\n-- Update table statistics\nANALYZE ragdoll_documents;\nANALYZE ragdoll_contents;\nANALYZE ragdoll_embeddings;\n\n-- Vacuum for space reclamation\nVACUUM ANALYZE ragdoll_documents;\nVACUUM ANALYZE ragdoll_contents;\nVACUUM ANALYZE ragdoll_embeddings;\n</code></pre>"},{"location":"operations/database-schema/#database-migrations","title":"Database Migrations","text":"<p>Ragdoll includes a comprehensive migration system with automatic setup, versioning, and backward compatibility management.</p>"},{"location":"operations/database-schema/#migration-files-overview","title":"Migration Files Overview","text":"<p>The migration system consists of four primary migration files:</p> <ol> <li>001_enable_postgresql_extensions.rb - PostgreSQL extensions setup</li> <li>004_create_ragdoll_documents.rb - Documents table and indexes</li> <li>005_create_ragdoll_embeddings.rb - Embeddings table with vector support</li> <li>006_create_ragdoll_contents.rb - Contents table with STI architecture</li> </ol>"},{"location":"operations/database-schema/#auto-migration-feature","title":"Auto-Migration Feature","text":""},{"location":"operations/database-schema/#automatic-database-setup","title":"Automatic Database Setup","text":"<pre><code># Ragdoll automatically runs migrations on first use\nRagdoll::Core.configure do |config|\n  config.database_config = {\n    adapter: 'postgresql',\n    database: 'ragdoll_production',\n    username: 'ragdoll',\n    password: ENV['DATABASE_PASSWORD'],\n    host: 'localhost',\n    port: 5432,\n    auto_migrate: true  # Enables automatic migrations\n  }\nend\n\n# First client creation triggers migration\nclient = Ragdoll::Core.client  # Automatically sets up database\n</code></pre>"},{"location":"operations/database-schema/#migration-trigger-points","title":"Migration Trigger Points","text":"<pre><code># Auto-migration triggers\nclass DatabaseManager\n  def self.ensure_database_ready!\n    return if @database_initialized\n\n    if Ragdoll.config.database_config[:auto_migrate]\n      run_pending_migrations!\n      verify_extensions!\n      @database_initialized = true\n    end\n  end\n\n  private\n\n  def self.run_pending_migrations!\n    ActiveRecord::Migration.verbose = true\n    ActiveRecord::MigrationContext.new(migrations_path).migrate\n  end\nend\n</code></pre>"},{"location":"operations/database-schema/#manual-migration-process","title":"Manual Migration Process","text":""},{"location":"operations/database-schema/#running-migrations-manually","title":"Running Migrations Manually","text":"<pre><code># Using ActiveRecord migration commands\nbundle exec rake db:migrate\n\n# Run specific migration\nbundle exec rake db:migrate:up VERSION=20240115000001\n\n# Rollback migrations\nbundle exec rake db:migrate:down VERSION=20240115000001\n\n# Check migration status\nbundle exec rake db:migrate:status\n</code></pre>"},{"location":"operations/database-schema/#custom-migration-runner","title":"Custom Migration Runner","text":"<pre><code># Manual migration execution\nclass MigrationRunner\n  def self.run_all_migrations\n    migration_context = ActiveRecord::MigrationContext.new(\n      Rails.root.join('db', 'migrate')\n    )\n\n    migration_context.migrate\n  end\n\n  def self.rollback_to_version(version)\n    migration_context = ActiveRecord::MigrationContext.new(\n      Rails.root.join('db', 'migrate')\n    )\n\n    migration_context.migrate(version)\n  end\n\n  def self.migration_status\n    migration_context = ActiveRecord::MigrationContext.new(\n      Rails.root.join('db', 'migrate')\n    )\n\n    migration_context.migrations_status\n  end\nend\n</code></pre>"},{"location":"operations/database-schema/#schema-versioning","title":"Schema Versioning","text":""},{"location":"operations/database-schema/#migration-version-management","title":"Migration Version Management","text":"<pre><code># Migration file naming convention\n# YYYYMMDDHHMMSS_migration_name.rb\n\n# 001_enable_postgresql_extensions.rb\nclass EnablePostgresqlExtensions &lt; ActiveRecord::Migration[7.0]\n  def up\n    execute \"CREATE EXTENSION IF NOT EXISTS vector\"\n    execute \"CREATE EXTENSION IF NOT EXISTS unaccent\"\n    execute \"CREATE EXTENSION IF NOT EXISTS pg_trgm\"\n    execute \"CREATE EXTENSION IF NOT EXISTS \\\"uuid-ossp\\\"\"\n  end\n\n  def down\n    # Extensions are rarely dropped to avoid data loss\n    # execute \"DROP EXTENSION IF EXISTS vector CASCADE\"\n  end\nend\n</code></pre>"},{"location":"operations/database-schema/#schema-version-tracking","title":"Schema Version Tracking","text":"<pre><code>-- ActiveRecord schema_migrations table\nCREATE TABLE schema_migrations (\n  version VARCHAR PRIMARY KEY\n);\n\n-- Check current schema version\nSELECT version FROM schema_migrations ORDER BY version DESC;\n\n-- Example output:\n-- 20240115000006  (006_create_ragdoll_contents)\n-- 20240115000005  (005_create_ragdoll_embeddings)\n-- 20240115000004  (004_create_ragdoll_documents)\n-- 20240115000001  (001_enable_postgresql_extensions)\n</code></pre>"},{"location":"operations/database-schema/#version-compatibility-matrix","title":"Version Compatibility Matrix","text":"Schema Version Ragdoll Version PostgreSQL pgvector Features 001-004 1.0.0 12+ 0.4.0+ Basic functionality 005 1.1.0 13+ 0.4.0+ Vector embeddings 006 1.2.0 13+ 0.5.0+ Multi-modal content 007+ 2.0.0+ 14+ 0.5.0+ Advanced features"},{"location":"operations/database-schema/#backward-compatibility","title":"Backward Compatibility","text":""},{"location":"operations/database-schema/#migration-safety-features","title":"Migration Safety Features","text":"<pre><code># Safe migration practices in Ragdoll\nclass SafeMigrationBase &lt; ActiveRecord::Migration[7.0]\n  # Disable DDL transactions for extension creation\n  disable_ddl_transaction!\n\n  def safe_add_column(table, column, type, options = {})\n    # Add column with default to avoid locks on large tables\n    add_column table, column, type, options.merge(default: options[:default])\n\n    # Remove default after column is added (if temporary)\n    if options[:temporary_default]\n      change_column_default table, column, nil\n    end\n  end\n\n  def safe_add_index(table, columns, options = {})\n    # Use concurrent index creation for large tables\n    options[:algorithm] = :concurrently\n    add_index table, columns, options\n  end\nend\n</code></pre>"},{"location":"operations/database-schema/#data-migration-strategies","title":"Data Migration Strategies","text":"<pre><code># Data-safe migration example\nclass AddNewMetadataFields &lt; ActiveRecord::Migration[7.0]\n  def up\n    # Add new columns with defaults\n    add_column :ragdoll_documents, :new_field, :string, default: ''\n\n    # Migrate existing data in batches\n    Document.find_in_batches(batch_size: 1000) do |batch|\n      batch.each do |document|\n        document.update_column(:new_field, migrate_old_data(document))\n      end\n    end\n  end\n\n  def down\n    remove_column :ragdoll_documents, :new_field\n  end\n\n  private\n\n  def migrate_old_data(document)\n    # Safe data transformation logic\n    document.old_field&amp;.transform_data || ''\n  end\nend\n</code></pre>"},{"location":"operations/database-schema/#rollback-safety","title":"Rollback Safety","text":"<pre><code># Reversible migration patterns\nclass ReversibleMigration &lt; ActiveRecord::Migration[7.0]\n  def change\n    # Automatically reversible operations\n    create_table :new_table do |t|\n      t.string :name, null: false\n      t.timestamps\n    end\n\n    add_index :new_table, :name\n  end\n\n  # For complex operations requiring explicit rollback\n  def up\n    # Forward migration\n    execute \"CREATE MATERIALIZED VIEW complex_view AS ...\"\n  end\n\n  def down\n    # Explicit rollback\n    execute \"DROP MATERIALIZED VIEW IF EXISTS complex_view\"\n  end\nend\n</code></pre>"},{"location":"operations/database-schema/#migration-best-practices","title":"Migration Best Practices","text":""},{"location":"operations/database-schema/#development-workflow","title":"Development Workflow","text":"<pre><code># 1. Create new migration\nrails generate migration AddFeatureToDocuments feature:string\n\n# 2. Edit migration file with safe practices\n# 3. Test migration on development data\nbundle exec rake db:migrate\n\n# 4. Test rollback\nbundle exec rake db:rollback\n\n# 5. Re-run migration\nbundle exec rake db:migrate\n\n# 6. Commit migration file\ngit add db/migrate/*\ngit commit -m \"Add feature column to documents\"\n</code></pre>"},{"location":"operations/database-schema/#production-deployment","title":"Production Deployment","text":"<pre><code># Production migration checklist:\n# 1. Backup database\npg_dump ragdoll_production &gt; backup_$(date +%Y%m%d_%H%M%S).sql\n\n# 2. Run migration with monitoring\ntime bundle exec rake db:migrate\n\n# 3. Verify data integrity\nbundle exec rake db:migrate:status\n\n# 4. Test application functionality\ncurl -f http://localhost:3000/health\n\n# 5. Monitor performance\npsql -c \"SELECT * FROM pg_stat_activity WHERE state = 'active';\"\n</code></pre>"},{"location":"operations/database-schema/#emergency-rollback-procedures","title":"Emergency Rollback Procedures","text":"<pre><code># If migration causes issues:\n# 1. Stop application\nsudo systemctl stop ragdoll\n\n# 2. Rollback migration\nbundle exec rake db:rollback\n\n# 3. Restore from backup if needed\npsql ragdoll_production &lt; backup_20240115_120000.sql\n\n# 4. Restart application\nsudo systemctl start ragdoll\n\n# 5. Investigate and fix migration\n</code></pre>"},{"location":"operations/database-schema/#database-maintenance","title":"Database Maintenance","text":""},{"location":"operations/database-schema/#regular-maintenance-tasks","title":"Regular Maintenance Tasks","text":""},{"location":"operations/database-schema/#vacuum-and-analyze","title":"Vacuum and Analyze","text":"<pre><code>-- Weekly maintenance routine\nVACUUM ANALYZE ragdoll_documents;\nVACUUM ANALYZE ragdoll_contents;\nVACUUM ANALYZE ragdoll_embeddings;\n\n-- Full vacuum (monthly, during low usage)\nVACUUM FULL ragdoll_embeddings;  -- Reclaim space from deleted vectors\n</code></pre>"},{"location":"operations/database-schema/#index-maintenance","title":"Index Maintenance","text":"<pre><code>-- Rebuild vector indexes after significant changes\nREINDEX INDEX index_ragdoll_embeddings_on_embedding_vector_cosine;\n\n-- Update statistics for query planner\nANALYZE ragdoll_embeddings;\n</code></pre>"},{"location":"operations/database-schema/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>-- Monitor slow queries\nSELECT query, mean_exec_time, calls, total_exec_time\nFROM pg_stat_statements \nWHERE query LIKE '%ragdoll_%'\nORDER BY mean_exec_time DESC\nLIMIT 10;\n\n-- Check index usage\nSELECT indexname, idx_scan, idx_tup_read, idx_tup_fetch\nFROM pg_stat_user_indexes \nWHERE tablename LIKE 'ragdoll_%'\nORDER BY idx_scan DESC;\n</code></pre>"},{"location":"operations/database-schema/#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"operations/database-schema/#backup-strategy","title":"Backup Strategy","text":"<pre><code># Full database backup\npg_dump -Fc ragdoll_production &gt; ragdoll_backup_$(date +%Y%m%d).dump\n\n# Schema-only backup\npg_dump -s ragdoll_production &gt; ragdoll_schema_$(date +%Y%m%d).sql\n\n# Data-only backup\npg_dump -a ragdoll_production &gt; ragdoll_data_$(date +%Y%m%d).sql\n</code></pre>"},{"location":"operations/database-schema/#recovery-procedures","title":"Recovery Procedures","text":"<pre><code># Restore full database\npg_restore -d ragdoll_production ragdoll_backup_20240115.dump\n\n# Restore specific table\npg_restore -d ragdoll_production -t ragdoll_embeddings ragdoll_backup_20240115.dump\n</code></pre> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"operations/monitoring/","title":"Monitoring &amp; Analytics","text":"<p>Ragdoll provides comprehensive monitoring and analytics capabilities built directly into the PostgreSQL database schema. The system tracks usage patterns, performance metrics, and system health through ActiveRecord models and native PostgreSQL features.</p>"},{"location":"operations/monitoring/#usage-tracking-and-system-health","title":"Usage Tracking and System Health","text":"<p>The monitoring system is built around PostgreSQL's native capabilities and pgvector optimization, providing real-time insights into system performance, usage patterns, and content analytics. All monitoring data is stored in the same database as your content, ensuring consistency and reducing infrastructure complexity.</p>"},{"location":"operations/monitoring/#usage-analytics","title":"Usage Analytics","text":"<p>Ragdoll automatically tracks usage patterns through the embedding model's built-in analytics fields. This data drives both performance optimization and business intelligence.</p>"},{"location":"operations/monitoring/#search-analytics","title":"Search Analytics","text":"<p>Every search operation is tracked through the <code>usage_count</code> and <code>returned_at</code> fields in the embeddings table:</p> <pre><code># Get search frequency data\nfreq_data = Ragdoll::Embedding\n  .frequently_used\n  .group(:embeddable_id)\n  .sum(:usage_count)\n\n# Popular content identification\npopular_embeddings = Ragdoll::Embedding\n  .where('usage_count &gt; ?', 10)\n  .joins(:embeddable)\n  .includes(embeddable: :document)\n  .order(usage_count: :desc)\n\n# Recent search patterns\nrecent_activity = Ragdoll::Embedding\n  .where('returned_at &gt; ?', 7.days.ago)\n  .group_by_day(:returned_at)\n  .count\n\n# Query performance metrics\nsearch_performance = {\n  avg_results_per_query: popular_embeddings.average(:usage_count),\n  total_searches: Ragdoll::Embedding.sum(:usage_count),\n  unique_content_accessed: Ragdoll::Embedding.where('usage_count &gt; 0').count\n}\n</code></pre>"},{"location":"operations/monitoring/#embedding-analytics","title":"Embedding Analytics","text":"<p>Track embedding model performance and usage patterns:</p> <pre><code># Embedding usage by model type\nusage_by_model = Ragdoll::Embedding\n  .joins(:embeddable)\n  .group('ragdoll_contents.embedding_model')\n  .count\n\n# Vector quality metrics through similarity distribution\nsimilarity_stats = {\n  high_quality: Ragdoll::Embedding.where('usage_count &gt; 5').count,\n  medium_quality: Ragdoll::Embedding.where('usage_count BETWEEN 1 AND 5').count,\n  unused: Ragdoll::Embedding.where(usage_count: 0).count\n}\n\n# Cache effectiveness (usage-based ranking)\ncache_metrics = {\n  frequently_accessed: Ragdoll::Embedding\n    .where('returned_at &gt; ?', 24.hours.ago)\n    .average(:usage_count),\n  recency_distribution: Ragdoll::Embedding\n    .where('returned_at IS NOT NULL')\n    .group_by_day(:returned_at, last: 30)\n    .count\n}\n</code></pre>"},{"location":"operations/monitoring/#document-analytics","title":"Document Analytics","text":"<p>Monitor document processing and access patterns:</p> <pre><code># Document processing success rates\nprocessing_stats = Ragdoll::Document.group(:status).count\n# =&gt; {\"processed\"=&gt;45, \"pending\"=&gt;3, \"error\"=&gt;2}\n\n# Content type distribution\ncontent_distribution = Ragdoll::Document.group(:document_type).count\n# =&gt; {\"text\"=&gt;25, \"pdf\"=&gt;15, \"image\"=&gt;8, \"audio\"=&gt;2}\n\n# Comprehensive document statistics\ndoc_stats = Ragdoll::Document.stats\n# Returns detailed hash with processing metrics, content counts, etc.\n\n# Storage utilization by content type\nstorage_metrics = {\n  text_content_count: Ragdoll::TextContent.count,\n  image_content_count: Ragdoll::ImageContent.count,\n  audio_content_count: Ragdoll::AudioContent.count,\n  total_embeddings: Ragdoll::Embedding.count,\n  avg_embeddings_per_document: Ragdoll::Document\n    .joins(:text_embeddings, :image_embeddings, :audio_embeddings)\n    .average('COUNT(*)')\n}\n</code></pre>"},{"location":"operations/monitoring/#system-health-monitoring","title":"System Health Monitoring","text":"<p>Monitor system health through PostgreSQL native features and ActiveRecord connection management.</p>"},{"location":"operations/monitoring/#database-health","title":"Database Health","text":"<p>Utilize PostgreSQL's built-in statistics and monitoring capabilities:</p> <pre><code># Connection pool status\npool_status = ActiveRecord::Base.connection_pool.stat\n# =&gt; {size: 20, checked_out: 3, checked_in: 17, ...}\n\n# Query performance metrics using PostgreSQL pg_stat_statements\nActiveRecord::Base.connection.execute(\"\n  SELECT query, calls, total_time, mean_time, rows\n  FROM pg_stat_statements\n  WHERE query LIKE '%ragdoll%'\n  ORDER BY mean_time DESC;\n\")\n\n# Index usage statistics\nindex_stats = ActiveRecord::Base.connection.execute(\"\n  SELECT schemaname, tablename, indexname, idx_tup_read, idx_tup_fetch\n  FROM pg_stat_user_indexes\n  WHERE schemaname = 'public'\n  AND tablename LIKE 'ragdoll_%';\n\")\n\n# Storage utilization\ntable_sizes = ActiveRecord::Base.connection.execute(\"\n  SELECT\n    tablename,\n    pg_size_pretty(pg_total_relation_size('ragdoll_'||tablename)) as size\n  FROM pg_tables\n  WHERE tablename LIKE 'ragdoll_%';\n\")\n</code></pre>"},{"location":"operations/monitoring/#background-job-health","title":"Background Job Health","text":"<p>Monitor ActiveJob performance through document status tracking:</p> <pre><code># Job success/failure tracking through document status\njob_health = {\n  successful_processing: Ragdoll::Document.where(status: 'processed').count,\n  failed_processing: Ragdoll::Document.where(status: 'error').count,\n  pending_jobs: Ragdoll::Document.where(status: 'pending').count,\n  currently_processing: Ragdoll::Document.where(status: 'processing').count\n}\n\n# Processing time analysis\nrecent_docs = Ragdoll::Document\n  .where('created_at &gt; ?', 24.hours.ago)\n  .where(status: 'processed')\n\nprocessing_times = recent_docs.map do |doc|\n  (doc.updated_at - doc.created_at).to_i\nend\n\nperformance_metrics = {\n  avg_processing_time: processing_times.sum / processing_times.length,\n  median_processing_time: processing_times.sort[processing_times.length / 2],\n  max_processing_time: processing_times.max,\n  success_rate: (job_health[:successful_processing].to_f /\n                (job_health[:successful_processing] + job_health[:failed_processing])) * 100\n}\n</code></pre>"},{"location":"operations/monitoring/#memory-and-cpu-monitoring","title":"Memory and CPU Monitoring","text":"<p>Integrate with system monitoring tools and PostgreSQL process information:</p> <pre><code># PostgreSQL process monitoring\nprocess_info = ActiveRecord::Base.connection.execute(\"\n  SELECT\n    pid,\n    usename,\n    application_name,\n    state,\n    query_start,\n    query\n  FROM pg_stat_activity\n  WHERE application_name LIKE '%ragdoll%';\n\")\n\n# Memory usage through Ruby process monitoring\nmemory_usage = {\n  process_memory: `ps -o rss= -p #{Process.pid}`.to_i * 1024, # bytes\n  gc_stats: GC.stat,\n  object_space: ObjectSpace.count_objects\n}\n\n# Connection monitoring\nconnection_health = {\n  active_connections: ActiveRecord::Base.connection_pool.connections.size,\n  checked_out: ActiveRecord::Base.connection_pool.stat[:checked_out],\n  available: ActiveRecord::Base.connection_pool.stat[:size] -\n             ActiveRecord::Base.connection_pool.stat[:checked_out]\n}\n</code></pre>"},{"location":"operations/monitoring/#metrics-collection","title":"Metrics Collection","text":"<p>Ragdoll provides built-in metrics collection through ActiveRecord queries and PostgreSQL native features.</p>"},{"location":"operations/monitoring/#built-in-metrics-endpoints","title":"Built-in Metrics Endpoints","text":"<p>Create monitoring endpoints using the comprehensive statistics methods:</p> <pre><code># Complete system metrics collection\ndef collect_system_metrics\n  {\n    timestamp: Time.current.iso8601,\n    documents: Ragdoll::Document.stats,\n    embeddings: {\n      total: Ragdoll::Embedding.count,\n      by_type: {\n        text: Ragdoll::Embedding.text_embeddings.count,\n        image: Ragdoll::Embedding.image_embeddings.count,\n        audio: Ragdoll::Embedding.audio_embeddings.count\n      },\n      usage_stats: {\n        total_searches: Ragdoll::Embedding.sum(:usage_count),\n        active_last_24h: Ragdoll::Embedding\n          .where('returned_at &gt; ?', 24.hours.ago).count,\n        never_used: Ragdoll::Embedding.where(usage_count: 0).count\n      }\n    },\n    performance: {\n      avg_similarity_threshold: Ragdoll.config.search[:similarity_threshold],\n      max_results_configured: Ragdoll.config.search[:max_results],\n      analytics_enabled: Ragdoll.config.search[:enable_analytics]\n    }\n  }\nend\n\n# Usage pattern metrics\ndef collect_usage_metrics\n  {\n    popular_content: Ragdoll::Embedding\n      .frequently_used\n      .limit(10)\n      .includes(embeddable: :document)\n      .map { |e| {\n        document_title: e.embeddable.document.title,\n        usage_count: e.usage_count,\n        last_accessed: e.returned_at\n      }},\n    search_patterns: {\n      daily_searches: Ragdoll::Embedding\n        .where('returned_at &gt; ?', 30.days.ago)\n        .group_by_day(:returned_at)\n        .sum(:usage_count),\n      content_type_preferences: Ragdoll::Embedding\n        .joins(:embeddable)\n        .group('ragdoll_contents.type')\n        .sum(:usage_count)\n    }\n  }\nend\n</code></pre>"},{"location":"operations/monitoring/#custom-metrics-definition","title":"Custom Metrics Definition","text":"<p>Extend the monitoring system with custom business metrics:</p> <pre><code>class CustomMetrics\n  def self.document_processing_velocity\n    # Documents processed per hour over last 24 hours\n    Ragdoll::Document\n      .where('updated_at &gt; ?', 24.hours.ago)\n      .where(status: 'processed')\n      .group_by_hour(:updated_at)\n      .count\n  end\n\n  def self.embedding_quality_distribution\n    # Distribution of embedding usage as quality indicator\n    {\n      high_quality: Ragdoll::Embedding.where('usage_count &gt;= 10').count,\n      medium_quality: Ragdoll::Embedding.where('usage_count BETWEEN 3 AND 9').count,\n      low_quality: Ragdoll::Embedding.where('usage_count BETWEEN 1 AND 2').count,\n      unused: Ragdoll::Embedding.where(usage_count: 0).count\n    }\n  end\n\n  def self.multi_modal_adoption\n    # Track multi-modal document usage\n    {\n      text_only: Ragdoll::Document.joins(:text_contents)\n        .where.not(id: Ragdoll::Document.joins(:image_contents).select(:id))\n        .where.not(id: Ragdoll::Document.joins(:audio_contents).select(:id))\n        .count,\n      multi_modal: Ragdoll::Document\n        .where(id: Ragdoll::Document.joins(:text_contents, :image_contents).select(:id))\n        .or(Ragdoll::Document.where(id: Ragdoll::Document.joins(:text_contents, :audio_contents).select(:id)))\n        .count\n    }\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#data-export-capabilities","title":"Data Export Capabilities","text":"<p>Export metrics data for external analysis:</p> <pre><code># Export to JSON for external monitoring systems\ndef export_metrics_json(start_date: 30.days.ago, end_date: Time.current)\n  {\n    export_info: {\n      generated_at: Time.current.iso8601,\n      period: { start: start_date.iso8601, end: end_date.iso8601 },\n      ragdoll_version: Ragdoll::Core::VERSION\n    },\n    documents: {\n      created: Ragdoll::Document\n        .where(created_at: start_date..end_date)\n        .group(:status, :document_type)\n        .count,\n      processing_times: Ragdoll::Document\n        .where(created_at: start_date..end_date, status: 'processed')\n        .pluck(:created_at, :updated_at)\n        .map { |created, updated| (updated - created).to_i }\n    },\n    search_analytics: {\n      total_searches: Ragdoll::Embedding\n        .where(returned_at: start_date..end_date)\n        .sum(:usage_count),\n      unique_content_accessed: Ragdoll::Embedding\n        .where(returned_at: start_date..end_date)\n        .distinct\n        .count(:embeddable_id)\n    }\n  }.to_json\nend\n\n# Export to CSV for spreadsheet analysis\ndef export_usage_csv\n  require 'csv'\n\n  CSV.generate(headers: true) do |csv|\n    csv &lt;&lt; ['Document Title', 'Document Type', 'Embedding Count', 'Total Usage', 'Last Accessed']\n\n    Ragdoll::Document.includes(:contents, :text_embeddings).find_each do |doc|\n      csv &lt;&lt; [\n        doc.title,\n        doc.document_type,\n        doc.total_embedding_count,\n        doc.text_embeddings.sum(:usage_count),\n        doc.text_embeddings.maximum(:returned_at)\n      ]\n    end\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#historical-data-retention","title":"Historical Data Retention","text":"<p>Manage historical metrics data with PostgreSQL partitioning and cleanup:</p> <pre><code># Data retention policies\nclass MetricsRetention\n  def self.cleanup_old_usage_data(retention_days: 365)\n    # Clear old returned_at timestamps but keep usage_count\n    old_threshold = retention_days.days.ago\n\n    Ragdoll::Embedding\n      .where('returned_at &lt; ?', old_threshold)\n      .update_all(returned_at: nil)\n  end\n\n  def self.archive_document_metrics(archive_after_days: 180)\n    # Archive processed documents older than threshold\n    archive_threshold = archive_after_days.days.ago\n\n    old_docs = Ragdoll::Document\n      .where('created_at &lt; ? AND status = ?', archive_threshold, 'processed')\n\n    # Could export to JSON before cleanup\n    archive_data = old_docs.map(&amp;:to_hash)\n\n    # Store archive data or remove based on retention policy\n    # This is application-specific implementation\n  end\n\n  def self.metrics_summary_for_period(days: 30)\n    period_start = days.days.ago\n\n    {\n      period: \"#{days} days\",\n      documents_processed: Ragdoll::Document\n        .where('created_at &gt; ? AND status = ?', period_start, 'processed')\n        .count,\n      total_searches: Ragdoll::Embedding\n        .where('returned_at &gt; ?', period_start)\n        .sum(:usage_count),\n      new_embeddings: Ragdoll::Embedding\n        .where('created_at &gt; ?', period_start)\n        .count\n    }\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#performance-dashboards","title":"Performance Dashboards","text":"<p>Create comprehensive dashboards using the built-in metrics collection and PostgreSQL analytics capabilities.</p>"},{"location":"operations/monitoring/#real-time-performance-views","title":"Real-time Performance Views","text":"<p>Build live monitoring dashboards with ActiveRecord queries:</p> <pre><code>class PerformanceDashboard\n  def self.realtime_stats\n    {\n      current_time: Time.current.iso8601,\n      system_status: {\n        total_documents: Ragdoll::Document.count,\n        processing_queue: Ragdoll::Document.where(status: 'pending').count,\n        currently_processing: Ragdoll::Document.where(status: 'processing').count,\n        failed_jobs: Ragdoll::Document.where(status: 'error').count\n      },\n      recent_activity: {\n        documents_added_today: Ragdoll::Document\n          .where('created_at &gt; ?', Date.current)\n          .count,\n        searches_last_hour: Ragdoll::Embedding\n          .where('returned_at &gt; ?', 1.hour.ago)\n          .sum(:usage_count),\n        embeddings_created_today: Ragdoll::Embedding\n          .where('created_at &gt; ?', Date.current)\n          .count\n      },\n      performance_indicators: {\n        avg_search_quality: Ragdoll::Embedding\n          .where('returned_at &gt; ?', 24.hours.ago)\n          .average(:usage_count) || 0,\n        content_utilization: (Ragdoll::Embedding.where('usage_count &gt; 0').count.to_f /\n                             Ragdoll::Embedding.count * 100).round(2),\n        processing_success_rate: calculate_success_rate\n      }\n    }\n  end\n\n  def self.calculate_success_rate\n    total = Ragdoll::Document.count\n    return 0 if total == 0\n\n    successful = Ragdoll::Document.where(status: 'processed').count\n    (successful.to_f / total * 100).round(2)\n  end\nend\n\n# Usage in a web dashboard\ndef dashboard_data\n  {\n    realtime: PerformanceDashboard.realtime_stats,\n    queue_health: {\n      processing_velocity: documents_per_hour,\n      error_rate: error_percentage_last_24h,\n      average_processing_time: avg_processing_time_minutes\n    }\n  }\nend\n</code></pre>"},{"location":"operations/monitoring/#historical-trend-analysis","title":"Historical Trend Analysis","text":"<p>Analyze trends over time using PostgreSQL date functions:</p> <pre><code>class TrendAnalysis\n  def self.document_processing_trends(days: 30)\n    end_date = Date.current\n    start_date = end_date - days.days\n\n    {\n      daily_processing: Ragdoll::Document\n        .where(created_at: start_date..end_date)\n        .where(status: 'processed')\n        .group_by_day(:updated_at, range: start_date..end_date)\n        .count,\n      daily_failures: Ragdoll::Document\n        .where(created_at: start_date..end_date)\n        .where(status: 'error')\n        .group_by_day(:updated_at, range: start_date..end_date)\n        .count,\n      content_type_trends: Ragdoll::Document\n        .where(created_at: start_date..end_date)\n        .group_by_week(:created_at, range: start_date..end_date)\n        .group(:document_type)\n        .count\n    }\n  end\n\n  def self.search_usage_trends(days: 30)\n    end_date = Date.current\n    start_date = end_date - days.days\n\n    {\n      daily_searches: Ragdoll::Embedding\n        .where(returned_at: start_date..end_date)\n        .group_by_day(:returned_at, range: start_date..end_date)\n        .sum(:usage_count),\n      popular_content_over_time: Ragdoll::Embedding\n        .joins(embeddable: :document)\n        .where(returned_at: start_date..end_date)\n        .group('ragdoll_documents.document_type')\n        .group_by_week(:returned_at, range: start_date..end_date)\n        .sum(:usage_count),\n      embedding_efficiency: Ragdoll::Embedding\n        .where('created_at &gt; ?', start_date)\n        .group_by_week(:created_at, range: start_date..end_date)\n        .average(:usage_count)\n    }\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#custom-dashboard-creation","title":"Custom Dashboard Creation","text":"<p>Framework for building custom monitoring dashboards:</p> <pre><code>class CustomDashboard\n  attr_reader :widgets, :refresh_interval\n\n  def initialize(name:, refresh_interval: 30)\n    @name = name\n    @refresh_interval = refresh_interval\n    @widgets = []\n  end\n\n  def add_widget(type:, title:, query_method:, **options)\n    @widgets &lt;&lt; {\n      type: type, # :counter, :chart, :table, :gauge\n      title: title,\n      query_method: query_method,\n      options: options\n    }\n  end\n\n  def render_data\n    @widgets.map do |widget|\n      {\n        type: widget[:type],\n        title: widget[:title],\n        data: send(widget[:query_method]),\n        options: widget[:options],\n        last_updated: Time.current.iso8601\n      }\n    end\n  end\n\n  # Example widget methods\n  def document_count_by_type\n    Ragdoll::Document.group(:document_type).count\n  end\n\n  def embedding_usage_distribution\n    {\n      'Never Used' =&gt; Ragdoll::Embedding.where(usage_count: 0).count,\n      'Low Usage (1-5)' =&gt; Ragdoll::Embedding.where(usage_count: 1..5).count,\n      'Medium Usage (6-20)' =&gt; Ragdoll::Embedding.where(usage_count: 6..20).count,\n      'High Usage (21+)' =&gt; Ragdoll::Embedding.where('usage_count &gt; 20').count\n    }\n  end\n\n  def recent_processing_times\n    Ragdoll::Document\n      .where('created_at &gt; ?', 24.hours.ago)\n      .where(status: 'processed')\n      .limit(50)\n      .pluck(:created_at, :updated_at)\n      .map { |created, updated|\n        {\n          document_id: created.to_i,\n          processing_time: ((updated - created) / 60).round(2) # minutes\n        }\n      }\n  end\nend\n\n# Usage example\ndashboard = CustomDashboard.new(name: \"Ragdoll System Health\", refresh_interval: 60)\ndashboard.add_widget(type: :counter, title: \"Total Documents\",\n                    query_method: :document_count_by_type)\ndashboard.add_widget(type: :chart, title: \"Embedding Usage\",\n                    query_method: :embedding_usage_distribution)\ndashboard.add_widget(type: :table, title: \"Recent Processing Times\",\n                    query_method: :recent_processing_times)\n</code></pre>"},{"location":"operations/monitoring/#alert-configuration","title":"Alert Configuration","text":"<p>Set up monitoring alerts based on system thresholds:</p> <pre><code>class AlertSystem\n  ALERT_THRESHOLDS = {\n    error_rate_percentage: 5.0,\n    queue_length: 100,\n    processing_time_minutes: 30,\n    disk_usage_percentage: 80.0,\n    connection_pool_usage: 80.0\n  }\n\n  def self.check_system_health\n    alerts = []\n\n    # Check error rate\n    error_rate = calculate_error_rate\n    if error_rate &gt; ALERT_THRESHOLDS[:error_rate_percentage]\n      alerts &lt;&lt; create_alert(\n        severity: :high,\n        type: :error_rate,\n        message: \"Error rate #{error_rate}% exceeds threshold #{ALERT_THRESHOLDS[:error_rate_percentage]}%\",\n        current_value: error_rate\n      )\n    end\n\n    # Check queue length\n    queue_length = Ragdoll::Document.where(status: 'pending').count\n    if queue_length &gt; ALERT_THRESHOLDS[:queue_length]\n      alerts &lt;&lt; create_alert(\n        severity: :medium,\n        type: :queue_length,\n        message: \"Processing queue length #{queue_length} exceeds threshold #{ALERT_THRESHOLDS[:queue_length]}\",\n        current_value: queue_length\n      )\n    end\n\n    # Check connection pool usage\n    pool_usage = connection_pool_usage_percentage\n    if pool_usage &gt; ALERT_THRESHOLDS[:connection_pool_usage]\n      alerts &lt;&lt; create_alert(\n        severity: :high,\n        type: :connection_pool,\n        message: \"Connection pool usage #{pool_usage}% exceeds threshold #{ALERT_THRESHOLDS[:connection_pool_usage]}%\",\n        current_value: pool_usage\n      )\n    end\n\n    alerts\n  end\n\n  private\n\n  def self.calculate_error_rate\n    total_docs = Ragdoll::Document.where('created_at &gt; ?', 24.hours.ago).count\n    return 0 if total_docs == 0\n\n    error_docs = Ragdoll::Document.where('created_at &gt; ?', 24.hours.ago)\n                                               .where(status: 'error').count\n    (error_docs.to_f / total_docs * 100).round(2)\n  end\n\n  def self.connection_pool_usage_percentage\n    pool = ActiveRecord::Base.connection_pool\n    (pool.stat[:checked_out].to_f / pool.stat[:size] * 100).round(2)\n  end\n\n  def self.create_alert(severity:, type:, message:, current_value:)\n    {\n      id: SecureRandom.uuid,\n      timestamp: Time.current.iso8601,\n      severity: severity,\n      type: type,\n      message: message,\n      current_value: current_value,\n      threshold: ALERT_THRESHOLDS[type],\n      system: 'ragdoll'\n    }\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#alerting-system","title":"Alerting System","text":"<p>Implement comprehensive alerting based on system thresholds and anomaly detection using PostgreSQL and ActiveRecord.</p>"},{"location":"operations/monitoring/#threshold-based-alerts","title":"Threshold-based Alerts","text":"<p>Define and monitor system thresholds:</p> <pre><code>class ThresholdAlerts\n  THRESHOLDS = {\n    # Processing performance\n    error_rate: { warning: 2.0, critical: 5.0 }, # percentage\n    queue_length: { warning: 50, critical: 100 },\n    avg_processing_time: { warning: 300, critical: 600 }, # seconds\n\n    # System resources\n    connection_pool_usage: { warning: 70.0, critical: 85.0 }, # percentage\n    disk_usage: { warning: 75.0, critical: 90.0 }, # percentage\n\n    # Content metrics\n    unused_embeddings: { warning: 50.0, critical: 70.0 }, # percentage\n    search_volume_drop: { warning: 30.0, critical: 50.0 } # percentage decrease\n  }\n\n  def self.check_all_thresholds\n    alerts = []\n\n    THRESHOLDS.each do |metric, thresholds|\n      current_value = send(\"get_#{metric}\")\n\n      if current_value &gt;= thresholds[:critical]\n        alerts &lt;&lt; create_threshold_alert(metric, :critical, current_value, thresholds[:critical])\n      elsif current_value &gt;= thresholds[:warning]\n        alerts &lt;&lt; create_threshold_alert(metric, :warning, current_value, thresholds[:warning])\n      end\n    end\n\n    alerts\n  end\n\n  private\n\n  def self.get_error_rate\n    total = Ragdoll::Document.where('created_at &gt; ?', 24.hours.ago).count\n    return 0 if total == 0\n\n    errors = Ragdoll::Document.where('created_at &gt; ?', 24.hours.ago)\n                                           .where(status: 'error').count\n    (errors.to_f / total * 100).round(2)\n  end\n\n  def self.get_queue_length\n    Ragdoll::Document.where(status: 'pending').count\n  end\n\n  def self.get_avg_processing_time\n    recent_docs = Ragdoll::Document\n      .where('created_at &gt; ?', 24.hours.ago)\n      .where(status: 'processed')\n      .pluck(:created_at, :updated_at)\n\n    return 0 if recent_docs.empty?\n\n    times = recent_docs.map { |created, updated| (updated - created).to_i }\n    times.sum / times.length\n  end\n\n  def self.get_unused_embeddings\n    total = Ragdoll::Embedding.count\n    return 0 if total == 0\n\n    unused = Ragdoll::Embedding.where(usage_count: 0).count\n    (unused.to_f / total * 100).round(2)\n  end\n\n  def self.create_threshold_alert(metric, severity, current_value, threshold)\n    {\n      id: SecureRandom.uuid,\n      type: :threshold,\n      metric: metric,\n      severity: severity,\n      current_value: current_value,\n      threshold: threshold,\n      message: \"#{metric.to_s.humanize} #{current_value} exceeds #{severity} threshold #{threshold}\",\n      timestamp: Time.current.iso8601\n    }\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#anomaly-detection","title":"Anomaly Detection","text":"<p>Detect unusual patterns in system behavior:</p> <pre><code>class AnomalyDetection\n  def self.detect_search_anomalies(lookback_days: 7)\n    anomalies = []\n\n    # Get baseline search volume\n    baseline_searches = daily_search_volume(lookback_days)\n    return anomalies if baseline_searches.empty?\n\n    baseline_avg = baseline_searches.values.sum / baseline_searches.length\n    baseline_std = calculate_standard_deviation(baseline_searches.values)\n\n    # Check today's volume\n    today_volume = daily_search_volume(1).values.first || 0\n\n    # Detect significant deviations (2 standard deviations)\n    if (today_volume - baseline_avg).abs &gt; (2 * baseline_std)\n      severity = today_volume &lt; baseline_avg ? :warning : :info\n      anomalies &lt;&lt; {\n        type: :search_volume_anomaly,\n        severity: severity,\n        current_value: today_volume,\n        baseline_average: baseline_avg.round(2),\n        deviation: ((today_volume - baseline_avg) / baseline_avg * 100).round(2),\n        message: \"Search volume #{today_volume} deviates significantly from baseline #{baseline_avg.round(2)}\"\n      }\n    end\n\n    anomalies\n  end\n\n  def self.detect_processing_anomalies\n    anomalies = []\n\n    # Check for unusual processing patterns\n    recent_times = Ragdoll::Document\n      .where('created_at &gt; ?', 24.hours.ago)\n      .where(status: 'processed')\n      .pluck(:created_at, :updated_at)\n      .map { |created, updated| (updated - created).to_i }\n\n    return anomalies if recent_times.length &lt; 10\n\n    avg_time = recent_times.sum / recent_times.length\n    std_dev = calculate_standard_deviation(recent_times)\n\n    # Find outliers (processing times &gt; 3 standard deviations)\n    outliers = recent_times.select { |time| (time - avg_time).abs &gt; (3 * std_dev) }\n\n    if outliers.any?\n      anomalies &lt;&lt; {\n        type: :processing_time_outliers,\n        severity: :warning,\n        outlier_count: outliers.length,\n        max_outlier_time: outliers.max,\n        baseline_average: avg_time.round(2),\n        message: \"#{outliers.length} documents had unusual processing times (max: #{outliers.max}s)\"\n      }\n    end\n\n    anomalies\n  end\n\n  private\n\n  def self.daily_search_volume(days)\n    Ragdoll::Embedding\n      .where('returned_at &gt; ?', days.days.ago)\n      .group_by_day(:returned_at, last: days)\n      .sum(:usage_count)\n  end\n\n  def self.calculate_standard_deviation(values)\n    return 0 if values.empty?\n\n    mean = values.sum / values.length\n    variance = values.sum { |v| (v - mean) ** 2 } / values.length\n    Math.sqrt(variance)\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#notification-channels","title":"Notification Channels","text":"<p>Integrate with various notification systems:</p> <pre><code>class NotificationManager\n  def self.send_alert(alert, channels: [:log, :webhook])\n    channels.each do |channel|\n      case channel\n      when :log\n        log_alert(alert)\n      when :webhook\n        send_webhook_alert(alert)\n      when :email\n        send_email_alert(alert)\n      when :slack\n        send_slack_alert(alert)\n      end\n    end\n  end\n\n  private\n\n  def self.log_alert(alert)\n    logger = defined?(Rails) ? Rails.logger : Logger.new(STDOUT)\n\n    case alert[:severity]\n    when :critical\n      logger.error \"[RAGDOLL CRITICAL] #{alert[:message]}\"\n    when :warning\n      logger.warn \"[RAGDOLL WARNING] #{alert[:message]}\"\n    else\n      logger.info \"[RAGDOLL INFO] #{alert[:message]}\"\n    end\n  end\n\n  def self.send_webhook_alert(alert)\n    webhook_url = ENV['RAGDOLL_WEBHOOK_URL']\n    return unless webhook_url\n\n    payload = {\n      service: 'ragdoll',\n      alert: alert,\n      timestamp: Time.current.iso8601,\n      environment: ENV['RAILS_ENV'] || 'development'\n    }\n\n    # Use Faraday or Net::HTTP to send webhook\n    require 'net/http'\n    require 'json'\n\n    uri = URI(webhook_url)\n    http = Net::HTTP.new(uri.host, uri.port)\n    http.use_ssl = uri.scheme == 'https'\n\n    request = Net::HTTP::Post.new(uri)\n    request['Content-Type'] = 'application/json'\n    request.body = payload.to_json\n\n    response = http.request(request)\n    puts \"Webhook sent: #{response.code}\" if response.code != '200'\n  end\n\n  def self.send_slack_alert(alert)\n    slack_webhook = ENV['SLACK_WEBHOOK_URL']\n    return unless slack_webhook\n\n    color = case alert[:severity]\n            when :critical then '#FF0000'\n            when :warning then '#FFA500'\n            else '#36A64F'\n            end\n\n    payload = {\n      attachments: [{\n        color: color,\n        title: \"Ragdoll #{alert[:severity].to_s.upcase} Alert\",\n        text: alert[:message],\n        fields: [\n          { title: \"Metric\", value: alert[:metric], short: true },\n          { title: \"Current Value\", value: alert[:current_value], short: true },\n          { title: \"Timestamp\", value: alert[:timestamp], short: false }\n        ]\n      }]\n    }\n\n    # Send to Slack webhook\n    # Implementation similar to webhook above\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#alert-escalation","title":"Alert Escalation","text":"<p>Implement alert escalation policies:</p> <pre><code>class AlertEscalation\n  ESCALATION_RULES = {\n    critical: {\n      immediate: [:log, :webhook, :slack],\n      after_5_minutes: [:email],\n      after_15_minutes: [:sms] # if configured\n    },\n    warning: {\n      immediate: [:log],\n      after_10_minutes: [:webhook],\n      after_30_minutes: [:email]\n    }\n  }\n\n  def self.process_alert(alert)\n    # Store alert for tracking\n    alert_record = store_alert(alert)\n\n    # Send immediate notifications\n    immediate_channels = ESCALATION_RULES.dig(alert[:severity], :immediate) || [:log]\n    NotificationManager.send_alert(alert, channels: immediate_channels)\n\n    # Schedule escalation if needed\n    schedule_escalation(alert_record) if alert[:severity] == :critical\n  end\n\n  def self.check_escalations\n    # This would typically be called by a background job\n    unresolved_alerts = get_unresolved_alerts\n\n    unresolved_alerts.each do |alert_record|\n      escalate_if_needed(alert_record)\n    end\n  end\n\n  private\n\n  def self.store_alert(alert)\n    # Store in database or memory store for tracking\n    {\n      id: alert[:id],\n      created_at: Time.current,\n      alert_data: alert,\n      escalation_level: 0,\n      resolved_at: nil\n    }\n  end\n\n  def self.escalate_if_needed(alert_record)\n    minutes_since_creation = (Time.current - alert_record[:created_at]) / 60\n    severity = alert_record[:alert_data][:severity]\n\n    escalation_rules = ESCALATION_RULES[severity] || {}\n\n    escalation_rules.each do |time_key, channels|\n      next unless time_key.to_s.include?('after_')\n\n      threshold_minutes = time_key.to_s.match(/after_(\\d+)_minutes/)&amp;.captures&amp;.first.to_i\n      next unless threshold_minutes\n\n      if minutes_since_creation &gt;= threshold_minutes &amp;&amp;\n         alert_record[:escalation_level] &lt; threshold_minutes\n\n        NotificationManager.send_alert(alert_record[:alert_data], channels: channels)\n        alert_record[:escalation_level] = threshold_minutes\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#integration-with-external-tools","title":"Integration with External Tools","text":"<p>Ragdoll integrates seamlessly with popular monitoring and observability platforms through standardized metrics and APIs.</p>"},{"location":"operations/monitoring/#prometheus-integration","title":"Prometheus Integration","text":"<p>Expose metrics in Prometheus format for scraping:</p> <pre><code># Prometheus metrics exporter\nclass PrometheusExporter\n  def self.metrics\n    output = []\n\n    # Document metrics\n    doc_stats = Ragdoll::Document.group(:status).count\n    doc_stats.each do |status, count|\n      output &lt;&lt; \"ragdoll_documents_total{status=\\\"#{status}\\\"} #{count}\"\n    end\n\n    # Embedding metrics\n    output &lt;&lt; \"ragdoll_embeddings_total #{Ragdoll::Embedding.count}\"\n    output &lt;&lt; \"ragdoll_embeddings_used_total #{Ragdoll::Embedding.where('usage_count &gt; 0').count}\"\n    output &lt;&lt; \"ragdoll_searches_total #{Ragdoll::Embedding.sum(:usage_count)}\"\n\n    # Processing metrics\n    recent_processing_times = Ragdoll::Document\n      .where('created_at &gt; ?', 24.hours.ago)\n      .where(status: 'processed')\n      .pluck(:created_at, :updated_at)\n      .map { |created, updated| (updated - created).to_i }\n\n    if recent_processing_times.any?\n      avg_time = recent_processing_times.sum / recent_processing_times.length\n      output &lt;&lt; \"ragdoll_avg_processing_time_seconds #{avg_time}\"\n    end\n\n    # Connection pool metrics\n    pool = ActiveRecord::Base.connection_pool\n    output &lt;&lt; \"ragdoll_connection_pool_size #{pool.stat[:size]}\"\n    output &lt;&lt; \"ragdoll_connection_pool_checked_out #{pool.stat[:checked_out]}\"\n    output &lt;&lt; \"ragdoll_connection_pool_checked_in #{pool.stat[:checked_in]}\"\n\n    # Content type distribution\n    content_types = Ragdoll::Document.group(:document_type).count\n    content_types.each do |type, count|\n      output &lt;&lt; \"ragdoll_documents_by_type{type=\\\"#{type}\\\"} #{count}\"\n    end\n\n    output.join(\"\\n\") + \"\\n\"\n  end\n\n  # Rack middleware for serving metrics\n  class Middleware\n    def initialize(app)\n      @app = app\n    end\n\n    def call(env)\n      if env['PATH_INFO'] == '/metrics' &amp;&amp; env['REQUEST_METHOD'] == 'GET'\n        metrics_response\n      else\n        @app.call(env)\n      end\n    end\n\n    private\n\n    def metrics_response\n      metrics = PrometheusExporter.metrics\n      [\n        200,\n        {\n          'Content-Type' =&gt; 'text/plain; version=0.0.4; charset=utf-8',\n          'Content-Length' =&gt; metrics.bytesize.to_s\n        },\n        [metrics]\n      ]\n    end\n  end\nend\n\n# Rails integration\n# In config/application.rb:\n# config.middleware.use PrometheusExporter::Middleware\n</code></pre>"},{"location":"operations/monitoring/#grafana-dashboard-templates","title":"Grafana Dashboard Templates","text":"<p>JSON dashboard configuration for Grafana:</p> <pre><code># Grafana dashboard generator\nclass GrafanaDashboard\n  def self.generate_dashboard_json\n    {\n      \"dashboard\" =&gt; {\n        \"id\" =&gt; nil,\n        \"title\" =&gt; \"Ragdoll Core Monitoring\",\n        \"tags\" =&gt; [\"ragdoll\", \"rag\", \"search\"],\n        \"timezone\" =&gt; \"browser\",\n        \"panels\" =&gt; [\n          {\n            \"id\" =&gt; 1,\n            \"title\" =&gt; \"Document Processing Status\",\n            \"type\" =&gt; \"stat\",\n            \"targets\" =&gt; [\n              {\n                \"expr\" =&gt; \"ragdoll_documents_total\",\n                \"legendFormat\" =&gt; \"{{ status }}\"\n              }\n            ],\n            \"gridPos\" =&gt; { \"h\" =&gt; 8, \"w\" =&gt; 12, \"x\" =&gt; 0, \"y\" =&gt; 0 }\n          },\n          {\n            \"id\" =&gt; 2,\n            \"title\" =&gt; \"Search Volume Over Time\",\n            \"type\" =&gt; \"graph\",\n            \"targets\" =&gt; [\n              {\n                \"expr\" =&gt; \"rate(ragdoll_searches_total[5m])\",\n                \"legendFormat\" =&gt; \"Searches per second\"\n              }\n            ],\n            \"gridPos\" =&gt; { \"h\" =&gt; 8, \"w\" =&gt; 12, \"x\" =&gt; 12, \"y\" =&gt; 0 }\n          },\n          {\n            \"id\" =&gt; 3,\n            \"title\" =&gt; \"Average Processing Time\",\n            \"type\" =&gt; \"singlestat\",\n            \"targets\" =&gt; [\n              {\n                \"expr\" =&gt; \"ragdoll_avg_processing_time_seconds\",\n                \"legendFormat\" =&gt; \"Seconds\"\n              }\n            ],\n            \"gridPos\" =&gt; { \"h\" =&gt; 4, \"w\" =&gt; 6, \"x\" =&gt; 0, \"y\" =&gt; 8 }\n          },\n          {\n            \"id\" =&gt; 4,\n            \"title\" =&gt; \"Connection Pool Usage\",\n            \"type\" =&gt; \"gauge\",\n            \"targets\" =&gt; [\n              {\n                \"expr\" =&gt; \"(ragdoll_connection_pool_checked_out / ragdoll_connection_pool_size) * 100\",\n                \"legendFormat\" =&gt; \"Pool Usage %\"\n              }\n            ],\n            \"gridPos\" =&gt; { \"h\" =&gt; 4, \"w\" =&gt; 6, \"x\" =&gt; 6, \"y\" =&gt; 8 }\n          }\n        ],\n        \"time\" =&gt; {\n          \"from\" =&gt; \"now-1h\",\n          \"to\" =&gt; \"now\"\n        },\n        \"refresh\" =&gt; \"30s\"\n      },\n      \"folderId\" =&gt; 0,\n      \"overwrite\" =&gt; true\n    }.to_json\n  }\nend\n</code></pre>"},{"location":"operations/monitoring/#new-relic-compatibility","title":"New Relic Compatibility","text":"<p>Integrate with New Relic APM and custom metrics:</p> <pre><code># New Relic custom metrics\nclass NewRelicIntegration\n  def self.record_custom_metrics\n    return unless defined?(NewRelic)\n\n    # Document processing metrics\n    doc_stats = Ragdoll::Document.group(:status).count\n    doc_stats.each do |status, count|\n      NewRelic::Agent.record_metric(\"Custom/Ragdoll/Documents/#{status}\", count)\n    end\n\n    # Search metrics\n    total_searches = Ragdoll::Embedding.sum(:usage_count)\n    NewRelic::Agent.record_metric(\"Custom/Ragdoll/Searches/Total\", total_searches)\n\n    # Processing performance\n    recent_times = calculate_recent_processing_times\n    if recent_times.any?\n      avg_time = recent_times.sum / recent_times.length\n      NewRelic::Agent.record_metric(\"Custom/Ragdoll/Processing/AverageTime\", avg_time)\n    end\n\n    # Embedding efficiency\n    used_embeddings = Ragdoll::Embedding.where('usage_count &gt; 0').count\n    total_embeddings = Ragdoll::Embedding.count\n    efficiency = total_embeddings &gt; 0 ? (used_embeddings.to_f / total_embeddings * 100) : 0\n    NewRelic::Agent.record_metric(\"Custom/Ragdoll/Embeddings/EfficiencyPercent\", efficiency)\n  end\n\n  # New Relic custom events\n  def self.track_search_event(query:, results_count:, processing_time:)\n    return unless defined?(NewRelic)\n\n    NewRelic::Agent.record_custom_event('RagdollSearch', {\n      query_length: query.length,\n      results_count: results_count,\n      processing_time_ms: processing_time,\n      timestamp: Time.current.to_i\n    })\n  end\n\n  def self.track_document_processing_event(document:, processing_time:, success:)\n    return unless defined?(NewRelic)\n\n    NewRelic::Agent.record_custom_event('RagdollDocumentProcessing', {\n      document_type: document.document_type,\n      document_size: document.content&amp;.length || 0,\n      processing_time_seconds: processing_time,\n      success: success,\n      embedding_count: document.total_embedding_count,\n      timestamp: Time.current.to_i\n    })\n  end\n\n  private\n\n  def self.calculate_recent_processing_times\n    Ragdoll::Document\n      .where('created_at &gt; ?', 24.hours.ago)\n      .where(status: 'processed')\n      .pluck(:created_at, :updated_at)\n      .map { |created, updated| (updated - created).to_i }\n  end\nend\n\n# Background job to send metrics\nclass MetricsReportingJob &lt; ActiveJob::Base\n  queue_as :default\n\n  def perform\n    NewRelicIntegration.record_custom_metrics\n  end\nend\n\n# Schedule regular metrics reporting\n# In Rails initializer or similar:\n# MetricsReportingJob.set(wait: 5.minutes).perform_later\n</code></pre>"},{"location":"operations/monitoring/#custom-monitoring-solutions","title":"Custom Monitoring Solutions","text":"<p>Framework for building custom monitoring integrations:</p> <pre><code>class CustomMonitoringAdapter\n  attr_reader :config, :client\n\n  def initialize(config = {})\n    @config = config\n    @client = initialize_client\n  end\n\n  def send_metrics(metrics)\n    case config[:type]\n    when :datadog\n      send_datadog_metrics(metrics)\n    when :statsd\n      send_statsd_metrics(metrics)\n    when :influxdb\n      send_influxdb_metrics(metrics)\n    when :custom_api\n      send_custom_api_metrics(metrics)\n    else\n      Rails.logger.info \"Custom metrics: #{metrics.to_json}\" if defined?(Rails)\n    end\n  end\n\n  def collect_all_metrics\n    {\n      timestamp: Time.current.to_i,\n      documents: document_metrics,\n      embeddings: embedding_metrics,\n      performance: performance_metrics,\n      system: system_metrics\n    }\n  end\n\n  private\n\n  def document_metrics\n    {\n      total: Ragdoll::Document.count,\n      by_status: Ragdoll::Document.group(:status).count,\n      by_type: Ragdoll::Document.group(:document_type).count,\n      processing_queue_length: Ragdoll::Document.where(status: 'pending').count\n    }\n  end\n\n  def embedding_metrics\n    {\n      total: Ragdoll::Embedding.count,\n      total_searches: Ragdoll::Embedding.sum(:usage_count),\n      used_embeddings: Ragdoll::Embedding.where('usage_count &gt; 0').count,\n      recent_searches: Ragdoll::Embedding\n        .where('returned_at &gt; ?', 1.hour.ago)\n        .sum(:usage_count)\n    }\n  end\n\n  def performance_metrics\n    recent_times = Ragdoll::Document\n      .where('created_at &gt; ?', 24.hours.ago)\n      .where(status: 'processed')\n      .pluck(:created_at, :updated_at)\n      .map { |created, updated| (updated - created).to_i }\n\n    {\n      avg_processing_time: recent_times.any? ? recent_times.sum / recent_times.length : 0,\n      processed_documents_24h: recent_times.length,\n      error_rate: calculate_error_rate,\n      embedding_efficiency: calculate_embedding_efficiency\n    }\n  end\n\n  def system_metrics\n    pool = ActiveRecord::Base.connection_pool\n    {\n      connection_pool_size: pool.stat[:size],\n      connection_pool_used: pool.stat[:checked_out],\n      connection_pool_available: pool.stat[:size] - pool.stat[:checked_out]\n    }\n  end\n\n  def send_datadog_metrics(metrics)\n    # Datadog StatsD format\n    metrics.each do |key, value|\n      if value.is_a?(Hash)\n        value.each do |subkey, subvalue|\n          send_metric(\"ragdoll.#{key}.#{subkey}\", subvalue, type: :gauge)\n        end\n      else\n        send_metric(\"ragdoll.#{key}\", value, type: :gauge)\n      end\n    end\n  end\n\n  def send_statsd_metrics(metrics)\n    # StatsD protocol implementation\n    # Similar to Datadog but with different format\n  end\n\n  def send_custom_api_metrics(metrics)\n    # Custom HTTP API endpoint\n    require 'net/http'\n    require 'json'\n\n    uri = URI(config[:endpoint])\n    http = Net::HTTP.new(uri.host, uri.port)\n    http.use_ssl = uri.scheme == 'https'\n\n    request = Net::HTTP::Post.new(uri)\n    request['Content-Type'] = 'application/json'\n    request['Authorization'] = \"Bearer #{config[:api_key]}\" if config[:api_key]\n    request.body = metrics.to_json\n\n    response = http.request(request)\n    Rails.logger.info \"Metrics sent: #{response.code}\" if defined?(Rails)\n  end\nend\n\n# Usage\nmonitoring = CustomMonitoringAdapter.new(\n  type: :datadog,\n  api_key: ENV['DATADOG_API_KEY'],\n  endpoint: 'https://api.datadoghq.com/api/v1/series'\n)\n\n# Collect and send metrics\nmetrics = monitoring.collect_all_metrics\nmonitoring.send_metrics(metrics)\n</code></pre>"},{"location":"operations/monitoring/#troubleshooting-guides","title":"Troubleshooting Guides","text":"<p>Comprehensive troubleshooting workflows for common Ragdoll issues, focusing on PostgreSQL and pgvector performance optimization.</p>"},{"location":"operations/monitoring/#common-performance-issues","title":"Common Performance Issues","text":""},{"location":"operations/monitoring/#slow-search-performance","title":"Slow Search Performance","text":"<p>Symptoms: - Search queries taking &gt; 2 seconds - High CPU usage during searches - Connection pool exhaustion</p> <p>Diagnostic Commands: <pre><code># Check pgvector index usage\nActiveRecord::Base.connection.execute(\"\n  EXPLAIN ANALYZE\n  SELECT * FROM ragdoll_embeddings\n  ORDER BY embedding_vector &lt;=&gt; '[0.1,0.2,...]'\n  LIMIT 10;\n\")\n\n# Check index statistics\nActiveRecord::Base.connection.execute(\"\n  SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read\n  FROM pg_stat_user_indexes\n  WHERE tablename = 'ragdoll_embeddings';\n\")\n\n# Monitor connection pool\npool_stats = ActiveRecord::Base.connection_pool.stat\nputs \"Pool usage: #{pool_stats[:checked_out]}/#{pool_stats[:size]}\"\n</code></pre></p> <p>Resolution Steps: 1. Optimize pgvector index: Ensure IVFFlat index is properly configured 2. Increase connection pool: Adjust <code>pool</code> setting in database.yml 3. Enable connection pooling: Use pgbouncer for high-load scenarios 4. Query optimization: Review embedding search filters and limits</p> <pre><code># Optimize search queries\nclass SearchOptimizer\n  def self.optimize_embedding_search\n    # Use index hints for better performance\n    Ragdoll::Embedding.connection.execute(\"\n      SET enable_seqscan = OFF;\n      SET work_mem = '256MB';\n    \")\n  end\n\n  def self.batch_similar_searches(query_embeddings, batch_size: 100)\n    # Process multiple searches in batches to reduce overhead\n    query_embeddings.each_slice(batch_size) do |batch|\n      # Process batch of searches\n      yield batch\n    end\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#high-memory-usage","title":"High Memory Usage","text":"<p>Symptoms: - Ruby process memory growth - PostgreSQL memory pressure - Frequent garbage collection</p> <p>Diagnostic Procedures: <pre><code># Memory usage analysis\ndef analyze_memory_usage\n  {\n    ruby_process_mb: `ps -o rss= -p #{Process.pid}`.to_i / 1024,\n    gc_stats: GC.stat,\n    object_counts: ObjectSpace.count_objects,\n    connection_pool: ActiveRecord::Base.connection_pool.stat\n  }\nend\n\n# PostgreSQL memory analysis\nActiveRecord::Base.connection.execute(\"\n  SELECT\n    setting AS shared_buffers_mb,\n    pg_size_pretty(pg_database_size(current_database())) AS db_size\n  FROM pg_settings\n  WHERE name = 'shared_buffers';\n\")\n</code></pre></p> <p>Resolution Strategies: 1. Optimize embeddings loading: Use <code>select</code> to load only needed columns 2. Implement connection pooling: Use pgbouncer or similar 3. Tune PostgreSQL memory: Adjust shared_buffers and work_mem 4. Regular cleanup: Implement data retention policies</p>"},{"location":"operations/monitoring/#document-processing-failures","title":"Document Processing Failures","text":"<p>Symptoms: - Documents stuck in 'processing' status - High error rates - Background job failures</p> <p>Diagnostic Commands: <pre><code># Check processing status distribution\nprocessing_status = Ragdoll::Document.group(:status).count\nputs \"Status distribution: #{processing_status}\"\n\n# Find stuck documents\nstuck_docs = Ragdoll::Document\n  .where(status: 'processing')\n  .where('updated_at &lt; ?', 1.hour.ago)\n\nputs \"Stuck documents: #{stuck_docs.count}\"\n\n# Check recent errors\nerror_docs = Ragdoll::Document\n  .where(status: 'error')\n  .where('updated_at &gt; ?', 24.hours.ago)\n  .includes(:contents)\n</code></pre></p> <p>Resolution Steps: 1. Reset stuck documents: Change status back to 'pending' 2. Check job queue: Ensure ActiveJob backend is running 3. Review error logs: Identify common failure patterns 4. Validate file access: Ensure file permissions and availability</p> <pre><code># Recovery procedures\nclass DocumentRecovery\n  def self.reset_stuck_documents\n    stuck_docs = Ragdoll::Document\n      .where(status: 'processing')\n      .where('updated_at &lt; ?', 1.hour.ago)\n\n    stuck_docs.update_all(status: 'pending')\n    puts \"Reset #{stuck_docs.count} stuck documents\"\n  end\n\n  def self.retry_failed_documents\n    failed_docs = Ragdoll::Document\n      .where(status: 'error')\n      .where('updated_at &gt; ?', 24.hours.ago)\n\n    failed_docs.each do |doc|\n      begin\n        doc.update!(status: 'pending')\n        # Trigger reprocessing\n        Ragdoll::ExtractTextJob.perform_later(doc.id)\n      rescue =&gt; e\n        puts \"Failed to retry document #{doc.id}: #{e.message}\"\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#diagnostic-procedures","title":"Diagnostic Procedures","text":""},{"location":"operations/monitoring/#system-health-check","title":"System Health Check","text":"<pre><code>class SystemHealthCheck\n  def self.run_full_diagnostic\n    results = {\n      timestamp: Time.current.iso8601,\n      database: check_database_health,\n      models: check_model_integrity,\n      performance: check_performance_metrics,\n      storage: check_storage_health,\n      jobs: check_job_health\n    }\n\n    generate_health_report(results)\n  end\n\n  private\n\n  def self.check_database_health\n    {\n      connection_status: ActiveRecord::Base.connected?,\n      pool_status: ActiveRecord::Base.connection_pool.stat,\n      table_sizes: get_table_sizes,\n      index_usage: get_index_usage_stats,\n      slow_queries: get_slow_queries\n    }\n  end\n\n  def self.check_model_integrity\n    {\n      total_documents: Ragdoll::Document.count,\n      orphaned_embeddings: find_orphaned_embeddings,\n      missing_content: find_documents_without_content,\n      invalid_embeddings: find_invalid_embeddings\n    }\n  end\n\n  def self.check_performance_metrics\n    recent_searches = Ragdoll::Embedding\n      .where('returned_at &gt; ?', 1.hour.ago)\n\n    {\n      searches_last_hour: recent_searches.sum(:usage_count),\n      avg_search_time: calculate_avg_search_time,\n      cache_hit_rate: calculate_cache_hit_rate,\n      processing_backlog: Ragdoll::Document.where(status: 'pending').count\n    }\n  end\n\n  def self.get_table_sizes\n    ActiveRecord::Base.connection.execute(\"\n      SELECT\n        tablename,\n        pg_size_pretty(pg_total_relation_size('ragdoll_'||tablename)) as size,\n        pg_total_relation_size('ragdoll_'||tablename) as bytes\n      FROM pg_tables\n      WHERE tablename LIKE 'ragdoll_%'\n      ORDER BY pg_total_relation_size('ragdoll_'||tablename) DESC;\n    \").to_a\n  end\n\n  def self.find_orphaned_embeddings\n    # Find embeddings without valid embeddable references\n    Ragdoll::Embedding.left_joins(:embeddable)\n      .where(ragdoll_contents: { id: nil })\n      .count\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#performance-profiling","title":"Performance Profiling","text":"<pre><code>class PerformanceProfiler\n  def self.profile_search_operation(query, iterations: 100)\n    require 'benchmark'\n\n    results = []\n    embedding_service = Ragdoll::EmbeddingService.new\n    search_engine = Ragdoll::SearchEngine.new(embedding_service)\n\n    # Warm up\n    3.times { search_engine.search_documents(query, limit: 10) }\n\n    # Profile multiple iterations\n    iterations.times do |i|\n      start_time = Time.current\n\n      begin\n        search_results = search_engine.search_documents(query, limit: 10)\n        end_time = Time.current\n\n        results &lt;&lt; {\n          iteration: i + 1,\n          duration_ms: ((end_time - start_time) * 1000).round(2),\n          results_count: search_results.length,\n          success: true\n        }\n      rescue =&gt; e\n        results &lt;&lt; {\n          iteration: i + 1,\n          error: e.message,\n          success: false\n        }\n      end\n    end\n\n    analyze_profile_results(results)\n  end\n\n  private\n\n  def self.analyze_profile_results(results)\n    successful_results = results.select { |r| r[:success] }\n\n    return { error: \"No successful iterations\" } if successful_results.empty?\n\n    durations = successful_results.map { |r| r[:duration_ms] }\n\n    {\n      total_iterations: results.length,\n      successful_iterations: successful_results.length,\n      success_rate: (successful_results.length.to_f / results.length * 100).round(2),\n      performance: {\n        min_ms: durations.min,\n        max_ms: durations.max,\n        avg_ms: (durations.sum / durations.length).round(2),\n        median_ms: durations.sort[durations.length / 2],\n        std_dev_ms: calculate_std_dev(durations).round(2)\n      },\n      percentiles: {\n        p50: percentile(durations, 50),\n        p90: percentile(durations, 90),\n        p95: percentile(durations, 95),\n        p99: percentile(durations, 99)\n      }\n    }\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#prevention-techniques","title":"Prevention Techniques","text":""},{"location":"operations/monitoring/#proactive-monitoring-setup","title":"Proactive Monitoring Setup","text":"<pre><code>class PreventiveMaintenance\n  def self.setup_monitoring_jobs\n    # Schedule regular health checks\n    HealthCheckJob.set(cron: '*/15 * * * *').perform_later # Every 15 minutes\n\n    # Schedule daily cleanup\n    CleanupJob.set(cron: '0 2 * * *').perform_later # Daily at 2 AM\n\n    # Schedule weekly analytics\n    WeeklyReportJob.set(cron: '0 8 * * 1').perform_later # Monday at 8 AM\n  end\n\n  def self.optimize_database_settings\n    # PostgreSQL optimization for pgvector\n    settings = {\n      'shared_buffers' =&gt; '256MB',\n      'work_mem' =&gt; '64MB',\n      'maintenance_work_mem' =&gt; '256MB',\n      'effective_cache_size' =&gt; '1GB',\n      'random_page_cost' =&gt; '1.1' # Optimized for SSD\n    }\n\n    settings.each do |setting, value|\n      ActiveRecord::Base.connection.execute(\n        \"ALTER SYSTEM SET #{setting} = '#{value}';\"\n      )\n    end\n\n    # Reload configuration\n    ActiveRecord::Base.connection.execute(\"SELECT pg_reload_conf();\")\n  end\n\n  def self.setup_automated_backups\n    # Database backup strategy\n    backup_script = &lt;&lt;~SCRIPT\n      #!/bin/bash\n      # Automated Ragdoll database backup\n\n      DB_NAME=\"ragdoll_production\"\n      BACKUP_DIR=\"/var/backups/ragdoll\"\n      DATE=$(date +%Y%m%d_%H%M%S)\n\n      # Create backup directory\n      mkdir -p $BACKUP_DIR\n\n      # Full database backup\n      pg_dump $DB_NAME | gzip &gt; $BACKUP_DIR/ragdoll_$DATE.sql.gz\n\n      # Cleanup old backups (keep 30 days)\n      find $BACKUP_DIR -name \"ragdoll_*.sql.gz\" -mtime +30 -delete\n\n      # Verify backup integrity\n      if [ $? -eq 0 ]; then\n        echo \"Backup completed successfully: ragdoll_$DATE.sql.gz\"\n      else\n        echo \"Backup failed!\" | mail -s \"Ragdoll Backup Failure\" admin@example.com\n      fi\n    SCRIPT\n\n    puts \"Add this script to crontab for daily backups:\"\n    puts \"0 3 * * * /path/to/ragdoll_backup.sh\"\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#configuration-best-practices","title":"Configuration Best Practices","text":"<pre><code>class ConfigurationValidator\n  def self.validate_production_config\n    issues = []\n    config = Ragdoll.config\n\n    # Database configuration validation\n    if config.database_config[:pool] &lt; 20\n      issues &lt;&lt; \"Connection pool size (#{config.database_config[:pool]}) may be too small for production\"\n    end\n\n    # Search configuration validation\n    if config.search[:max_results] &gt; 100\n      issues &lt;&lt; \"max_results (#{config.search[:max_results]}) may impact performance\"\n    end\n\n    if config.search[:similarity_threshold] &lt; 0.5\n      issues &lt;&lt; \"similarity_threshold (#{config.search[:similarity_threshold]}) may return too many irrelevant results\"\n    end\n\n    # Analytics configuration\n    unless config.search[:enable_analytics]\n      issues &lt;&lt; \"Analytics disabled - monitoring capabilities will be limited\"\n    end\n\n    # Memory settings validation\n    if config.chunking[:text][:max_tokens] &gt; 2000\n      issues &lt;&lt; \"text chunk size (#{config.chunking[:text][:max_tokens]}) may cause memory issues\"\n    end\n\n    display_validation_results(issues)\n  end\n\n  private\n\n  def self.display_validation_results(issues)\n    if issues.empty?\n      puts \"\u2705 Configuration validation passed\"\n    else\n      puts \"\u26a0\ufe0f Configuration issues found:\"\n      issues.each_with_index do |issue, index|\n        puts \"#{index + 1}. #{issue}\"\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"operations/monitoring/#summary","title":"Summary","text":"<p>Ragdoll's monitoring and analytics system provides comprehensive insights into system performance, usage patterns, and health metrics through PostgreSQL-native features and ActiveRecord integration. The built-in analytics track embedding usage for intelligent caching, while the flexible alerting system ensures proactive issue detection.</p> <p>Key monitoring capabilities include: - Usage Analytics: Search patterns, content popularity, embedding efficiency - Performance Metrics: Processing times, error rates, system resource usage - Health Monitoring: Database status, connection pools, job queue health - External Integrations: Prometheus, Grafana, New Relic, and custom solutions - Proactive Alerting: Threshold-based alerts with escalation policies - Troubleshooting Tools: Diagnostic procedures and automated recovery</p> <p>All monitoring data leverages PostgreSQL's built-in statistics and pgvector optimization for minimal performance impact while providing maximum visibility into system behavior.</p> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"operations/troubleshooting/","title":"Troubleshooting","text":""},{"location":"operations/troubleshooting/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>This guide provides comprehensive troubleshooting information for Ragdoll, covering installation, runtime, configuration, and performance issues with practical solutions.</p>"},{"location":"operations/troubleshooting/#quick-diagnostic-commands","title":"Quick Diagnostic Commands","text":"<pre><code># Check system health\nclient = Ragdoll::Core.client\nputs \"System healthy: #{client.healthy?}\"\nputs \"Database connected: #{Ragdoll::Core::Database.connected?}\"\nputs client.stats\n</code></pre>"},{"location":"operations/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"operations/troubleshooting/#dependency-problems","title":"Dependency Problems","text":""},{"location":"operations/troubleshooting/#ruby-version-compatibility","title":"Ruby Version Compatibility","text":"<p>Issue: Ragdoll requires Ruby 3.0+ <pre><code>ERROR: ragdoll requires Ruby version &gt;= 3.0.0\n</code></pre></p> <p>Solution: <pre><code># Check Ruby version\nruby --version\n\n# Install Ruby 3.2+ with rbenv\nrbenv install 3.2.0\nrbenv global 3.2.0\n\n# Or with RVM\nrvm install 3.2.0\nrvm use 3.2.0 --default\n</code></pre></p>"},{"location":"operations/troubleshooting/#gem-installation-failures","title":"Gem Installation Failures","text":"<p>Issue: Native extension compilation fails <pre><code>ERROR: Failed to build gem native extension.\n</code></pre></p> <p>Solution: <pre><code># macOS - Install Xcode command line tools\nxcode-select --install\n\n# Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install build-essential libpq-dev\n\n# CentOS/RHEL\nsudo yum groupinstall \"Development Tools\"\nsudo yum install postgresql-devel\n</code></pre></p>"},{"location":"operations/troubleshooting/#system-library-requirements","title":"System Library Requirements","text":"<p>Issue: Missing system dependencies <pre><code>ERROR: pg requires libpq-dev\n</code></pre></p> <p>Solution: <pre><code># macOS with Homebrew\nbrew install postgresql libpq\n\n# Ubuntu/Debian\nsudo apt-get install libpq-dev postgresql-client\n\n# Add to environment\nexport PATH=\"/usr/local/opt/libpq/bin:$PATH\"\n</code></pre></p>"},{"location":"operations/troubleshooting/#database-setup-issues","title":"Database Setup Issues","text":""},{"location":"operations/troubleshooting/#postgresql-connection-problems","title":"PostgreSQL Connection Problems","text":"<p>Issue: Cannot connect to PostgreSQL <pre><code>ERROR: connection to server on socket \"/tmp/.s.PGSQL.5432\" failed\n</code></pre></p> <p>Diagnosis: <pre><code># Test database connection\nbegin\n  ActiveRecord::Base.establish_connection(\n    adapter: 'postgresql',\n    database: 'ragdoll_development',\n    username: 'ragdoll',\n    password: 'your_password',\n    host: 'localhost',\n    port: 5432\n  )\n  puts \"Connection successful\"\nrescue =&gt; e\n  puts \"Connection failed: #{e.message}\"\nend\n</code></pre></p> <p>Solutions: <pre><code># Start PostgreSQL service\n# macOS with Homebrew\nbrew services start postgresql\n\n# Ubuntu/Debian\nsudo systemctl start postgresql\nsudo systemctl enable postgresql\n\n# Create database and user\nsudo -u postgres psql\nCREATE DATABASE ragdoll_development;\nCREATE USER ragdoll WITH PASSWORD 'secure_password';\nGRANT ALL PRIVILEGES ON DATABASE ragdoll_development TO ragdoll;\n\\q\n</code></pre></p>"},{"location":"operations/troubleshooting/#pgvector-extension-installation","title":"pgvector Extension Installation","text":"<p>Issue: pgvector extension not available <pre><code>ERROR: extension \"vector\" is not available\n</code></pre></p> <p>Solution: <pre><code># Install pgvector\n# macOS with Homebrew\nbrew install pgvector\n\n# Ubuntu/Debian\nsudo apt install postgresql-16-pgvector\n\n# Manual installation\ngit clone https://github.com/pgvector/pgvector.git\ncd pgvector\nmake\nsudo make install\n\n# Enable extension in database\nsudo -u postgres psql -d ragdoll_development\nCREATE EXTENSION vector;\n\\q\n</code></pre></p>"},{"location":"operations/troubleshooting/#migration-failures","title":"Migration Failures","text":"<p>Issue: Database migrations fail <pre><code>ERROR: relation \"ragdoll_documents\" already exists\n</code></pre></p> <p>Diagnosis: <pre><code># Check migration status\nRagdoll::Core::Database.setup(auto_migrate: false)\nActiveRecord::Base.connection.execute(\n  \"SELECT version FROM schema_migrations ORDER BY version\"\n)\n</code></pre></p> <p>Solution: <pre><code># Reset database (DESTRUCTIVE - development only)\nRagdoll::Core::Database.reset!\n\n# Or manually fix migration state\nActiveRecord::Base.connection.execute(\n  \"DELETE FROM schema_migrations WHERE version = '004'\"\n)\nRagdoll::Core::Database.migrate!\n</code></pre></p>"},{"location":"operations/troubleshooting/#runtime-issues","title":"Runtime Issues","text":""},{"location":"operations/troubleshooting/#document-processing-errors","title":"Document Processing Errors","text":""},{"location":"operations/troubleshooting/#file-format-parsing-failures","title":"File Format Parsing Failures","text":"<p>Issue: Unsupported or corrupted file formats <pre><code>ERROR: Unable to parse PDF file: Invalid PDF structure\n</code></pre></p> <p>Diagnosis: <pre><code># Test document parsing\nbegin\n  result = Ragdoll::DocumentProcessor.parse('/path/to/document.pdf')\n  puts \"Parsing successful: #{result[:document_type]}\"\n  puts \"Content length: #{result[:content]&amp;.length || 0}\"\nrescue =&gt; e\n  puts \"Parsing failed: #{e.message}\"\n  puts \"Error class: #{e.class}\"\nend\n</code></pre></p> <p>Solutions: <pre><code># Check file integrity\nfile_path = '/path/to/document.pdf'\nif File.exist?(file_path)\n  puts \"File size: #{File.size(file_path)} bytes\"\n  puts \"File readable: #{File.readable?(file_path)}\"\n\n  # Try alternative parsing approach\n  begin\n    content = File.read(file_path, encoding: 'binary')\n    puts \"File header: #{content[0..10].bytes.map(&amp;:to_s).join(' ')}\"\n  rescue =&gt; e\n    puts \"Cannot read file: #{e.message}\"\n  end\nelse\n  puts \"File does not exist: #{file_path}\"\nend\n</code></pre></p>"},{"location":"operations/troubleshooting/#content-extraction-problems","title":"Content Extraction Problems","text":"<p>Issue: Empty or garbled content extraction <pre><code>WARNING: Extracted content is empty or contains only whitespace\n</code></pre></p> <p>Diagnosis: <pre><code># Debug content extraction\nclass DebugDocumentProcessor &lt; Ragdoll::DocumentProcessor\n  def self.parse_with_debug(file_path)\n    puts \"Processing: #{file_path}\"\n    puts \"File size: #{File.size(file_path)}\"\n\n    result = parse(file_path)\n\n    puts \"Document type: #{result[:document_type]}\"\n    puts \"Content length: #{result[:content]&amp;.length || 0}\"\n    puts \"Content preview: #{result[:content]&amp;.[](0..200)}\"\n    puts \"Metadata keys: #{result[:metadata]&amp;.keys}\"\n\n    result\n  end\nend\n</code></pre></p>"},{"location":"operations/troubleshooting/#search-and-embedding-issues","title":"Search and Embedding Issues","text":""},{"location":"operations/troubleshooting/#vector-generation-failures","title":"Vector Generation Failures","text":"<p>Issue: Embedding service errors <pre><code>ERROR: Failed to generate embedding: OpenAI API key not configured\n</code></pre></p> <p>Diagnosis: <pre><code># Test embedding generation\nservice = Ragdoll::EmbeddingService.new\nbegin\n  embedding = service.generate_embedding(\"test text\")\n  puts \"Embedding generated: #{embedding&amp;.length || 0} dimensions\"\nrescue =&gt; e\n  puts \"Embedding failed: #{e.message}\"\n  puts \"Configuration check:\"\n  puts \"OpenAI API Key: #{ENV['OPENAI_API_KEY'] ? 'Set' : 'Missing'}\"\nend\n</code></pre></p> <p>Solutions: <pre><code># Check and fix configuration\nRagdoll::Core.configure do |config|\n  # Verify API keys are set\n  config.ruby_llm_config[:openai][:api_key] = ENV['OPENAI_API_KEY']\n\n  if ENV['OPENAI_API_KEY'].nil? || ENV['OPENAI_API_KEY'].empty?\n    puts \"ERROR: OPENAI_API_KEY environment variable not set\"\n    puts \"Set it with: export OPENAI_API_KEY=your_api_key\"\n  end\nend\n</code></pre></p>"},{"location":"operations/troubleshooting/#similarity-search-problems","title":"Similarity Search Problems","text":"<p>Issue: Search returns no results despite having documents <pre><code>INFO: Search query returned 0 results\n</code></pre></p> <p>Diagnosis: <pre><code># Debug search process\nclient = Ragdoll::Core.client\nquery = \"machine learning\"\n\nputs \"Documents in database: #{Ragdoll::Document.count}\"\nputs \"Embeddings in database: #{Ragdoll::Embedding.count}\"\n\n# Test embedding generation\nembedding_service = Ragdoll::EmbeddingService.new\nquery_embedding = embedding_service.generate_embedding(query)\nputs \"Query embedding dimensions: #{query_embedding&amp;.length || 0}\"\n\n# Test direct embedding search\nresults = Ragdoll::Embedding.search_similar(\n  query_embedding,\n  limit: 10,\n  threshold: 0.1  # Lower threshold for debugging\n)\nputs \"Direct search results: #{results.length}\"\n</code></pre></p>"},{"location":"operations/troubleshooting/#database-problems","title":"Database Problems","text":""},{"location":"operations/troubleshooting/#connection-timeouts","title":"Connection Timeouts","text":"<p>Issue: Database operations timing out <pre><code>ERROR: connection timed out\n</code></pre></p> <p>Solutions: <pre><code># Increase connection timeout\nRagdoll::Core.configure do |config|\n  config.database_config.merge!({\n    pool: 10,\n    timeout: 10000,  # 10 seconds\n    checkout_timeout: 5,\n    reaping_frequency: 10\n  })\nend\n\n# Monitor connection pool\npool = ActiveRecord::Base.connection_pool\nputs \"Pool size: #{pool.size}\"\nputs \"Checked out: #{pool.checked_out_connections.size}\"\nputs \"Available: #{pool.available_connection_count}\"\n</code></pre></p>"},{"location":"operations/troubleshooting/#index-performance-issues","title":"Index Performance Issues","text":"<p>Issue: Slow vector similarity searches <pre><code>WARNING: Vector search took 5.2 seconds\n</code></pre></p> <p>Diagnosis: <pre><code>-- Check index usage\nEXPLAIN (ANALYZE, BUFFERS) \nSELECT * FROM ragdoll_embeddings \nORDER BY embedding_vector &lt;-&gt; '[0.1,0.2,0.3,...]'::vector \nLIMIT 10;\n\n-- Check index statistics\nSELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch\nFROM pg_stat_user_indexes \nWHERE tablename = 'ragdoll_embeddings';\n</code></pre></p> <p>Solutions: <pre><code>-- Recreate vector index with better parameters\nDROP INDEX IF EXISTS ragdoll_embeddings_vector_idx;\nCREATE INDEX CONCURRENTLY ragdoll_embeddings_vector_idx \nON ragdoll_embeddings \nUSING ivfflat (embedding_vector vector_cosine_ops) \nWITH (lists = 100);\n\n-- Or use HNSW for better performance\nCREATE INDEX CONCURRENTLY ragdoll_embeddings_hnsw_idx \nON ragdoll_embeddings \nUSING hnsw (embedding_vector vector_cosine_ops) \nWITH (m = 16, ef_construction = 64);\n</code></pre></p>"},{"location":"operations/troubleshooting/#configuration-issues","title":"Configuration Issues","text":""},{"location":"operations/troubleshooting/#llm-provider-configuration","title":"LLM Provider Configuration","text":""},{"location":"operations/troubleshooting/#api-key-validation","title":"API Key Validation","text":"<p>Issue: Invalid or expired API keys <pre><code>ERROR: Authentication failed for OpenAI API\n</code></pre></p> <p>Validation Script: <pre><code># Test API key validity\nclass APIKeyValidator\n  def self.validate_openai_key(api_key)\n    require 'faraday'\n\n    conn = Faraday.new(url: 'https://api.openai.com')\n    response = conn.get('/v1/models') do |req|\n      req.headers['Authorization'] = \"Bearer #{api_key}\"\n    end\n\n    case response.status\n    when 200\n      puts \"\u2713 OpenAI API key is valid\"\n      true\n    when 401\n      puts \"\u2717 OpenAI API key is invalid or expired\"\n      false\n    else\n      puts \"? Unexpected response: #{response.status}\"\n      false\n    end\n  end\nend\n\nAPIKeyValidator.validate_openai_key(ENV['OPENAI_API_KEY'])\n</code></pre></p>"},{"location":"operations/troubleshooting/#provider-specific-issues","title":"Provider-Specific Issues","text":"<p>Issue: Rate limiting errors <pre><code>ERROR: Rate limit exceeded for OpenAI API\n</code></pre></p> <p>Solutions: <pre><code># Implement exponential backoff\nclass RateLimitHandler\n  def self.with_retry(max_retries: 3, base_delay: 1)\n    retries = 0\n\n    begin\n      yield\n    rescue =&gt; e\n      if e.message.include?('rate limit') &amp;&amp; retries &lt; max_retries\n        delay = base_delay * (2 ** retries)\n        puts \"Rate limited, retrying in #{delay} seconds...\"\n        sleep(delay)\n        retries += 1\n        retry\n      else\n        raise e\n      end\n    end\n  end\nend\n\n# Use with embedding generation\nRateLimitHandler.with_retry do\n  service.generate_embedding(text)\nend\n</code></pre></p>"},{"location":"operations/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"operations/troubleshooting/#slow-search-performance","title":"Slow Search Performance","text":"<p>Diagnosis Tools: <pre><code># Performance profiler\nclass SearchProfiler\n  def self.profile_search(query, iterations: 10)\n    times = []\n\n    iterations.times do\n      start_time = Time.current\n      client = Ragdoll::Core.client\n      results = client.search(query: query)\n      end_time = Time.current\n\n      duration = (end_time - start_time) * 1000\n      times &lt;&lt; duration\n\n      puts \"Search #{iterations}: #{duration.round(2)}ms (#{results[:total_results]} results)\"\n    end\n\n    avg_time = times.sum / times.length\n    puts \"Average search time: #{avg_time.round(2)}ms\"\n    puts \"Min: #{times.min.round(2)}ms, Max: #{times.max.round(2)}ms\"\n  end\nend\n\nSearchProfiler.profile_search(\"machine learning\")\n</code></pre></p>"},{"location":"operations/troubleshooting/#background-job-issues","title":"Background Job Issues","text":""},{"location":"operations/troubleshooting/#queue-backlog-problems","title":"Queue Backlog Problems","text":"<p>Diagnosis: <pre><code># Monitor job queues\nif defined?(Sidekiq)\n  puts \"Queue sizes:\"\n  Sidekiq::Queue.all.each do |queue|\n    puts \"  #{queue.name}: #{queue.size} jobs\"\n  end\n\n  puts \"\\nFailed jobs: #{Sidekiq::RetrySet.new.size}\"\n  puts \"Dead jobs: #{Sidekiq::DeadSet.new.size}\"\nelse\n  puts \"Using inline job processing\"\nend\n</code></pre></p> <p>Solutions: <pre><code># Scale workers dynamically\nclass JobQueueMonitor\n  def self.monitor_and_scale\n    queue = Sidekiq::Queue.new('embeddings')\n\n    if queue.size &gt; 100\n      puts \"High queue size detected: #{queue.size}\"\n      # Scale up workers (implementation depends on deployment)\n      scale_workers_up\n    elsif queue.size &lt; 10\n      puts \"Low queue size: #{queue.size}\"\n      # Scale down workers\n      scale_workers_down\n    end\n  end\nend\n</code></pre></p>"},{"location":"operations/troubleshooting/#debugging-tools","title":"Debugging Tools","text":""},{"location":"operations/troubleshooting/#logging-configuration","title":"Logging Configuration","text":"<pre><code># Enhanced logging setup\nRagdoll::Core.configure do |config|\n  config.logging_config[:level] = :debug\n\n  # Custom logger with detailed formatting\n  logger = Logger.new(STDOUT)\n  logger.formatter = proc do |severity, datetime, progname, msg|\n    \"[#{datetime.strftime('%Y-%m-%d %H:%M:%S')}] #{severity.ljust(5)} #{progname}: #{msg}\\n\"\n  end\n\n  config.database_config[:logger] = logger\nend\n</code></pre>"},{"location":"operations/troubleshooting/#development-console","title":"Development Console","text":"<pre><code># Debug console helpers\nclass RagdollDebug\n  def self.system_info\n    {\n      ruby_version: RUBY_VERSION,\n      rails_version: defined?(Rails) ? Rails.version : 'N/A',\n      database_adapter: ActiveRecord::Base.connection.adapter_name,\n      total_documents: Ragdoll::Document.count,\n      total_embeddings: Ragdoll::Embedding.count,\n      last_document: Ragdoll::Document.last&amp;.title,\n      system_healthy: Ragdoll::Core.client.healthy?\n    }\n  end\n\n  def self.test_pipeline(text = \"Hello world\")\n    puts \"Testing complete pipeline...\"\n\n    # Test embedding generation\n    service = Ragdoll::EmbeddingService.new\n    embedding = service.generate_embedding(text)\n    puts \"\u2713 Embedding generated: #{embedding.length} dimensions\"\n\n    # Test search\n    client = Ragdoll::Core.client\n    results = client.search(query: text)\n    puts \"\u2713 Search completed: #{results[:total_results]} results\"\n\n    true\n  rescue =&gt; e\n    puts \"\u2717 Pipeline test failed: #{e.message}\"\n    false\n  end\nend\n\n# Usage in console\nRagdollDebug.system_info\nRagdollDebug.test_pipeline\n</code></pre>"},{"location":"operations/troubleshooting/#error-reference","title":"Error Reference","text":""},{"location":"operations/troubleshooting/#common-error-messages","title":"Common Error Messages","text":"Error Cause Solution <code>Ragdoll::Core::EmbeddingError</code> LLM API issues Check API keys and network <code>Ragdoll::Core::DocumentError</code> File processing failure Verify file format and permissions <code>Ragdoll::Core::SearchError</code> Search operation failure Check database indexes <code>Ragdoll::Core::ConfigurationError</code> Invalid configuration Validate configuration settings <code>ActiveRecord::ConnectionNotEstablished</code> Database connection failure Check PostgreSQL service and credentials"},{"location":"operations/troubleshooting/#support-resources","title":"Support Resources","text":""},{"location":"operations/troubleshooting/#self-diagnosis-checklist","title":"Self-Diagnosis Checklist","text":"<ul> <li> Ruby version 3.0+ installed</li> <li> PostgreSQL service running</li> <li> pgvector extension installed</li> <li> Database user has proper permissions</li> <li> API keys configured in environment</li> <li> Required system libraries installed</li> <li> Network connectivity to LLM providers</li> <li> Sufficient disk space for embeddings</li> <li> Memory allocation adequate for workload</li> <li> Background job system functioning</li> </ul>"},{"location":"operations/troubleshooting/#getting-help","title":"Getting Help","text":"<ol> <li>Documentation: Check the Architecture Guide and Configuration Guide</li> <li>Logs: Enable debug logging and examine error messages</li> <li>System Health: Run the diagnostic commands provided above  </li> <li>Community: Search existing issues and discussions</li> <li>Issue Reporting: Provide system info, error logs, and reproduction steps</li> </ol>"},{"location":"operations/troubleshooting/#emergency-recovery","title":"Emergency Recovery","text":"<pre><code># Complete system reset (DEVELOPMENT ONLY)\nRagdoll::Core::Database.reset!\nRagdoll::Core.configure do |config|\n  # Restore minimal configuration\n  config.database_config[:auto_migrate] = true\nend\n\n# Verify system health\nclient = Ragdoll::Core.client\nputs \"System recovered: #{client.healthy?}\"\n</code></pre> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"rails/","title":"Ragdoll Rails Integration","text":"<p>The <code>ragdoll-rails</code> gem provides seamless integration between the Ragdoll RAG system and Ruby on Rails applications. This Rails engine brings powerful document processing, semantic search, and LLM capabilities directly into your Rails app.</p>"},{"location":"rails/#overview","title":"Overview","text":"<p>Ragdoll Rails is a complete Rails engine that provides:</p> <ul> <li>ActiveRecord Models for documents, embeddings, and content</li> <li>RESTful Controllers for managing documents and search</li> <li>View Helpers for search interfaces and document display</li> <li>Background Jobs for document processing</li> <li>Database Migrations for Ragdoll schema</li> <li>Rails Generators for quick setup</li> <li>Configuration Management for LLM providers and settings</li> </ul>"},{"location":"rails/#key-features","title":"Key Features","text":""},{"location":"rails/#rails-engine-architecture","title":"\ud83c\udfd7\ufe0f Rails Engine Architecture","text":"<ul> <li>Drop-in integration with existing Rails apps</li> <li>Follows Rails conventions and patterns</li> <li>Mountable engine with isolated namespace</li> </ul>"},{"location":"rails/#document-management","title":"\ud83d\udcc4 Document Management","text":"<ul> <li>ActiveRecord models for documents and content</li> <li>File upload handling with Active Storage</li> <li>Automatic content extraction and processing</li> <li>Metadata management and search</li> </ul>"},{"location":"rails/#search-capabilities","title":"\ud83d\udd0d Search Capabilities","text":"<ul> <li>Semantic search with vector embeddings</li> <li>Hybrid search combining semantic and keyword matching</li> <li>Search result ranking and filtering</li> <li>Faceted search with metadata</li> </ul>"},{"location":"rails/#view-components","title":"\ud83c\udfa8 View Components","text":"<ul> <li>Pre-built search forms and result displays</li> <li>Document viewers for various file types</li> <li>Upload interfaces with progress tracking</li> <li>Responsive, accessible UI components</li> </ul>"},{"location":"rails/#background-processing","title":"\u2699\ufe0f Background Processing","text":"<ul> <li>Sidekiq/Resque integration for document processing</li> <li>Chunked content processing for large documents</li> <li>Embedding generation and indexing</li> <li>Retry mechanisms and error handling</li> </ul>"},{"location":"rails/#quick-start","title":"Quick Start","text":"<p>Add to your Gemfile: <pre><code>gem 'ragdoll-rails'\n</code></pre></p> <p>Run the installer: <pre><code>rails generate ragdoll:install\n</code></pre></p> <p>Mount the engine: <pre><code># config/routes.rb\nmount Ragdoll::Engine =&gt; '/ragdoll'\n</code></pre></p> <p>Start processing documents: <pre><code># Upload and process a document\ndocument = Ragdoll::Document.create!(\n  title: \"My Document\",\n  file: uploaded_file,\n  metadata: { category: \"research\" }\n)\n\n# Search across documents\nresults = Ragdoll::Document.search(\"machine learning\", limit: 10)\n</code></pre></p>"},{"location":"rails/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    subgraph \"Rails Application\"\n        A[Your Controllers] --&gt; B[Ragdoll Models]\n        A --&gt; C[Ragdoll Helpers]\n        D[Your Views] --&gt; C\n    end\n\n    subgraph \"Ragdoll Engine\"\n        B --&gt; E[Document Model]\n        B --&gt; F[Content Model]\n        B --&gt; G[Embedding Model]\n        H[Document Controller] --&gt; B\n        I[Search Controller] --&gt; B\n    end\n\n    subgraph \"Background Jobs\"\n        J[Processing Job] --&gt; K[Content Extraction]\n        J --&gt; L[Embedding Generation]\n        J --&gt; M[Indexing]\n    end\n\n    subgraph \"External Services\"\n        N[LLM Providers]\n        O[Vector Database]\n        P[File Storage]\n    end\n\n    B --&gt; J\n    L --&gt; N\n    G --&gt; O\n    E --&gt; P</code></pre>"},{"location":"rails/#integration-patterns","title":"Integration Patterns","text":""},{"location":"rails/#1-full-integration","title":"1. Full Integration","text":"<p>Mount the complete engine with all features: <pre><code>mount Ragdoll::Engine =&gt; '/documents'\n</code></pre></p>"},{"location":"rails/#2-api-only","title":"2. API Only","text":"<p>Use models and controllers without views: <pre><code># Custom controller inheriting from Ragdoll\nclass DocumentsController &lt; Ragdoll::DocumentsController\n  # Your customizations\nend\n</code></pre></p>"},{"location":"rails/#3-models-only","title":"3. Models Only","text":"<p>Use just the ActiveRecord models: <pre><code>class Article &lt; ApplicationRecord\n  include Ragdoll::Searchable\n\n  ragdoll_document title: :title, content: :body\nend\n</code></pre></p>"},{"location":"rails/#use-cases","title":"Use Cases","text":""},{"location":"rails/#knowledge-bases","title":"\ud83d\udcda Knowledge Bases","text":"<ul> <li>Internal documentation systems</li> <li>Customer support knowledge bases</li> <li>Product documentation sites</li> <li>FAQ systems with semantic search</li> </ul>"},{"location":"rails/#research-platforms","title":"\ud83d\udd2c Research Platforms","text":"<ul> <li>Academic paper repositories</li> <li>Research data management</li> <li>Literature review tools</li> <li>Citation and reference systems</li> </ul>"},{"location":"rails/#content-management","title":"\ud83d\udcca Content Management","text":"<ul> <li>Document management systems</li> <li>Media asset libraries</li> <li>Content discovery platforms</li> <li>Digital asset management</li> </ul>"},{"location":"rails/#enterprise-applications","title":"\ud83c\udfe2 Enterprise Applications","text":"<ul> <li>Corporate knowledge management</li> <li>Policy and procedure systems</li> <li>Training material platforms</li> <li>Compliance documentation</li> </ul>"},{"location":"rails/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Set up Ragdoll in your Rails app</li> <li>Configuration - Configure models, views, and background jobs</li> <li>Models Reference - Learn about the ActiveRecord models</li> <li>Controllers - Understand the provided controllers</li> <li>Views &amp; Helpers - Use the view components and helpers</li> <li>Generators - Use Rails generators for quick setup</li> <li>Examples - See real-world implementation patterns</li> </ul>"},{"location":"rails/configuration/","title":"Rails Configuration","text":"<p>Comprehensive configuration guide for ragdoll-rails, covering all aspects from basic setup to advanced customization.</p>"},{"location":"rails/configuration/#configuration-structure","title":"Configuration Structure","text":"<p>Ragdoll Rails uses a centralized configuration system accessible through <code>Ragdoll.configuration</code>:</p> <pre><code># config/initializers/ragdoll.rb\nRagdoll.configure do |config|\n  # Your configuration here\nend\n</code></pre>"},{"location":"rails/configuration/#core-configuration","title":"Core Configuration","text":""},{"location":"rails/configuration/#llm-provider-settings","title":"LLM Provider Settings","text":"<pre><code>Ragdoll.configure do |config|\n  # Provider selection\n  config.llm_provider = :openai  # :openai, :anthropic, :ollama, :azure\n\n  # API credentials\n  config.openai_api_key = Rails.application.credentials.openai_api_key\n  config.anthropic_api_key = Rails.application.credentials.anthropic_api_key\n\n  # Base URLs (optional for custom endpoints)\n  config.openai_base_url = \"https://api.openai.com/v1\"\n  config.anthropic_base_url = \"https://api.anthropic.com\"\n\n  # Model specifications\n  config.default_chat_model = \"gpt-4\"\n  config.default_embedding_model = \"text-embedding-3-small\"\n  config.embedding_dimensions = 1536\n\n  # Request settings\n  config.llm_timeout = 30.seconds\n  config.max_retries = 3\n  config.retry_delay = 1.second\nend\n</code></pre>"},{"location":"rails/configuration/#database-configuration","title":"Database Configuration","text":"<pre><code>Ragdoll.configure do |config|\n  # Vector database settings\n  config.vector_dimensions = 1536\n  config.similarity_function = :cosine  # :cosine, :euclidean, :dot_product\n\n  # Connection settings\n  config.database_pool_size = 10\n  config.database_timeout = 30.seconds\n\n  # Indexing options\n  config.create_vector_indexes = true\n  config.index_type = :ivfflat  # :ivfflat, :hnsw\n  config.index_lists = 100      # For IVFFlat indexes\nend\n</code></pre>"},{"location":"rails/configuration/#document-processing","title":"Document Processing","text":""},{"location":"rails/configuration/#file-handling","title":"File Handling","text":"<pre><code>Ragdoll.configure do |config|\n  # Supported file types\n  config.allowed_file_types = %w[\n    .pdf .doc .docx .txt .md .html .rtf\n    .jpg .jpeg .png .gif .webp\n    .mp3 .wav .m4a .flac\n  ]\n\n  # File size limits\n  config.max_file_size = 50.megabytes\n  config.max_image_size = 10.megabytes\n  config.max_audio_size = 100.megabytes\n\n  # Storage settings\n  config.storage_service = :local  # or :s3, :gcs, :azure\n  config.preserve_original_files = true\n  config.generate_thumbnails = true\n  config.thumbnail_sizes = {\n    small: '150x150',\n    medium: '300x300',\n    large: '600x600'\n  }\nend\n</code></pre>"},{"location":"rails/configuration/#content-processing","title":"Content Processing","text":"<pre><code>Ragdoll.configure do |config|\n  # Text processing\n  config.chunk_size = 1000\n  config.chunk_overlap = 200\n  config.min_chunk_size = 100\n  config.max_chunk_size = 2000\n\n  # Content extraction\n  config.extract_metadata = true\n  config.extract_images = true\n  config.extract_audio = false\n  config.perform_ocr = true\n\n  # Content cleaning\n  config.sanitize_content = true\n  config.remove_empty_chunks = true\n  config.normalize_whitespace = true\n\n  # Language detection\n  config.detect_language = true\n  config.default_language = 'en'\nend\n</code></pre>"},{"location":"rails/configuration/#search-configuration","title":"Search Configuration","text":""},{"location":"rails/configuration/#search-behavior","title":"Search Behavior","text":"<pre><code>Ragdoll.configure do |config|\n  # Default search settings\n  config.default_search_limit = 10\n  config.max_search_limit = 100\n  config.similarity_threshold = 0.7\n  config.min_similarity_threshold = 0.1\n\n  # Search types\n  config.enable_semantic_search = true\n  config.enable_keyword_search = true\n  config.enable_hybrid_search = true\n  config.hybrid_search_weight = 0.7  # Weight for semantic vs keyword\n\n  # Result ranking\n  config.boost_recent_documents = true\n  config.recency_boost_factor = 0.1\n  config.boost_popular_documents = true\n  config.popularity_boost_factor = 0.05\nend\n</code></pre>"},{"location":"rails/configuration/#search-features","title":"Search Features","text":"<pre><code>Ragdoll.configure do |config|\n  # Advanced features\n  config.enable_faceted_search = true\n  config.enable_autocomplete = true\n  config.enable_spell_check = true\n  config.enable_query_expansion = true\n\n  # Caching\n  config.cache_search_results = true\n  config.search_cache_duration = 1.hour\n  config.cache_embeddings = true\n  config.embedding_cache_duration = 24.hours\nend\n</code></pre>"},{"location":"rails/configuration/#background-jobs","title":"Background Jobs","text":""},{"location":"rails/configuration/#job-processing","title":"Job Processing","text":"<pre><code>Ragdoll.configure do |config|\n  # Job adapter\n  config.job_adapter = :sidekiq  # :sidekiq, :resque, :delayed_job, :inline\n\n  # Queue configuration\n  config.processing_queue = :ragdoll\n  config.embedding_queue = :ragdoll_embeddings\n  config.indexing_queue = :ragdoll_indexing\n  config.cleanup_queue = :ragdoll_cleanup\n\n  # Processing options\n  config.async_processing = true\n  config.batch_processing = true\n  config.batch_size = 10\n\n  # Retry configuration\n  config.job_max_retries = 3\n  config.job_retry_delay = 30.seconds\n  config.job_timeout = 5.minutes\nend\n</code></pre>"},{"location":"rails/configuration/#performance-tuning","title":"Performance Tuning","text":"<pre><code>Ragdoll.configure do |config|\n  # Concurrency\n  config.max_concurrent_jobs = 5\n  config.embedding_batch_size = 50\n  config.indexing_batch_size = 100\n\n  # Memory management\n  config.memory_limit_per_job = 512.megabytes\n  config.cleanup_temp_files = true\n  config.gc_after_processing = true\nend\n</code></pre>"},{"location":"rails/configuration/#security-access-control","title":"Security &amp; Access Control","text":""},{"location":"rails/configuration/#authentication-integration","title":"Authentication Integration","text":"<pre><code>Ragdoll.configure do |config|\n  # User methods\n  config.current_user_method = :current_user\n  config.authenticate_user_method = :authenticate_user!\n\n  # User model\n  config.user_class = 'User'\n  config.user_foreign_key = :user_id\n\n  # Session handling\n  config.require_authentication = true\n  config.allow_anonymous_search = false\nend\n</code></pre>"},{"location":"rails/configuration/#authorization","title":"Authorization","text":"<pre><code>Ragdoll.configure do |config|\n  # Document access control\n  config.authorize_document_access = -&gt;(document, user) {\n    return true if user&amp;.admin?\n    return false unless user\n\n    case document.visibility\n    when 'public' then true\n    when 'private' then document.user == user\n    when 'team' then document.user.team == user.team\n    else false\n    end\n  }\n\n  # Search authorization\n  config.authorize_search = -&gt;(user) {\n    user&amp;.active? &amp;&amp; (user.premium? || user.search_credits &gt; 0)\n  }\n\n  # Upload permissions\n  config.authorize_upload = -&gt;(user) {\n    user&amp;.active? &amp;&amp; user.can_upload_documents?\n  }\nend\n</code></pre>"},{"location":"rails/configuration/#content-security","title":"Content Security","text":"<pre><code>Ragdoll.configure do |config|\n  # Content filtering\n  config.content_filters = [\n    :remove_pii,        # Remove personally identifiable information\n    :sanitize_html,     # Clean HTML content\n    :check_virus,       # Virus scanning\n    :validate_content   # Content validation\n  ]\n\n  # File validation\n  config.validate_file_signatures = true\n  config.scan_for_malware = true\n  config.quarantine_suspicious_files = true\n\n  # Data privacy\n  config.anonymize_user_data = false\n  config.encrypt_sensitive_content = true\n  config.audit_access = true\nend\n</code></pre>"},{"location":"rails/configuration/#ui-views-configuration","title":"UI &amp; Views Configuration","text":""},{"location":"rails/configuration/#view-settings","title":"View Settings","text":"<pre><code>Ragdoll.configure do |config|\n  # Theme and styling\n  config.ui_theme = :default  # :default, :dark, :minimal\n  config.custom_css_path = 'ragdoll/custom'\n  config.custom_js_path = 'ragdoll/custom'\n\n  # Layout options\n  config.layout = 'application'\n  config.use_turbo = true\n  config.use_stimulus = true\n\n  # Search interface\n  config.search_placeholder = \"Search documents...\"\n  config.show_search_filters = true\n  config.show_result_thumbnails = true\n  config.results_per_page = 20\n\n  # Document viewer\n  config.enable_document_preview = true\n  config.preview_max_size = 5.megabytes\n  config.syntax_highlighting = true\nend\n</code></pre>"},{"location":"rails/configuration/#internationalization","title":"Internationalization","text":"<pre><code>Ragdoll.configure do |config|\n  # Language settings\n  config.default_locale = :en\n  config.available_locales = [:en, :es, :fr, :de, :ja]\n  config.fallback_locale = :en\n\n  # UI text customization\n  config.custom_translations = {\n    en: {\n      ragdoll: {\n        search: {\n          placeholder: \"What are you looking for?\",\n          no_results: \"No documents found matching your search.\"\n        }\n      }\n    }\n  }\nend\n</code></pre>"},{"location":"rails/configuration/#monitoring-logging","title":"Monitoring &amp; Logging","text":""},{"location":"rails/configuration/#logging-configuration","title":"Logging Configuration","text":"<pre><code>Ragdoll.configure do |config|\n  # Logger settings\n  config.logger = Rails.logger\n  config.log_level = Rails.env.production? ? :info : :debug\n  config.log_format = :json  # :json, :text\n\n  # Log targets\n  config.log_search_queries = true\n  config.log_processing_times = true\n  config.log_llm_requests = false  # Disable in production for security\n  config.log_user_actions = true\n\n  # Performance logging\n  config.slow_query_threshold = 1.second\n  config.log_slow_queries = true\n  config.profile_processing = Rails.env.development?\nend\n</code></pre>"},{"location":"rails/configuration/#metrics-analytics","title":"Metrics &amp; Analytics","text":"<pre><code>Ragdoll.configure do |config|\n  # Metrics collection\n  config.collect_metrics = true\n  config.metrics_backend = :prometheus  # :prometheus, :statsd, :custom\n\n  # Analytics\n  config.track_search_analytics = true\n  config.track_user_behavior = true\n  config.analytics_retention = 90.days\n\n  # Health checks\n  config.enable_health_checks = true\n  config.health_check_path = '/health/ragdoll'\n  config.include_detailed_health = Rails.env.development?\nend\n</code></pre>"},{"location":"rails/configuration/#environment-specific-configuration","title":"Environment-Specific Configuration","text":""},{"location":"rails/configuration/#development-environment","title":"Development Environment","text":"<pre><code># config/environments/development.rb\nRails.application.configure do\n  config.after_initialize do\n    Ragdoll.configure do |ragdoll|\n      ragdoll.log_level = :debug\n      ragdoll.async_processing = false\n      ragdoll.profile_processing = true\n      ragdoll.enable_detailed_health = true\n      ragdoll.cache_search_results = false\n      ragdoll.llm_timeout = 60.seconds  # Longer timeout for debugging\n    end\n  end\nend\n</code></pre>"},{"location":"rails/configuration/#production-environment","title":"Production Environment","text":"<pre><code># config/environments/production.rb\nRails.application.configure do\n  config.after_initialize do\n    Ragdoll.configure do |ragdoll|\n      ragdoll.log_level = :warn\n      ragdoll.async_processing = true\n      ragdoll.cache_search_results = true\n      ragdoll.collect_metrics = true\n      ragdoll.audit_access = true\n      ragdoll.encrypt_sensitive_content = true\n      ragdoll.scan_for_malware = true\n    end\n  end\nend\n</code></pre>"},{"location":"rails/configuration/#test-environment","title":"Test Environment","text":"<pre><code># config/environments/test.rb\nRails.application.configure do\n  config.after_initialize do\n    Ragdoll.configure do |ragdoll|\n      ragdoll.job_adapter = :inline\n      ragdoll.async_processing = false\n      ragdoll.llm_provider = :mock\n      ragdoll.cache_search_results = false\n      ragdoll.log_level = :error\n    end\n  end\nend\n</code></pre>"},{"location":"rails/configuration/#custom-providers","title":"Custom Providers","text":""},{"location":"rails/configuration/#custom-llm-provider","title":"Custom LLM Provider","text":"<pre><code>Ragdoll.configure do |config|\n  config.custom_llm_providers[:my_provider] = {\n    class: 'MyCustomLLMProvider',\n    config: {\n      api_key: ENV['MY_LLM_API_KEY'],\n      base_url: 'https://api.myprovider.com'\n    }\n  }\nend\n\n# app/services/my_custom_llm_provider.rb\nclass MyCustomLLMProvider &lt; Ragdoll::LLMProviders::Base\n  def generate_embedding(text)\n    # Custom implementation\n  end\n\n  def generate_text(prompt)\n    # Custom implementation\n  end\nend\n</code></pre>"},{"location":"rails/configuration/#custom-storage-provider","title":"Custom Storage Provider","text":"<pre><code>Ragdoll.configure do |config|\n  config.custom_storage_providers[:my_storage] = {\n    class: 'MyCustomStorageProvider',\n    config: {\n      endpoint: ENV['MY_STORAGE_ENDPOINT'],\n      credentials: ENV['MY_STORAGE_CREDENTIALS']\n    }\n  }\nend\n</code></pre>"},{"location":"rails/configuration/#configuration-validation","title":"Configuration Validation","text":""},{"location":"rails/configuration/#runtime-validation","title":"Runtime Validation","text":"<pre><code>Ragdoll.configure do |config|\n  # Enable validation\n  config.validate_on_startup = true\n  config.strict_validation = Rails.env.production?\n\n  # Custom validators\n  config.custom_validators = [\n    -&gt;(config) { \n      raise \"Invalid chunk size\" if config.chunk_size &gt; config.max_chunk_size\n    }\n  ]\nend\n</code></pre>"},{"location":"rails/configuration/#configuration-testing","title":"Configuration Testing","text":"<pre><code># spec/support/ragdoll_config_spec.rb\nRSpec.describe \"Ragdoll Configuration\" do\n  it \"has valid configuration\" do\n    expect { Ragdoll.configuration.validate! }.not_to raise_error\n  end\n\n  it \"can connect to LLM provider\" do\n    expect(Ragdoll::LLMService.new.health_check).to be_truthy\n  end\nend\n</code></pre>"},{"location":"rails/controllers/","title":"Rails Controllers","text":"<p>The ragdoll-rails gem provides a comprehensive set of controllers that handle document management, search functionality, and administrative tasks. These controllers follow Rails conventions and can be easily customized or extended.</p>"},{"location":"rails/controllers/#controller-overview","title":"Controller Overview","text":"<p>The Ragdoll Rails controllers are organized into logical groups:</p> <ul> <li>Document Management: Upload, view, edit, and delete documents</li> <li>Search: Perform searches and manage search interfaces  </li> <li>Content: Handle content chunks and embeddings</li> <li>Admin: Administrative functions and monitoring</li> <li>API: RESTful API endpoints</li> </ul>"},{"location":"rails/controllers/#core-controllers","title":"Core Controllers","text":""},{"location":"rails/controllers/#ragdolldocumentscontroller","title":"Ragdoll::DocumentsController","text":"<p>The main controller for document operations.</p>"},{"location":"rails/controllers/#actions","title":"Actions","text":"<pre><code>class Ragdoll::DocumentsController &lt; Ragdoll::ApplicationController\n  before_action :authenticate_user!, except: [:show] if Ragdoll.configuration.require_authentication\n  before_action :set_document, only: [:show, :edit, :update, :destroy, :download, :reprocess]\n  before_action :authorize_document!, only: [:show, :edit, :update, :destroy]\n\n  # GET /documents\n  def index\n    @documents = documents_scope\n      .includes(:user, :contents)\n      .page(params[:page])\n      .per(params[:per_page] || 20)\n\n    @documents = @documents.where(status: params[:status]) if params[:status].present?\n    @documents = @documents.where(content_type: params[:content_type]) if params[:content_type].present?\n\n    respond_to do |format|\n      format.html\n      format.json { render json: @documents }\n      format.csv { send_data documents_csv, filename: \"documents-#{Date.current}.csv\" }\n    end\n  end\n\n  # GET /documents/1\n  def show\n    @content_preview = @document.content_preview(1000)\n    @related_documents = @document.find_similar(limit: 5)\n\n    respond_to do |format|\n      format.html\n      format.json { render json: @document, include: [:contents, :embeddings] }\n      format.pdf { render_pdf }\n    end\n  end\n\n  # GET /documents/new\n  def new\n    @document = documents_scope.build\n  end\n\n  # GET /documents/1/edit\n  def edit\n  end\n\n  # POST /documents\n  def create\n    @document = documents_scope.build(document_params)\n    @document.user = current_user if respond_to?(:current_user)\n\n    respond_to do |format|\n      if @document.save\n        enqueue_processing if should_auto_process?\n        format.html { redirect_to @document, notice: 'Document was successfully created.' }\n        format.json { render json: @document, status: :created }\n      else\n        format.html { render :new }\n        format.json { render json: @document.errors, status: :unprocessable_entity }\n      end\n    end\n  end\n\n  # PATCH/PUT /documents/1\n  def update\n    respond_to do |format|\n      if @document.update(document_params)\n        enqueue_reprocessing if file_changed?\n        format.html { redirect_to @document, notice: 'Document was successfully updated.' }\n        format.json { render json: @document }\n      else\n        format.html { render :edit }\n        format.json { render json: @document.errors, status: :unprocessable_entity }\n      end\n    end\n  end\n\n  # DELETE /documents/1\n  def destroy\n    @document.destroy\n    respond_to do |format|\n      format.html { redirect_to documents_url, notice: 'Document was successfully deleted.' }\n      format.json { head :no_content }\n    end\n  end\n\n  # GET /documents/1/download\n  def download\n    authorize_download!\n    redirect_to rails_blob_path(@document.file, disposition: \"attachment\")\n  end\n\n  # POST /documents/1/reprocess\n  def reprocess\n    authorize_reprocess!\n    Ragdoll::ProcessDocumentJob.perform_later(@document)\n    redirect_to @document, notice: 'Document reprocessing has been queued.'\n  end\n\n  private\n\n  def documents_scope\n    if respond_to?(:current_user) &amp;&amp; current_user\n      Ragdoll::Document.accessible_by(current_user)\n    else\n      Ragdoll::Document.public_documents\n    end\n  end\n\n  def set_document\n    @document = documents_scope.find(params[:id])\n  end\n\n  def document_params\n    params.require(:document).permit(:title, :description, :file, :metadata, images: [])\n  end\n\n  def authorize_document!\n    return if Ragdoll.configuration.authorize_document_access.call(@document, current_user)\n\n    respond_to do |format|\n      format.html { redirect_to root_path, alert: 'Access denied.' }\n      format.json { render json: { error: 'Access denied' }, status: :forbidden }\n    end\n  end\n\n  def should_auto_process?\n    Ragdoll.configuration.auto_process_documents &amp;&amp; @document.file.attached?\n  end\n\n  def enqueue_processing\n    Ragdoll::ProcessDocumentJob.perform_later(@document)\n  end\n\n  def enqueue_reprocessing\n    return unless @document.file.attached?\n\n    @document.update!(status: 'pending')\n    Ragdoll::ProcessDocumentJob.perform_later(@document)\n  end\n\n  def file_changed?\n    @document.saved_change_to_attribute?(:file) || @document.file.attachment.changed?\n  end\n\n  def render_pdf\n    # Custom PDF rendering logic\n    pdf = Ragdoll::PdfRenderer.new(@document).render\n    send_data pdf, filename: \"#{@document.title}.pdf\", type: 'application/pdf'\n  end\n\n  def documents_csv\n    Ragdoll::CsvExporter.new(@documents).export\n  end\nend\n</code></pre>"},{"location":"rails/controllers/#custom-actions","title":"Custom Actions","text":"<pre><code>class Ragdoll::DocumentsController &lt; Ragdoll::ApplicationController\n  # GET /documents/bulk_upload\n  def bulk_upload\n    @upload_session = Ragdoll::UploadSession.new\n  end\n\n  # POST /documents/bulk_create\n  def bulk_create\n    @upload_session = Ragdoll::UploadSession.new(bulk_upload_params)\n\n    if @upload_session.valid?\n      @upload_session.process_files\n      redirect_to documents_path, notice: \"#{@upload_session.file_count} files uploaded successfully.\"\n    else\n      render :bulk_upload\n    end\n  end\n\n  # GET /documents/stats\n  def stats\n    @stats = {\n      total_documents: documents_scope.count,\n      processed_documents: documents_scope.processed.count,\n      total_size: documents_scope.sum(:file_size),\n      recent_uploads: documents_scope.recent(7).count,\n      top_content_types: documents_scope.group(:content_type).count.sort_by(&amp;:last).reverse.first(5)\n    }\n\n    respond_to do |format|\n      format.html\n      format.json { render json: @stats }\n    end\n  end\n\n  private\n\n  def bulk_upload_params\n    params.require(:upload_session).permit(files: [], metadata: {})\n  end\nend\n</code></pre>"},{"location":"rails/controllers/#ragdollsearchcontroller","title":"Ragdoll::SearchController","text":"<p>Handles search functionality and interfaces.</p> <pre><code>class Ragdoll::SearchController &lt; Ragdoll::ApplicationController\n  before_action :authenticate_user!, if: :authentication_required?\n  before_action :authorize_search!, if: :authorization_required?\n  before_action :set_search_params, only: [:index, :suggestions, :facets]\n\n  # GET /search\n  def index\n    @search_service = Ragdoll::SearchService.new(@query, search_options)\n    @results = @search_service.call\n    @facets = @search_service.facets if params[:include_facets]\n    @suggestions = @search_service.suggestions if @results.empty?\n\n    # Track search analytics\n    track_search_event if Ragdoll.configuration.track_search_analytics\n\n    respond_to do |format|\n      format.html\n      format.json { render json: search_response }\n      format.turbo_stream { render :index }\n    end\n  end\n\n  # GET /search/suggestions\n  def suggestions\n    @suggestions = Ragdoll::SuggestionService.new(@query).call\n\n    respond_to do |format|\n      format.json { render json: @suggestions }\n    end\n  end\n\n  # GET /search/facets\n  def facets\n    @facets = Ragdoll::FacetService.new(@query, facet_options).call\n\n    respond_to do |format|\n      format.json { render json: @facets }\n      format.html { render partial: 'facets', locals: { facets: @facets } }\n    end\n  end\n\n  # POST /search/save\n  def save\n    @saved_search = current_user.saved_searches.build(saved_search_params)\n\n    if @saved_search.save\n      render json: @saved_search, status: :created\n    else\n      render json: @saved_search.errors, status: :unprocessable_entity\n    end\n  end\n\n  private\n\n  def set_search_params\n    @query = params[:q] || params[:query]\n    @filters = params[:filters] || {}\n    @sort = params[:sort] || 'relevance'\n    @page = params[:page] || 1\n    @per_page = [params[:per_page].to_i, 50].min.positive? || 10\n  end\n\n  def search_options\n    {\n      filters: @filters,\n      sort: @sort,\n      page: @page,\n      per_page: @per_page,\n      include_facets: params[:include_facets],\n      search_type: params[:search_type] || 'hybrid',\n      threshold: params[:threshold]&amp;.to_f || Ragdoll.configuration.similarity_threshold,\n      user: current_user\n    }\n  end\n\n  def facet_options\n    {\n      facet_fields: params[:facet_fields] || ['content_type', 'user', 'created_at'],\n      facet_limit: params[:facet_limit] || 10\n    }\n  end\n\n  def search_response\n    {\n      query: @query,\n      results: @results.map { |result| serialize_search_result(result) },\n      total_count: @results.total_count,\n      page: @page,\n      per_page: @per_page,\n      facets: @facets,\n      suggestions: @suggestions,\n      search_time: @search_service.search_time\n    }\n  end\n\n  def serialize_search_result(result)\n    {\n      id: result.id,\n      title: result.title,\n      description: result.description,\n      content_preview: result.content_preview,\n      similarity_score: result.similarity_score,\n      url: document_path(result),\n      metadata: result.metadata,\n      user: result.user&amp;.name,\n      created_at: result.created_at\n    }\n  end\n\n  def track_search_event\n    Ragdoll::SearchAnalytics.track(\n      query: @query,\n      user: current_user,\n      results_count: @results.size,\n      search_type: params[:search_type],\n      response_time: @search_service.search_time\n    )\n  end\n\n  def authentication_required?\n    !Ragdoll.configuration.allow_anonymous_search\n  end\n\n  def authorization_required?\n    Ragdoll.configuration.authorize_search.present?\n  end\n\n  def authorize_search!\n    return if Ragdoll.configuration.authorize_search.call(current_user)\n\n    respond_to do |format|\n      format.html { redirect_to root_path, alert: 'Search access denied.' }\n      format.json { render json: { error: 'Search access denied' }, status: :forbidden }\n    end\n  end\n\n  def saved_search_params\n    params.require(:saved_search).permit(:name, :query, :filters, :is_public)\n  end\nend\n</code></pre>"},{"location":"rails/controllers/#ragdolladminadmincontroller","title":"Ragdoll::Admin::AdminController","text":"<p>Base controller for administrative functions.</p> <pre><code>class Ragdoll::Admin::AdminController &lt; Ragdoll::ApplicationController\n  before_action :authenticate_admin!\n  layout 'ragdoll/admin'\n\n  protected\n\n  def authenticate_admin!\n    return if current_user&amp;.admin?\n\n    respond_to do |format|\n      format.html { redirect_to root_path, alert: 'Admin access required.' }\n      format.json { render json: { error: 'Admin access required' }, status: :forbidden }\n    end\n  end\nend\n\nclass Ragdoll::Admin::DashboardController &lt; Ragdoll::Admin::AdminController\n  # GET /admin/dashboard\n  def index\n    @stats = {\n      total_documents: Ragdoll::Document.count,\n      processed_documents: Ragdoll::Document.processed.count,\n      pending_documents: Ragdoll::Document.pending.count,\n      failed_documents: Ragdoll::Document.failed.count,\n      total_storage: Ragdoll::Document.sum(:file_size),\n      active_users: active_users_count,\n      recent_searches: recent_searches_count,\n      system_health: system_health_check\n    }\n\n    @recent_uploads = Ragdoll::Document.recent(7).limit(10)\n    @processing_queue_size = processing_queue_size\n    @error_documents = Ragdoll::Document.failed.limit(5)\n  end\n\n  # GET /admin/system_info\n  def system_info\n    @system_info = {\n      ragdoll_version: Ragdoll::VERSION,\n      rails_version: Rails.version,\n      ruby_version: RUBY_VERSION,\n      database_version: database_version,\n      redis_version: redis_version,\n      background_job_adapter: Ragdoll.configuration.job_adapter,\n      llm_provider: Ragdoll.configuration.llm_provider,\n      storage_service: Ragdoll.configuration.storage_service\n    }\n\n    respond_to do |format|\n      format.html\n      format.json { render json: @system_info }\n    end\n  end\n\n  private\n\n  def active_users_count\n    # Implementation depends on your user model\n    User.joins(:documents).where(documents: { created_at: 1.week.ago.. }).distinct.count\n  end\n\n  def recent_searches_count\n    # Implementation depends on search analytics\n    Ragdoll::SearchAnalytics.where(created_at: 1.day.ago..).count\n  end\n\n  def system_health_check\n    {\n      database: database_healthy?,\n      llm_provider: llm_provider_healthy?,\n      background_jobs: background_jobs_healthy?,\n      storage: storage_healthy?\n    }\n  end\n\n  def processing_queue_size\n    case Ragdoll.configuration.job_adapter\n    when :sidekiq\n      Sidekiq::Queue.new(Ragdoll.configuration.processing_queue).size\n    when :resque\n      Resque.size(Ragdoll.configuration.processing_queue)\n    else\n      0\n    end\n  end\n\n  def database_healthy?\n    ActiveRecord::Base.connection.active?\n  rescue\n    false\n  end\n\n  def llm_provider_healthy?\n    Ragdoll::LLMService.new.health_check\n  rescue\n    false\n  end\n\n  def background_jobs_healthy?\n    case Ragdoll.configuration.job_adapter\n    when :sidekiq\n      Sidekiq.redis(&amp;:ping) == 'PONG'\n    else\n      true\n    end\n  rescue\n    false\n  end\n\n  def storage_healthy?\n    ActiveStorage::Blob.service.exist?('health_check')\n  rescue\n    false\n  end\nend\n</code></pre>"},{"location":"rails/controllers/#api-controllers","title":"API Controllers","text":""},{"location":"rails/controllers/#ragdollapiv1basecontroller","title":"Ragdoll::Api::V1::BaseController","text":"<p>Base API controller with common functionality.</p> <pre><code>class Ragdoll::Api::V1::BaseController &lt; ActionController::API\n  include ActionController::HttpAuthentication::Token::ControllerMethods\n\n  before_action :authenticate_api_user!\n  before_action :set_default_format\n\n  rescue_from ActiveRecord::RecordNotFound, with: :render_not_found\n  rescue_from ActiveRecord::RecordInvalid, with: :render_unprocessable_entity\n  rescue_from Ragdoll::AuthorizationError, with: :render_forbidden\n\n  protected\n\n  def authenticate_api_user!\n    authenticate_or_request_with_http_token do |token, options|\n      @current_api_user = User.find_by(api_token: token)\n    end\n  end\n\n  def current_api_user\n    @current_api_user\n  end\n\n  def set_default_format\n    request.format = :json unless params[:format]\n  end\n\n  def render_success(data = nil, message = nil, status = :ok)\n    response = { success: true }\n    response[:message] = message if message\n    response[:data] = data if data\n    render json: response, status: status\n  end\n\n  def render_error(message, status = :bad_request, errors = nil)\n    response = { success: false, message: message }\n    response[:errors] = errors if errors\n    render json: response, status: status\n  end\n\n  def render_not_found(exception)\n    render_error(\"Record not found\", :not_found)\n  end\n\n  def render_unprocessable_entity(exception)\n    render_error(\"Validation failed\", :unprocessable_entity, exception.record.errors)\n  end\n\n  def render_forbidden(exception)\n    render_error(\"Access denied\", :forbidden)\n  end\nend\n</code></pre>"},{"location":"rails/controllers/#ragdollapiv1documentscontroller","title":"Ragdoll::Api::V1::DocumentsController","text":"<p>RESTful API for document operations.</p> <pre><code>class Ragdoll::Api::V1::DocumentsController &lt; Ragdoll::Api::V1::BaseController\n  before_action :set_document, only: [:show, :update, :destroy, :reprocess]\n  before_action :authorize_document!, only: [:show, :update, :destroy]\n\n  # GET /api/v1/documents\n  def index\n    @documents = documents_scope\n      .includes(:user, :contents)\n      .page(params[:page])\n      .per(params[:per_page] || 20)\n\n    apply_filters\n\n    render json: {\n      documents: @documents.map { |doc| serialize_document(doc) },\n      pagination: pagination_meta(@documents)\n    }\n  end\n\n  # GET /api/v1/documents/:id\n  def show\n    render json: {\n      document: serialize_document(@document, include_content: true)\n    }\n  end\n\n  # POST /api/v1/documents\n  def create\n    @document = documents_scope.build(document_params)\n    @document.user = current_api_user\n\n    if @document.save\n      enqueue_processing if should_auto_process?\n      render_success(serialize_document(@document), \"Document created successfully\", :created)\n    else\n      render_error(\"Document creation failed\", :unprocessable_entity, @document.errors)\n    end\n  end\n\n  # PATCH /api/v1/documents/:id\n  def update\n    if @document.update(document_params)\n      enqueue_reprocessing if file_changed?\n      render_success(serialize_document(@document), \"Document updated successfully\")\n    else\n      render_error(\"Document update failed\", :unprocessable_entity, @document.errors)\n    end\n  end\n\n  # DELETE /api/v1/documents/:id\n  def destroy\n    @document.destroy\n    render_success(nil, \"Document deleted successfully\")\n  end\n\n  # POST /api/v1/documents/:id/reprocess\n  def reprocess\n    Ragdoll::ProcessDocumentJob.perform_later(@document)\n    render_success(nil, \"Document reprocessing queued\")\n  end\n\n  # POST /api/v1/documents/bulk_upload\n  def bulk_upload\n    files = params[:files] || []\n    metadata = params[:metadata] || {}\n\n    results = []\n    errors = []\n\n    files.each_with_index do |file, index|\n      document = documents_scope.build(\n        title: file.original_filename,\n        file: file,\n        metadata: metadata,\n        user: current_api_user\n      )\n\n      if document.save\n        enqueue_processing if should_auto_process?\n        results &lt;&lt; serialize_document(document)\n      else\n        errors &lt;&lt; { index: index, filename: file.original_filename, errors: document.errors }\n      end\n    end\n\n    render json: {\n      success: errors.empty?,\n      uploaded: results.size,\n      failed: errors.size,\n      documents: results,\n      errors: errors\n    }\n  end\n\n  private\n\n  def documents_scope\n    Ragdoll::Document.accessible_by(current_api_user)\n  end\n\n  def set_document\n    @document = documents_scope.find(params[:id])\n  end\n\n  def document_params\n    params.require(:document).permit(:title, :description, :file, metadata: {})\n  end\n\n  def apply_filters\n    @documents = @documents.where(status: params[:status]) if params[:status].present?\n    @documents = @documents.where(content_type: params[:content_type]) if params[:content_type].present?\n    @documents = @documents.where('created_at &gt;= ?', params[:created_after]) if params[:created_after].present?\n    @documents = @documents.where('created_at &lt;= ?', params[:created_before]) if params[:created_before].present?\n  end\n\n  def serialize_document(document, include_content: false)\n    result = {\n      id: document.id,\n      title: document.title,\n      description: document.description,\n      content_type: document.content_type,\n      file_size: document.file_size,\n      status: document.status,\n      metadata: document.metadata,\n      created_at: document.created_at,\n      updated_at: document.updated_at,\n      user: {\n        id: document.user&amp;.id,\n        name: document.user&amp;.name\n      },\n      urls: {\n        self: api_v1_document_url(document),\n        download: document_url(document) + '/download'\n      }\n    }\n\n    if include_content\n      result[:content] = {\n        preview: document.content_preview,\n        chunks_count: document.contents.count,\n        embeddings_count: document.embeddings.count\n      }\n    end\n\n    result\n  end\n\n  def pagination_meta(collection)\n    {\n      current_page: collection.current_page,\n      total_pages: collection.total_pages,\n      total_count: collection.total_count,\n      per_page: collection.limit_value\n    }\n  end\n\n  def authorize_document!\n    return if Ragdoll.configuration.authorize_document_access.call(@document, current_api_user)\n\n    raise Ragdoll::AuthorizationError, \"Access denied to document #{@document.id}\"\n  end\n\n  def should_auto_process?\n    Ragdoll.configuration.auto_process_documents &amp;&amp; @document.file.attached?\n  end\n\n  def enqueue_processing\n    Ragdoll::ProcessDocumentJob.perform_later(@document)\n  end\n\n  def enqueue_reprocessing\n    return unless @document.file.attached?\n\n    @document.update!(status: 'pending')\n    Ragdoll::ProcessDocumentJob.perform_later(@document)\n  end\n\n  def file_changed?\n    @document.saved_change_to_attribute?(:file) || @document.file.attachment.changed?\n  end\nend\n</code></pre>"},{"location":"rails/controllers/#controller-customization","title":"Controller Customization","text":""},{"location":"rails/controllers/#inheriting-from-ragdoll-controllers","title":"Inheriting from Ragdoll Controllers","text":"<pre><code>class DocumentsController &lt; Ragdoll::DocumentsController\n  before_action :set_company_scope\n\n  private\n\n  def documents_scope\n    current_user.company.documents\n  end\n\n  def set_company_scope\n    @company = current_user.company\n  end\n\n  def document_params\n    super.merge(company_id: @company.id)\n  end\nend\n</code></pre>"},{"location":"rails/controllers/#custom-authorization","title":"Custom Authorization","text":"<pre><code>class Ragdoll::DocumentsController &lt; Ragdoll::ApplicationController\n  include Pundit::Authorization\n\n  def show\n    authorize @document\n    # ... rest of action\n  end\n\n  def create\n    @document = documents_scope.build(document_params)\n    authorize @document\n    # ... rest of action\n  end\nend\n\n# app/policies/ragdoll/document_policy.rb\nclass Ragdoll::DocumentPolicy &lt; ApplicationPolicy\n  def show?\n    user.admin? || record.user == user || record.public?\n  end\n\n  def create?\n    user.present? &amp;&amp; user.can_upload_documents?\n  end\n\n  def update?\n    user.admin? || record.user == user\n  end\n\n  def destroy?\n    user.admin? || record.user == user\n  end\nend\n</code></pre>"},{"location":"rails/controllers/#adding-custom-actions","title":"Adding Custom Actions","text":"<pre><code>class Ragdoll::DocumentsController &lt; Ragdoll::ApplicationController\n  # GET /documents/:id/preview\n  def preview\n    @document = documents_scope.find(params[:id])\n    authorize_document!\n\n    @preview_content = Ragdoll::PreviewService.new(@document).generate\n\n    respond_to do |format|\n      format.html { render layout: false }\n      format.json { render json: { preview: @preview_content } }\n    end\n  end\n\n  # POST /documents/:id/share\n  def share\n    @document = documents_scope.find(params[:id])\n    authorize_document!\n\n    @share_link = Ragdoll::ShareLinkService.new(@document, current_user).create\n\n    respond_to do |format|\n      format.json { render json: { share_url: @share_link.url } }\n    end\n  end\n\n  # POST /documents/:id/favorite\n  def favorite\n    @document = documents_scope.find(params[:id])\n    current_user.favorite(@document)\n\n    respond_to do |format|\n      format.json { render json: { favorited: true } }\n    end\n  end\nend\n</code></pre> <p>This comprehensive controller documentation provides everything you need to understand, use, and customize the Ragdoll Rails controllers for your specific needs.</p>"},{"location":"rails/examples/","title":"Rails Examples","text":"<p>Real-world examples and implementation patterns for using ragdoll-rails in various scenarios.</p>"},{"location":"rails/examples/#complete-application-examples","title":"Complete Application Examples","text":""},{"location":"rails/examples/#knowledge-base-application","title":"Knowledge Base Application","text":"<p>A complete knowledge base application using Ragdoll Rails.</p>"},{"location":"rails/examples/#models","title":"Models","text":"<pre><code># app/models/user.rb\nclass User &lt; ApplicationRecord\n  has_many :articles, dependent: :destroy\n  has_many :saved_searches, dependent: :destroy\n\n  enum role: { viewer: 0, editor: 1, admin: 2 }\n\n  def can_upload_documents?\n    editor? || admin?\n  end\nend\n\n# app/models/article.rb\nclass Article &lt; ApplicationRecord\n  include Ragdoll::Searchable\n\n  belongs_to :user\n  belongs_to :category, optional: true\n  has_many_attached :attachments\n\n  validates :title, presence: true, length: { maximum: 255 }\n  validates :content, presence: true\n  validates :status, inclusion: { in: %w[draft published archived] }\n\n  ragdoll_searchable do |config|\n    config.content_field = :content\n    config.title_field = :title\n    config.metadata_fields = [:category_name, :tags, :author_name, :status]\n    config.chunk_size = 800\n    config.auto_process = true\n    config.process_on_create = true\n    config.process_on_update = true\n\n    config.custom_metadata = -&gt;(article) {\n      {\n        category_name: article.category&amp;.name,\n        author_name: article.user.name,\n        word_count: article.content.split.size,\n        reading_time: (article.content.split.size / 200.0).ceil,\n        tags: article.tag_list,\n        last_updated: article.updated_at.iso8601\n      }\n    }\n  end\n\n  scope :published, -&gt; { where(status: 'published') }\n  scope :by_category, -&gt;(category) { where(category: category) }\n  scope :by_user, -&gt;(user) { where(user: user) }\n  scope :recent, -&gt;(days = 30) { where(created_at: days.days.ago..) }\n\n  def category_name\n    category&amp;.name\n  end\n\n  def author_name\n    user.name\n  end\n\n  def tag_list\n    tags.split(',').map(&amp;:strip) if tags.present?\n  end\n\n  def reading_time_minutes\n    (content.split.size / 200.0).ceil\n  end\nend\n\n# app/models/category.rb\nclass Category &lt; ApplicationRecord\n  has_many :articles, dependent: :destroy\n\n  validates :name, presence: true, uniqueness: true\n  validates :description, presence: true\n\n  scope :with_articles, -&gt; { joins(:articles).distinct }\n\n  def article_count\n    articles.published.count\n  end\nend\n\n# app/models/saved_search.rb\nclass SavedSearch &lt; ApplicationRecord\n  belongs_to :user\n\n  validates :name, presence: true\n  validates :query, presence: true\n\n  scope :public_searches, -&gt; { where(is_public: true) }\n  scope :by_user, -&gt;(user) { where(user: user) }\n\n  def execute\n    options = filters.present? ? { filters: filters } : {}\n    Article.search(query, options)\n  end\nend\n</code></pre>"},{"location":"rails/examples/#controllers","title":"Controllers","text":"<pre><code># app/controllers/application_controller.rb\nclass ApplicationController &lt; ActionController::Base\n  before_action :authenticate_user!\n  before_action :configure_permitted_parameters, if: :devise_controller?\n\n  protected\n\n  def configure_permitted_parameters\n    devise_parameter_sanitizer.permit(:sign_up, keys: [:name])\n    devise_parameter_sanitizer.permit(:account_update, keys: [:name])\n  end\nend\n\n# app/controllers/articles_controller.rb\nclass ArticlesController &lt; ApplicationController\n  before_action :set_article, only: [:show, :edit, :update, :destroy]\n  before_action :check_edit_permission, only: [:edit, :update, :destroy]\n\n  def index\n    @articles = Article.published.includes(:user, :category)\n    @articles = @articles.by_category(params[:category_id]) if params[:category_id].present?\n    @articles = @articles.page(params[:page]).per(20)\n\n    @categories = Category.with_articles\n  end\n\n  def show\n    @related_articles = @article.find_similar(limit: 5)\n      .where.not(id: @article.id)\n      .where(status: 'published')\n  end\n\n  def new\n    @article = current_user.articles.build\n    @categories = Category.all\n  end\n\n  def create\n    @article = current_user.articles.build(article_params)\n\n    if @article.save\n      redirect_to @article, notice: 'Article was successfully created.'\n    else\n      @categories = Category.all\n      render :new\n    end\n  end\n\n  def edit\n    @categories = Category.all\n  end\n\n  def update\n    if @article.update(article_params)\n      redirect_to @article, notice: 'Article was successfully updated.'\n    else\n      @categories = Category.all\n      render :edit\n    end\n  end\n\n  def destroy\n    @article.destroy\n    redirect_to articles_url, notice: 'Article was successfully deleted.'\n  end\n\n  private\n\n  def set_article\n    @article = Article.find(params[:id])\n  end\n\n  def article_params\n    params.require(:article).permit(:title, :content, :category_id, :tags, :status, attachments: [])\n  end\n\n  def check_edit_permission\n    redirect_to articles_path, alert: 'Access denied.' unless can_edit_article?(@article)\n  end\n\n  def can_edit_article?(article)\n    current_user.admin? || article.user == current_user\n  end\nend\n\n# app/controllers/search_controller.rb\nclass SearchController &lt; ApplicationController\n  before_action :set_search_params, only: [:index, :suggestions]\n\n  def index\n    return unless @query.present?\n\n    @search_service = ArticleSearchService.new(@query, search_options)\n    @results = @search_service.call\n    @facets = @search_service.facets\n    @suggestions = @search_service.suggestions if @results.empty?\n\n    track_search_analytics\n  end\n\n  def suggestions\n    @suggestions = ArticleSuggestionService.new(@query).call\n    render json: @suggestions\n  end\n\n  def saved_searches\n    @saved_searches = current_user.saved_searches.order(:name)\n    @public_searches = SavedSearch.public_searches.includes(:user).limit(10)\n  end\n\n  def save_search\n    @saved_search = current_user.saved_searches.build(saved_search_params)\n\n    if @saved_search.save\n      render json: { success: true, id: @saved_search.id }\n    else\n      render json: { success: false, errors: @saved_search.errors }\n    end\n  end\n\n  private\n\n  def set_search_params\n    @query = params[:q]\n    @filters = params[:filters] || {}\n    @sort = params[:sort] || 'relevance'\n    @page = params[:page] || 1\n  end\n\n  def search_options\n    {\n      filters: @filters,\n      sort: @sort,\n      page: @page,\n      per_page: 15,\n      include_facets: true,\n      search_type: params[:search_type] || 'hybrid'\n    }\n  end\n\n  def track_search_analytics\n    SearchAnalytic.create!(\n      user: current_user,\n      query: @query,\n      results_count: @results.size,\n      search_type: params[:search_type] || 'hybrid',\n      filters: @filters,\n      response_time: @search_service.search_time\n    )\n  end\n\n  def saved_search_params\n    params.require(:saved_search).permit(:name, :query, :is_public, filters: {})\n  end\nend\n</code></pre>"},{"location":"rails/examples/#services","title":"Services","text":"<pre><code># app/services/article_search_service.rb\nclass ArticleSearchService\n  attr_reader :search_time\n\n  def initialize(query, options = {})\n    @query = query\n    @options = options\n    @search_time = 0\n  end\n\n  def call\n    start_time = Time.current\n\n    results = perform_search\n    results = apply_filters(results) if @options[:filters].present?\n    results = apply_sorting(results)\n    results = paginate_results(results)\n\n    @search_time = Time.current - start_time\n\n    add_facets(results) if @options[:include_facets]\n\n    results\n  end\n\n  def facets\n    return {} unless @options[:include_facets]\n\n    {\n      'category' =&gt; category_facets,\n      'author' =&gt; author_facets,\n      'status' =&gt; status_facets,\n      'created_at' =&gt; date_facets\n    }\n  end\n\n  def suggestions\n    return [] if @query.blank?\n\n    # Use a suggestion service or implement basic suggestions\n    ArticleSuggestionService.new(@query).call\n  end\n\n  private\n\n  def perform_search\n    case @options[:search_type]\n    when 'semantic'\n      Article.semantic_search(@query, limit: 100)\n    when 'keyword'\n      Article.keyword_search(@query).limit(100)\n    else\n      Article.search(@query, limit: 100)\n    end\n  end\n\n  def apply_filters(results)\n    filtered = results\n\n    if @options[:filters][:category].present?\n      category = Category.find(@options[:filters][:category])\n      filtered = filtered.where(category: category)\n    end\n\n    if @options[:filters][:author].present?\n      user = User.find(@options[:filters][:author])\n      filtered = filtered.where(user: user)\n    end\n\n    if @options[:filters][:date_range].present?\n      case @options[:filters][:date_range]\n      when 'week'\n        filtered = filtered.where(created_at: 1.week.ago..)\n      when 'month'\n        filtered = filtered.where(created_at: 1.month.ago..)\n      when 'year'\n        filtered = filtered.where(created_at: 1.year.ago..)\n      end\n    end\n\n    filtered\n  end\n\n  def apply_sorting(results)\n    case @options[:sort]\n    when 'date_desc'\n      results.order(created_at: :desc)\n    when 'date_asc'\n      results.order(created_at: :asc)\n    when 'title'\n      results.order(:title)\n    else\n      results # Keep relevance sorting from search\n    end\n  end\n\n  def paginate_results(results)\n    page = @options[:page] || 1\n    per_page = @options[:per_page] || 15\n\n    results.page(page).per(per_page)\n  end\n\n  def add_facets(results)\n    # Add facet information to results\n    results.define_singleton_method(:facets) { facets }\n  end\n\n  def category_facets\n    Article.published.joins(:category)\n      .group('categories.name')\n      .count\n      .map { |name, count| { name: name, count: count } }\n  end\n\n  def author_facets\n    Article.published.joins(:user)\n      .group('users.name')\n      .count\n      .map { |name, count| { name: name, count: count } }\n      .sort_by { |item| -item[:count] }\n      .first(10)\n  end\n\n  def status_facets\n    Article.group(:status).count\n      .map { |status, count| { name: status.humanize, value: status, count: count } }\n  end\n\n  def date_facets\n    [\n      { name: 'Past Week', value: 'week', count: Article.where(created_at: 1.week.ago..).count },\n      { name: 'Past Month', value: 'month', count: Article.where(created_at: 1.month.ago..).count },\n      { name: 'Past Year', value: 'year', count: Article.where(created_at: 1.year.ago..).count }\n    ]\n  end\nend\n\n# app/services/article_suggestion_service.rb\nclass ArticleSuggestionService\n  def initialize(query)\n    @query = query.to_s.downcase.strip\n  end\n\n  def call\n    return [] if @query.blank? || @query.length &lt; 3\n\n    suggestions = []\n\n    # Title-based suggestions\n    suggestions += title_suggestions\n\n    # Tag-based suggestions\n    suggestions += tag_suggestions\n\n    # Category-based suggestions\n    suggestions += category_suggestions\n\n    suggestions.uniq.first(5)\n  end\n\n  private\n\n  def title_suggestions\n    Article.published\n      .where(\"LOWER(title) LIKE ?\", \"%#{@query}%\")\n      .limit(3)\n      .pluck(:title)\n  end\n\n  def tag_suggestions\n    # Assuming tags are stored as comma-separated strings\n    Article.published\n      .where(\"LOWER(tags) LIKE ?\", \"%#{@query}%\")\n      .pluck(:tags)\n      .flat_map { |tags| tags.split(',').map(&amp;:strip) }\n      .select { |tag| tag.downcase.include?(@query) }\n      .uniq\n      .first(2)\n  end\n\n  def category_suggestions\n    Category.where(\"LOWER(name) LIKE ?\", \"%#{@query}%\")\n      .pluck(:name)\n      .first(2)\n  end\nend\n</code></pre>"},{"location":"rails/examples/#views","title":"Views","text":"<pre><code>&lt;!-- app/views/articles/index.html.erb --&gt;\n&lt;div class=\"articles-index\"&gt;\n  &lt;div class=\"page-header\"&gt;\n    &lt;h1&gt;Knowledge Base&lt;/h1&gt;\n    &lt;div class=\"header-actions\"&gt;\n      &lt;%= link_to \"Search Articles\", search_path, class: \"btn btn-outline-primary\" %&gt;\n      &lt;% if current_user.can_upload_documents? %&gt;\n        &lt;%= link_to \"New Article\", new_article_path, class: \"btn btn-primary\" %&gt;\n      &lt;% end %&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"filters-section\"&gt;\n    &lt;%= form_with url: articles_path, method: :get, local: true, class: \"filter-form\" do |form| %&gt;\n      &lt;div class=\"filter-group\"&gt;\n        &lt;%= form.select :category_id, \n            options_from_collection_for_select(@categories, :id, :name, params[:category_id]), \n            { prompt: 'All Categories' }, \n            { class: \"form-select\" } %&gt;\n      &lt;/div&gt;\n      &lt;%= form.submit \"Filter\", class: \"btn btn-outline-primary\" %&gt;\n    &lt;% end %&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"articles-grid\"&gt;\n    &lt;% @articles.each do |article| %&gt;\n      &lt;div class=\"article-card\"&gt;\n        &lt;div class=\"article-content\"&gt;\n          &lt;h3 class=\"article-title\"&gt;\n            &lt;%= link_to article.title, article_path(article) %&gt;\n          &lt;/h3&gt;\n\n          &lt;p class=\"article-excerpt\"&gt;\n            &lt;%= truncate(strip_tags(article.content), length: 150) %&gt;\n          &lt;/p&gt;\n\n          &lt;div class=\"article-meta\"&gt;\n            &lt;span class=\"author\"&gt;By &lt;%= article.author_name %&gt;&lt;/span&gt;\n            &lt;span class=\"date\"&gt;&lt;%= article.created_at.strftime(\"%B %d, %Y\") %&gt;&lt;/span&gt;\n            &lt;% if article.category %&gt;\n              &lt;span class=\"category\"&gt;&lt;%= article.category.name %&gt;&lt;/span&gt;\n            &lt;% end %&gt;\n            &lt;span class=\"reading-time\"&gt;&lt;%= article.reading_time_minutes %&gt; min read&lt;/span&gt;\n          &lt;/div&gt;\n\n          &lt;% if article.tag_list.present? %&gt;\n            &lt;div class=\"article-tags\"&gt;\n              &lt;% article.tag_list.each do |tag| %&gt;\n                &lt;span class=\"tag\"&gt;&lt;%= tag %&gt;&lt;/span&gt;\n              &lt;% end %&gt;\n            &lt;/div&gt;\n          &lt;% end %&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;% end %&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"pagination-wrapper\"&gt;\n    &lt;%= paginate @articles %&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n</code></pre> <pre><code>&lt;!-- app/views/articles/show.html.erb --&gt;\n&lt;div class=\"article-show\"&gt;\n  &lt;div class=\"article-header\"&gt;\n    &lt;div class=\"breadcrumb\"&gt;\n      &lt;%= link_to \"Articles\", articles_path %&gt;\n      &lt;% if @article.category %&gt;\n        \u2192 &lt;%= link_to @article.category.name, articles_path(category_id: @article.category.id) %&gt;\n      &lt;% end %&gt;\n      \u2192 &lt;%= @article.title %&gt;\n    &lt;/div&gt;\n\n    &lt;h1 class=\"article-title\"&gt;&lt;%= @article.title %&gt;&lt;/h1&gt;\n\n    &lt;div class=\"article-meta\"&gt;\n      &lt;div class=\"author-info\"&gt;\n        &lt;strong&gt;&lt;%= @article.author_name %&gt;&lt;/strong&gt;\n        &lt;span class=\"publish-date\"&gt;\n          Published &lt;%= @article.created_at.strftime(\"%B %d, %Y\") %&gt;\n        &lt;/span&gt;\n        &lt;% if @article.updated_at &gt; @article.created_at + 1.hour %&gt;\n          &lt;span class=\"update-date\"&gt;\n            (Updated &lt;%= @article.updated_at.strftime(\"%B %d, %Y\") %&gt;)\n          &lt;/span&gt;\n        &lt;% end %&gt;\n      &lt;/div&gt;\n\n      &lt;div class=\"article-stats\"&gt;\n        &lt;span class=\"reading-time\"&gt;\n          &lt;i class=\"bi bi-clock\"&gt;&lt;/i&gt; &lt;%= @article.reading_time_minutes %&gt; min read\n        &lt;/span&gt;\n        &lt;% if @article.category %&gt;\n          &lt;span class=\"category\"&gt;\n            &lt;i class=\"bi bi-tag\"&gt;&lt;/i&gt; &lt;%= @article.category.name %&gt;\n          &lt;/span&gt;\n        &lt;% end %&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;% if can_edit_article?(@article) %&gt;\n      &lt;div class=\"article-actions\"&gt;\n        &lt;%= link_to \"Edit\", edit_article_path(@article), class: \"btn btn-secondary\" %&gt;\n        &lt;%= link_to \"Delete\", article_path(@article), method: :delete,\n                    confirm: \"Are you sure?\", class: \"btn btn-outline-danger\" %&gt;\n      &lt;/div&gt;\n    &lt;% end %&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"article-content\"&gt;\n    &lt;%= simple_format(@article.content) %&gt;\n  &lt;/div&gt;\n\n  &lt;% if @article.attachments.any? %&gt;\n    &lt;div class=\"article-attachments\"&gt;\n      &lt;h3&gt;Attachments&lt;/h3&gt;\n      &lt;% @article.attachments.each do |attachment| %&gt;\n        &lt;div class=\"attachment\"&gt;\n          &lt;%= link_to attachment.filename, rails_blob_path(attachment, disposition: \"attachment\") %&gt;\n          &lt;span class=\"file-size\"&gt;(&lt;%= number_to_human_size(attachment.byte_size) %&gt;)&lt;/span&gt;\n        &lt;/div&gt;\n      &lt;% end %&gt;\n    &lt;/div&gt;\n  &lt;% end %&gt;\n\n  &lt;% if @article.tag_list.present? %&gt;\n    &lt;div class=\"article-tags\"&gt;\n      &lt;h4&gt;Tags:&lt;/h4&gt;\n      &lt;% @article.tag_list.each do |tag| %&gt;\n        &lt;span class=\"tag\"&gt;&lt;%= tag %&gt;&lt;/span&gt;\n      &lt;% end %&gt;\n    &lt;/div&gt;\n  &lt;% end %&gt;\n\n  &lt;% if @related_articles.any? %&gt;\n    &lt;div class=\"related-articles\"&gt;\n      &lt;h3&gt;Related Articles&lt;/h3&gt;\n      &lt;div class=\"related-grid\"&gt;\n        &lt;% @related_articles.each do |article| %&gt;\n          &lt;div class=\"related-card\"&gt;\n            &lt;h4&gt;&lt;%= link_to article.title, article_path(article) %&gt;&lt;/h4&gt;\n            &lt;p&gt;&lt;%= truncate(strip_tags(article.content), length: 100) %&gt;&lt;/p&gt;\n            &lt;small&gt;By &lt;%= article.author_name %&gt;&lt;/small&gt;\n          &lt;/div&gt;\n        &lt;% end %&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;% end %&gt;\n&lt;/div&gt;\n</code></pre> <pre><code>&lt;!-- app/views/search/index.html.erb --&gt;\n&lt;div class=\"search-page\"&gt;\n  &lt;div class=\"search-header\"&gt;\n    &lt;h1&gt;Search Knowledge Base&lt;/h1&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"search-form\"&gt;\n    &lt;%= form_with url: search_path, method: :get, local: true, \n                  data: { controller: \"search\", search_suggestions_url_value: suggestions_search_path } do |form| %&gt;\n      &lt;div class=\"search-input-group\"&gt;\n        &lt;%= form.text_field :q, value: params[:q], \n                           placeholder: \"Search articles, categories, or tags...\", \n                           class: \"search-input form-control\",\n                           data: { \n                             action: \"input-&gt;search#suggest\",\n                             search_target: \"input\"\n                           } %&gt;\n        &lt;%= form.submit \"Search\", class: \"btn btn-primary\" %&gt;\n      &lt;/div&gt;\n\n      &lt;div class=\"search-options\"&gt;\n        &lt;div class=\"row\"&gt;\n          &lt;div class=\"col-md-4\"&gt;\n            &lt;%= form.select :search_type, [\n              ['Smart Search (Recommended)', 'hybrid'],\n              ['Meaning-based Search', 'semantic'],\n              ['Exact Word Search', 'keyword']\n            ], { selected: params[:search_type] || 'hybrid' }, { class: \"form-select\" } %&gt;\n          &lt;/div&gt;\n          &lt;div class=\"col-md-4\"&gt;\n            &lt;%= form.select :sort, [\n              ['Most Relevant', 'relevance'],\n              ['Newest First', 'date_desc'],\n              ['Oldest First', 'date_asc'],\n              ['Alphabetical', 'title']\n            ], { selected: params[:sort] || 'relevance' }, { class: \"form-select\" } %&gt;\n          &lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;% end %&gt;\n\n    &lt;div class=\"search-suggestions\" data-search-target=\"suggestions\" style=\"display: none;\"&gt;\n      &lt;!-- Populated via Stimulus --&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;% if @results.present? %&gt;\n    &lt;div class=\"search-results-section\"&gt;\n      &lt;div class=\"search-meta\"&gt;\n        &lt;p&gt;Found &lt;strong&gt;&lt;%= pluralize(@results.total_count, 'article') %&gt;&lt;/strong&gt; \n           &lt;% if params[:q].present? %&gt;for \"&lt;strong&gt;&lt;%= params[:q] %&gt;&lt;/strong&gt;\"&lt;% end %&gt;\n           &lt;span class=\"search-time\"&gt;in &lt;%= number_with_precision(@search_service.search_time, precision: 3) %&gt;s&lt;/span&gt;\n        &lt;/p&gt;\n      &lt;/div&gt;\n\n      &lt;div class=\"search-content\"&gt;\n        &lt;% if @facets.present? %&gt;\n          &lt;div class=\"search-facets\"&gt;\n            &lt;h4&gt;Filter Results&lt;/h4&gt;\n\n            &lt;% @facets.each do |facet_name, facet_data| %&gt;\n              &lt;div class=\"facet-group\"&gt;\n                &lt;h5&gt;&lt;%= facet_name.humanize %&gt;&lt;/h5&gt;\n                &lt;% facet_data.each do |item| %&gt;\n                  &lt;div class=\"facet-item\"&gt;\n                    &lt;%= link_to \"#{item[:name]} (#{item[:count]})\", \n                                search_path(q: params[:q], filters: { facet_name =&gt; item[:value] || item[:name] }),\n                                class: \"facet-link\" %&gt;\n                  &lt;/div&gt;\n                &lt;% end %&gt;\n              &lt;/div&gt;\n            &lt;% end %&gt;\n          &lt;/div&gt;\n        &lt;% end %&gt;\n\n        &lt;div class=\"search-results\"&gt;\n          &lt;% @results.each do |article| %&gt;\n            &lt;div class=\"search-result\"&gt;\n              &lt;div class=\"result-header\"&gt;\n                &lt;h3 class=\"result-title\"&gt;\n                  &lt;%= link_to article.title, article_path(article) %&gt;\n                &lt;/h3&gt;\n                &lt;div class=\"result-meta\"&gt;\n                  &lt;% if respond_to?(:similarity_score) &amp;&amp; article.respond_to?(:similarity_score) %&gt;\n                    &lt;span class=\"similarity-score\"&gt;\n                      &lt;%= number_to_percentage(article.similarity_score * 100, precision: 1) %&gt; match\n                    &lt;/span&gt;\n                  &lt;% end %&gt;\n                  &lt;span class=\"author\"&gt;by &lt;%= article.author_name %&gt;&lt;/span&gt;\n                  &lt;span class=\"date\"&gt;&lt;%= article.created_at.strftime(\"%b %d, %Y\") %&gt;&lt;/span&gt;\n                  &lt;% if article.category %&gt;\n                    &lt;span class=\"category\"&gt;&lt;%= article.category.name %&gt;&lt;/span&gt;\n                  &lt;% end %&gt;\n                &lt;/div&gt;\n              &lt;/div&gt;\n\n              &lt;div class=\"result-content\"&gt;\n                &lt;p class=\"result-snippet\"&gt;\n                  &lt;%= search_result_snippet(article.content, params[:q]) %&gt;\n                &lt;/p&gt;\n              &lt;/div&gt;\n            &lt;/div&gt;\n          &lt;% end %&gt;\n\n          &lt;div class=\"search-pagination\"&gt;\n            &lt;%= paginate @results %&gt;\n          &lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;% elsif params[:q].present? %&gt;\n    &lt;div class=\"no-results\"&gt;\n      &lt;h3&gt;No articles found&lt;/h3&gt;\n      &lt;p&gt;Try different keywords or check your spelling.&lt;/p&gt;\n\n      &lt;% if @suggestions.present? %&gt;\n        &lt;div class=\"search-suggestions\"&gt;\n          &lt;p&gt;Did you mean:&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;% @suggestions.each do |suggestion| %&gt;\n              &lt;li&gt;&lt;%= link_to suggestion, search_path(q: suggestion) %&gt;&lt;/li&gt;\n            &lt;% end %&gt;\n          &lt;/ul&gt;\n        &lt;/div&gt;\n      &lt;% end %&gt;\n    &lt;/div&gt;\n  &lt;% end %&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"rails/examples/#document-management-system","title":"Document Management System","text":"<p>A corporate document management system with advanced features.</p>"},{"location":"rails/examples/#advanced-model-with-versioning","title":"Advanced Model with Versioning","text":"<pre><code># app/models/document.rb\nclass Document &lt; ApplicationRecord\n  include Ragdoll::Searchable\n\n  belongs_to :user\n  belongs_to :department\n  has_many :document_versions, dependent: :destroy\n  has_many :document_shares, dependent: :destroy\n  has_many :shared_with_users, through: :document_shares, source: :user\n  has_many :document_comments, dependent: :destroy\n  has_one_attached :file\n\n  validates :title, presence: true\n  validates :visibility, inclusion: { in: %w[private department company public] }\n\n  ragdoll_searchable do |config|\n    config.content_field = :extracted_content\n    config.title_field = :title\n    config.metadata_fields = [:department_name, :document_type, :tags, :visibility]\n    config.chunk_size = 1200\n    config.auto_process = true\n\n    config.custom_metadata = -&gt;(doc) {\n      {\n        department_name: doc.department.name,\n        document_type: doc.document_type,\n        file_extension: doc.file_extension,\n        version: doc.current_version,\n        shared_count: doc.document_shares.count,\n        comment_count: doc.document_comments.count,\n        last_accessed: doc.last_accessed_at,\n        security_level: doc.security_level\n      }\n    }\n  end\n\n  enum document_type: {\n    policy: 0,\n    procedure: 1,\n    manual: 2,\n    report: 3,\n    presentation: 4,\n    specification: 5,\n    other: 99\n  }\n\n  enum security_level: {\n    public: 0,\n    internal: 1,\n    confidential: 2,\n    restricted: 3\n  }\n\n  scope :accessible_by, -&gt;(user) {\n    where(\n      \"(visibility = 'public') OR \" \\\n      \"(visibility = 'company' AND :user_id IS NOT NULL) OR \" \\\n      \"(visibility = 'department' AND department_id = :dept_id) OR \" \\\n      \"(user_id = :user_id) OR \" \\\n      \"(id IN (SELECT document_id FROM document_shares WHERE user_id = :user_id))\",\n      user_id: user&amp;.id,\n      dept_id: user&amp;.department_id\n    )\n  }\n\n  def create_version!\n    self.current_version += 1\n    document_versions.create!(\n      version_number: current_version,\n      title: title,\n      content: extracted_content,\n      file_data: file.attached? ? file.blob : nil,\n      created_by: self.user\n    )\n    save!\n  end\n\n  def share_with!(user, permission: 'read')\n    document_shares.find_or_create_by(user: user) do |share|\n      share.permission = permission\n    end\n  end\n\n  def accessible_by?(user)\n    return true if user == self.user\n\n    case visibility\n    when 'public' then true\n    when 'company' then user.present?\n    when 'department' then user&amp;.department == department\n    when 'private' then false\n    else false\n    end || document_shares.exists?(user: user)\n  end\n\n  def track_access!(user)\n    update!(\n      last_accessed_at: Time.current,\n      access_count: access_count + 1\n    )\n\n    DocumentAccess.create!(\n      document: self,\n      user: user,\n      accessed_at: Time.current,\n      ip_address: Current.ip_address,\n      user_agent: Current.user_agent\n    )\n  end\nend\n\n# app/models/document_version.rb\nclass DocumentVersion &lt; ApplicationRecord\n  belongs_to :document\n  belongs_to :created_by, class_name: 'User'\n  has_one_attached :file\n\n  validates :version_number, presence: true, uniqueness: { scope: :document_id }\n\n  scope :ordered, -&gt; { order(version_number: :desc) }\n\n  def restore!\n    document.update!(\n      title: title,\n      extracted_content: content,\n      current_version: version_number\n    )\n\n    if file_data\n      document.file.attach(file_data)\n      document.ragdoll_process!\n    end\n  end\nend\n\n# app/models/document_share.rb\nclass DocumentShare &lt; ApplicationRecord\n  belongs_to :document\n  belongs_to :user\n\n  validates :permission, inclusion: { in: %w[read write admin] }\n  validates :user_id, uniqueness: { scope: :document_id }\n\n  scope :with_write_access, -&gt; { where(permission: %w[write admin]) }\n  scope :with_admin_access, -&gt; { where(permission: 'admin') }\nend\n</code></pre>"},{"location":"rails/examples/#advanced-search-with-filters","title":"Advanced Search with Filters","text":"<pre><code># app/services/advanced_document_search_service.rb\nclass AdvancedDocumentSearchService\n  include ActionView::Helpers::DateHelper\n\n  attr_reader :search_time, :total_results\n\n  def initialize(query, user, options = {})\n    @query = query\n    @user = user\n    @options = options\n    @search_time = 0\n    @total_results = 0\n  end\n\n  def call\n    start_time = Time.current\n\n    results = perform_search\n    results = apply_security_filters(results)\n    results = apply_advanced_filters(results)\n    results = apply_sorting(results)\n    results = paginate_results(results)\n\n    @search_time = Time.current - start_time\n    @total_results = results.total_count\n\n    enhance_results(results)\n  end\n\n  def facets\n    {\n      'document_type' =&gt; document_type_facets,\n      'department' =&gt; department_facets,\n      'security_level' =&gt; security_level_facets,\n      'file_type' =&gt; file_type_facets,\n      'date_range' =&gt; date_range_facets,\n      'author' =&gt; author_facets\n    }\n  end\n\n  private\n\n  def perform_search\n    if @query.present?\n      Document.search(@query, search_options)\n    else\n      Document.accessible_by(@user)\n    end\n  end\n\n  def search_options\n    {\n      limit: 1000, # Get more results for filtering\n      search_type: @options[:search_type] || 'hybrid',\n      threshold: @options[:threshold] || 0.5\n    }\n  end\n\n  def apply_security_filters(results)\n    # Additional security filtering beyond basic accessibility\n    filtered = results.accessible_by(@user)\n\n    # Filter by security clearance\n    if @user.security_clearance.present?\n      max_level = security_level_mapping[@user.security_clearance]\n      filtered = filtered.where(security_level: 0..max_level)\n    end\n\n    filtered\n  end\n\n  def apply_advanced_filters(results)\n    filtered = results\n\n    # Document type filter\n    if @options[:document_type].present?\n      filtered = filtered.where(document_type: @options[:document_type])\n    end\n\n    # Department filter\n    if @options[:department_id].present?\n      filtered = filtered.where(department_id: @options[:department_id])\n    end\n\n    # File type filter\n    if @options[:file_type].present?\n      filtered = filtered.joins(:file_attachment)\n        .where(active_storage_blobs: { content_type: @options[:file_type] })\n    end\n\n    # Date range filter\n    if @options[:date_range].present?\n      filtered = apply_date_filter(filtered, @options[:date_range])\n    end\n\n    # Security level filter\n    if @options[:security_level].present?\n      filtered = filtered.where(security_level: @options[:security_level])\n    end\n\n    # Author filter\n    if @options[:author_id].present?\n      filtered = filtered.where(user_id: @options[:author_id])\n    end\n\n    # Tags filter\n    if @options[:tags].present?\n      tag_conditions = @options[:tags].map { \"tags ILIKE ?\" }\n      tag_values = @options[:tags].map { |tag| \"%#{tag}%\" }\n      filtered = filtered.where(tag_conditions.join(' AND '), *tag_values)\n    end\n\n    # File size filter\n    if @options[:min_size].present? || @options[:max_size].present?\n      filtered = apply_file_size_filter(filtered)\n    end\n\n    filtered\n  end\n\n  def apply_date_filter(results, date_range)\n    case date_range\n    when 'today'\n      results.where(created_at: Date.current.beginning_of_day..)\n    when 'week'\n      results.where(created_at: 1.week.ago..)\n    when 'month'\n      results.where(created_at: 1.month.ago..)\n    when 'quarter'\n      results.where(created_at: 3.months.ago..)\n    when 'year'\n      results.where(created_at: 1.year.ago..)\n    when 'custom'\n      if @options[:start_date] &amp;&amp; @options[:end_date]\n        start_date = Date.parse(@options[:start_date])\n        end_date = Date.parse(@options[:end_date])\n        results.where(created_at: start_date.beginning_of_day..end_date.end_of_day)\n      else\n        results\n      end\n    else\n      results\n    end\n  end\n\n  def apply_file_size_filter(results)\n    joins_clause = &lt;&lt;~SQL\n      LEFT JOIN active_storage_attachments asa ON asa.record_id = documents.id \n        AND asa.record_type = 'Document' AND asa.name = 'file'\n      LEFT JOIN active_storage_blobs asb ON asb.id = asa.blob_id\n    SQL\n\n    filtered = results.joins(joins_clause)\n\n    if @options[:min_size].present?\n      min_bytes = parse_file_size(@options[:min_size])\n      filtered = filtered.where('asb.byte_size &gt;= ?', min_bytes)\n    end\n\n    if @options[:max_size].present?\n      max_bytes = parse_file_size(@options[:max_size])\n      filtered = filtered.where('asb.byte_size &lt;= ?', max_bytes)\n    end\n\n    filtered\n  end\n\n  def apply_sorting(results)\n    case @options[:sort]\n    when 'created_desc'\n      results.order(created_at: :desc)\n    when 'created_asc'\n      results.order(created_at: :asc)\n    when 'updated_desc'\n      results.order(updated_at: :desc)\n    when 'title_asc'\n      results.order(:title)\n    when 'title_desc'\n      results.order(title: :desc)\n    when 'size_desc'\n      results.joins(:file_attachment, :file_blob).order('active_storage_blobs.byte_size DESC')\n    when 'access_count'\n      results.order(access_count: :desc)\n    else\n      results # Keep relevance ordering from search\n    end\n  end\n\n  def paginate_results(results)\n    page = @options[:page] || 1\n    per_page = [@options[:per_page] || 20, 100].min\n\n    results.page(page).per(per_page)\n  end\n\n  def enhance_results(results)\n    # Add additional data to results\n    results.each do |document|\n      # Track that this document appeared in search results\n      SearchResult.create!(\n        user: @user,\n        document: document,\n        query: @query,\n        position: results.index(document) + 1,\n        similarity_score: document.respond_to?(:similarity_score) ? document.similarity_score : nil\n      )\n    end\n\n    results\n  end\n\n  def document_type_facets\n    accessible_documents.group(:document_type).count\n      .map { |type, count| { name: type.humanize, value: type, count: count } }\n  end\n\n  def department_facets\n    accessible_documents.joins(:department)\n      .group('departments.name')\n      .count\n      .map { |name, count| { name: name, value: name, count: count } }\n  end\n\n  def security_level_facets\n    accessible_documents.group(:security_level).count\n      .map { |level, count| { name: level.humanize, value: level, count: count } }\n  end\n\n  def file_type_facets\n    accessible_documents.joins(:file_attachment, :file_blob)\n      .group('active_storage_blobs.content_type')\n      .count\n      .map { |type, count| { name: format_content_type(type), value: type, count: count } }\n      .sort_by { |item| -item[:count] }\n      .first(10)\n  end\n\n  def date_range_facets\n    [\n      { name: 'Today', value: 'today', count: accessible_documents.where(created_at: Date.current.beginning_of_day..).count },\n      { name: 'This Week', value: 'week', count: accessible_documents.where(created_at: 1.week.ago..).count },\n      { name: 'This Month', value: 'month', count: accessible_documents.where(created_at: 1.month.ago..).count },\n      { name: 'This Quarter', value: 'quarter', count: accessible_documents.where(created_at: 3.months.ago..).count },\n      { name: 'This Year', value: 'year', count: accessible_documents.where(created_at: 1.year.ago..).count }\n    ]\n  end\n\n  def author_facets\n    accessible_documents.joins(:user)\n      .group('users.name')\n      .count\n      .map { |name, count| { name: name, count: count } }\n      .sort_by { |item| -item[:count] }\n      .first(15)\n  end\n\n  def accessible_documents\n    @accessible_documents ||= Document.accessible_by(@user)\n  end\n\n  def security_level_mapping\n    {\n      'basic' =&gt; 0,\n      'standard' =&gt; 1,\n      'elevated' =&gt; 2,\n      'top_secret' =&gt; 3\n    }\n  end\n\n  def format_content_type(content_type)\n    case content_type\n    when 'application/pdf' then 'PDF Documents'\n    when /^image\\// then 'Images'\n    when /word|doc/ then 'Word Documents'\n    when /excel|sheet/ then 'Spreadsheets'\n    when /powerpoint|presentation/ then 'Presentations'\n    else content_type.humanize\n    end\n  end\n\n  def parse_file_size(size_string)\n    # Parse size strings like \"10MB\", \"500KB\", \"2GB\"\n    size_string = size_string.to_s.upcase\n    number = size_string.scan(/\\d+/).first.to_f\n    unit = size_string.scan(/[A-Z]+/).first\n\n    case unit\n    when 'KB' then number * 1024\n    when 'MB' then number * 1024 * 1024\n    when 'GB' then number * 1024 * 1024 * 1024\n    else number\n    end\n  end\nend\n</code></pre> <p>This comprehensive example demonstrates how to build sophisticated document management applications using ragdoll-rails with advanced search, security, versioning, and analytics features.</p>"},{"location":"rails/generators/","title":"Rails Generators","text":"<p>The ragdoll-rails gem provides a comprehensive set of Rails generators to help you quickly set up and customize your document management system. These generators follow Rails conventions and can be easily customized.</p>"},{"location":"rails/generators/#installation-generator","title":"Installation Generator","text":""},{"location":"rails/generators/#ragdollinstall","title":"ragdoll:install","text":"<p>The main installation generator sets up the basic Ragdoll configuration in your Rails application.</p> <pre><code>rails generate ragdoll:install\n</code></pre>"},{"location":"rails/generators/#what-it-creates","title":"What it creates:","text":"<ul> <li>Configuration initializer (<code>config/initializers/ragdoll.rb</code>)</li> <li>Database migrations for core models</li> <li>Routes configuration</li> <li>Basic stylesheet and JavaScript imports</li> <li>Sample configuration files</li> </ul>"},{"location":"rails/generators/#generated-files","title":"Generated Files:","text":"<pre><code># config/initializers/ragdoll.rb\nRagdoll.configure do |config|\n  # LLM Provider configuration\n  config.llm_provider = :openai\n  config.openai_api_key = Rails.application.credentials.openai_api_key\n\n  # Default models\n  config.default_embedding_model = \"text-embedding-3-small\"\n  config.default_chat_model = \"gpt-4\"\n\n  # Processing settings\n  config.chunk_size = 1000\n  config.chunk_overlap = 200\n  config.auto_process_documents = true\n\n  # Authentication (customize for your app)\n  config.current_user_method = :current_user\n  config.authenticate_user_method = :authenticate_user!\nend\n</code></pre> <pre><code># config/routes.rb (added to existing routes)\nRails.application.routes.draw do\n  mount Ragdoll::Engine =&gt; '/ragdoll'\n  # ... your existing routes\nend\n</code></pre>"},{"location":"rails/generators/#options","title":"Options:","text":"<pre><code># Skip database migrations\nrails generate ragdoll:install --skip-migrations\n\n# Skip routes configuration\nrails generate ragdoll:install --skip-routes\n\n# Specify a different mount path\nrails generate ragdoll:install --mount-path=/documents\n\n# Skip sample configuration\nrails generate ragdoll:install --skip-config\n</code></pre>"},{"location":"rails/generators/#model-generators","title":"Model Generators","text":""},{"location":"rails/generators/#ragdollmodel","title":"ragdoll:model","text":"<p>Generates a model that integrates with Ragdoll search functionality.</p> <pre><code>rails generate ragdoll:model Article title:string content:text author:string\n</code></pre>"},{"location":"rails/generators/#generated-model","title":"Generated Model:","text":"<pre><code># app/models/article.rb\nclass Article &lt; ApplicationRecord\n  include Ragdoll::Searchable\n\n  ragdoll_searchable do |config|\n    config.content_field = :content\n    config.title_field = :title\n    config.metadata_fields = [:author]\n    config.chunk_size = 1000\n    config.auto_process = true\n  end\n\n  validates :title, presence: true\n  validates :content, presence: true\nend\n</code></pre>"},{"location":"rails/generators/#generated-migration","title":"Generated Migration:","text":"<pre><code># db/migrate/create_articles.rb\nclass CreateArticles &lt; ActiveRecord::Migration[7.0]\n  def change\n    create_table :articles, id: :uuid do |t|\n      t.string :title, null: false\n      t.text :content\n      t.string :author\n      t.json :metadata, default: {}\n      t.tsvector :search_vector\n      t.references :user, type: :uuid, foreign_key: true\n\n      t.timestamps\n    end\n\n    add_index :articles, :search_vector, using: :gin\n    add_index :articles, :author\n  end\nend\n</code></pre>"},{"location":"rails/generators/#options_1","title":"Options:","text":"<pre><code># Specify content field\nrails generate ragdoll:model Post title:string body:text --content-field=body\n\n# Skip user association\nrails generate ragdoll:model Document title:string --skip-user\n\n# Custom chunk size\nrails generate ragdoll:model Article content:text --chunk-size=800\n\n# Skip search configuration\nrails generate ragdoll:model BasicModel name:string --skip-search\n</code></pre>"},{"location":"rails/generators/#ragdollsearchable","title":"ragdoll:searchable","text":"<p>Adds Ragdoll search functionality to an existing model.</p> <pre><code>rails generate ragdoll:searchable Article\n</code></pre>"},{"location":"rails/generators/#generated-changes","title":"Generated Changes:","text":"<pre><code># app/models/article.rb (modified)\nclass Article &lt; ApplicationRecord\n  include Ragdoll::Searchable\n\n  ragdoll_searchable do |config|\n    config.content_field = :content  # Detected automatically\n    config.title_field = :title      # Detected automatically\n    config.auto_process = true\n  end\n\n  # ... existing code\nend\n</code></pre>"},{"location":"rails/generators/#generated-migration_1","title":"Generated Migration:","text":"<pre><code># db/migrate/add_ragdoll_to_articles.rb\nclass AddRagdollToArticles &lt; ActiveRecord::Migration[7.0]\n  def change\n    add_column :articles, :metadata, :json, default: {}\n    add_column :articles, :search_vector, :tsvector\n    add_column :articles, :ragdoll_status, :string, default: 'pending'\n    add_column :articles, :ragdoll_processed_at, :datetime\n\n    add_index :articles, :search_vector, using: :gin\n    add_index :articles, :ragdoll_status\n  end\nend\n</code></pre>"},{"location":"rails/generators/#controller-generators","title":"Controller Generators","text":""},{"location":"rails/generators/#ragdollcontroller","title":"ragdoll:controller","text":"<p>Generates a controller with Ragdoll document management functionality.</p> <pre><code>rails generate ragdoll:controller Documents\n</code></pre>"},{"location":"rails/generators/#generated-controller","title":"Generated Controller:","text":"<pre><code># app/controllers/documents_controller.rb\nclass DocumentsController &lt; ApplicationController\n  include Ragdoll::ControllerHelpers\n\n  before_action :authenticate_user!\n  before_action :set_document, only: [:show, :edit, :update, :destroy]\n\n  # GET /documents\n  def index\n    @documents = current_user.documents\n      .includes(:ragdoll_contents)\n      .page(params[:page])\n      .per(20)\n\n    @documents = @documents.where(ragdoll_status: params[:status]) if params[:status].present?\n  end\n\n  # GET /documents/1\n  def show\n    @related_documents = @document.find_similar(limit: 5)\n  end\n\n  # GET /documents/new\n  def new\n    @document = current_user.documents.build\n  end\n\n  # POST /documents\n  def create\n    @document = current_user.documents.build(document_params)\n\n    if @document.save\n      redirect_to @document, notice: 'Document was successfully created.'\n    else\n      render :new\n    end\n  end\n\n  # PATCH/PUT /documents/1\n  def update\n    if @document.update(document_params)\n      redirect_to @document, notice: 'Document was successfully updated.'\n    else\n      render :edit\n    end\n  end\n\n  # DELETE /documents/1\n  def destroy\n    @document.destroy\n    redirect_to documents_url, notice: 'Document was successfully deleted.'\n  end\n\n  private\n\n  def set_document\n    @document = current_user.documents.find(params[:id])\n  end\n\n  def document_params\n    params.require(:document).permit(:title, :content, :file, metadata: {})\n  end\nend\n</code></pre>"},{"location":"rails/generators/#generated-views","title":"Generated Views:","text":"<p>The generator creates a complete set of views:</p> <ul> <li><code>app/views/documents/index.html.erb</code></li> <li><code>app/views/documents/show.html.erb</code></li> <li><code>app/views/documents/new.html.erb</code></li> <li><code>app/views/documents/edit.html.erb</code></li> <li><code>app/views/documents/_form.html.erb</code></li> </ul>"},{"location":"rails/generators/#options_2","title":"Options:","text":"<pre><code># Generate API controller instead\nrails generate ragdoll:controller Documents --api\n\n# Skip view generation\nrails generate ragdoll:controller Documents --skip-views\n\n# Generate with search functionality\nrails generate ragdoll:controller Documents --with-search\n\n# Custom parent controller\nrails generate ragdoll:controller Documents --parent=AdminController\n</code></pre>"},{"location":"rails/generators/#ragdollsearch_controller","title":"ragdoll:search_controller","text":"<p>Generates a controller specifically for search functionality.</p> <pre><code>rails generate ragdoll:search_controller Search\n</code></pre>"},{"location":"rails/generators/#generated-controller_1","title":"Generated Controller:","text":"<pre><code># app/controllers/search_controller.rb\nclass SearchController &lt; ApplicationController\n  include Ragdoll::SearchHelpers\n\n  # GET /search\n  def index\n    return unless params[:q].present?\n\n    @query = params[:q]\n    @search_results = perform_search(@query, search_options)\n    @facets = @search_results.facets if params[:include_facets]\n\n    track_search_analytics if respond_to?(:track_search_analytics)\n  end\n\n  # GET /search/suggestions\n  def suggestions\n    @suggestions = Ragdoll::SuggestionService.new(params[:q]).call\n    render json: @suggestions\n  end\n\n  # GET /search/autocomplete\n  def autocomplete\n    @completions = Ragdoll::AutocompleteService.new(params[:q]).call\n    render json: @completions\n  end\n\n  private\n\n  def search_options\n    {\n      limit: params[:limit] || 20,\n      page: params[:page] || 1,\n      filters: params[:filters] || {},\n      search_type: params[:search_type] || 'hybrid',\n      include_facets: params[:include_facets]\n    }\n  end\n\n  def perform_search(query, options = {})\n    case params[:model]\n    when 'Document'\n      Ragdoll::Document.search(query, options)\n    else\n      # Search across all searchable models\n      Ragdoll::GlobalSearchService.new(query, options).call\n    end\n  end\nend\n</code></pre>"},{"location":"rails/generators/#view-generators","title":"View Generators","text":""},{"location":"rails/generators/#ragdollviews","title":"ragdoll:views","text":"<p>Copies Ragdoll views to your application for customization.</p> <pre><code>rails generate ragdoll:views\n</code></pre>"},{"location":"rails/generators/#copies-all-views","title":"Copies all views:","text":"<pre><code>app/views/ragdoll/\n\u251c\u2500\u2500 documents/\n\u2502   \u251c\u2500\u2500 index.html.erb\n\u2502   \u251c\u2500\u2500 show.html.erb\n\u2502   \u251c\u2500\u2500 new.html.erb\n\u2502   \u251c\u2500\u2500 edit.html.erb\n\u2502   \u2514\u2500\u2500 _form.html.erb\n\u251c\u2500\u2500 search/\n\u2502   \u251c\u2500\u2500 index.html.erb\n\u2502   \u251c\u2500\u2500 _search_form.html.erb\n\u2502   \u251c\u2500\u2500 _search_results.html.erb\n\u2502   \u2514\u2500\u2500 _facets.html.erb\n\u2514\u2500\u2500 shared/\n    \u251c\u2500\u2500 _document_card.html.erb\n    \u251c\u2500\u2500 _upload_form.html.erb\n    \u2514\u2500\u2500 _pagination.html.erb\n</code></pre>"},{"location":"rails/generators/#options_3","title":"Options:","text":"<pre><code># Copy only specific views\nrails generate ragdoll:views --only=documents\n\n# Copy to custom directory\nrails generate ragdoll:views --path=admin/ragdoll\n</code></pre>"},{"location":"rails/generators/#ragdolllayout","title":"ragdoll:layout","text":"<p>Generates a layout file optimized for Ragdoll functionality.</p> <pre><code>rails generate ragdoll:layout\n</code></pre>"},{"location":"rails/generators/#generated-layout","title":"Generated Layout:","text":"<pre><code>&lt;!-- app/views/layouts/ragdoll.html.erb --&gt;\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Document Management&lt;/title&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"&gt;\n    &lt;%= csrf_meta_tags %&gt;\n    &lt;%= csp_meta_tag %&gt;\n\n    &lt;%= stylesheet_link_tag \"ragdoll/application\", \"data-turbo-track\": \"reload\" %&gt;\n    &lt;%= javascript_importmap_tags %&gt;\n    &lt;%= javascript_include_tag \"ragdoll/application\", \"data-turbo-track\": \"reload\" %&gt;\n  &lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;nav class=\"ragdoll-navbar\"&gt;\n      &lt;div class=\"container\"&gt;\n        &lt;%= link_to \"Documents\", ragdoll.documents_path, class: \"navbar-brand\" %&gt;\n\n        &lt;div class=\"navbar-nav\"&gt;\n          &lt;%= link_to \"Upload\", ragdoll.new_document_path, class: \"nav-link\" %&gt;\n          &lt;%= link_to \"Search\", ragdoll.search_path, class: \"nav-link\" %&gt;\n          &lt;% if user_signed_in? %&gt;\n            &lt;%= link_to \"My Documents\", ragdoll.documents_path(user: current_user), class: \"nav-link\" %&gt;\n          &lt;% end %&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/nav&gt;\n\n    &lt;main class=\"ragdoll-main\"&gt;\n      &lt;div class=\"container\"&gt;\n        &lt;% if notice %&gt;\n          &lt;div class=\"alert alert-success alert-dismissible\"&gt;\n            &lt;%= notice %&gt;\n            &lt;button type=\"button\" class=\"btn-close\" data-bs-dismiss=\"alert\"&gt;&lt;/button&gt;\n          &lt;/div&gt;\n        &lt;% end %&gt;\n\n        &lt;% if alert %&gt;\n          &lt;div class=\"alert alert-danger alert-dismissible\"&gt;\n            &lt;%= alert %&gt;\n            &lt;button type=\"button\" class=\"btn-close\" data-bs-dismiss=\"alert\"&gt;&lt;/button&gt;\n          &lt;/div&gt;\n        &lt;% end %&gt;\n\n        &lt;%= yield %&gt;\n      &lt;/div&gt;\n    &lt;/main&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"rails/generators/#configuration-generators","title":"Configuration Generators","text":""},{"location":"rails/generators/#ragdollconfig","title":"ragdoll:config","text":"<p>Generates advanced configuration files.</p> <pre><code>rails generate ragdoll:config\n</code></pre>"},{"location":"rails/generators/#generated-files_1","title":"Generated Files:","text":"<pre><code># config/ragdoll.yml\ndefault: &amp;default\n  llm:\n    provider: openai\n    models:\n      embedding: \"text-embedding-3-small\"\n      chat: \"gpt-4\"\n\n  processing:\n    chunk_size: 1000\n    chunk_overlap: 200\n    auto_process: true\n    background_jobs: true\n\n  search:\n    default_limit: 20\n    similarity_threshold: 0.7\n    enable_facets: true\n\ndevelopment:\n  &lt;&lt;: *default\n  llm:\n    provider: openai\n    api_key: &lt;%= ENV['OPENAI_API_KEY'] %&gt;\n\ntest:\n  &lt;&lt;: *default\n  llm:\n    provider: mock\n\nproduction:\n  &lt;&lt;: *default\n  llm:\n    provider: openai\n    api_key: &lt;%= Rails.application.credentials.openai_api_key %&gt;\n</code></pre> <pre><code># lib/tasks/ragdoll.rake\nnamespace :ragdoll do\n  desc \"Process all pending documents\"\n  task process_pending: :environment do\n    Ragdoll::Document.pending.find_each do |document|\n      Ragdoll::ProcessDocumentJob.perform_later(document)\n    end\n  end\n\n  desc \"Reindex all documents\"\n  task reindex: :environment do\n    Ragdoll::Document.processed.find_each do |document|\n      Ragdoll::IndexDocumentJob.perform_later(document)\n    end\n  end\n\n  desc \"Clean up failed documents\"\n  task cleanup_failed: :environment do\n    failed_documents = Ragdoll::Document.failed.where('updated_at &lt; ?', 24.hours.ago)\n    puts \"Cleaning up #{failed_documents.count} failed documents\"\n    failed_documents.destroy_all\n  end\n\n  desc \"Generate search analytics report\"\n  task analytics_report: :environment do\n    report = Ragdoll::AnalyticsService.new.generate_report\n    puts report\n  end\nend\n</code></pre>"},{"location":"rails/generators/#ragdollcredentials","title":"ragdoll:credentials","text":"<p>Sets up encrypted credentials for LLM providers.</p> <pre><code>rails generate ragdoll:credentials\n</code></pre>"},{"location":"rails/generators/#adds-to-credentials","title":"Adds to credentials:","text":"<pre><code># config/credentials.yml.enc (decrypted view)\nopenai:\n  api_key: sk-your-openai-key-here\n\nanthropic:\n  api_key: sk-ant-your-anthropic-key-here\n\nazure:\n  api_key: your-azure-key\n  endpoint: https://your-resource.openai.azure.com/\n</code></pre>"},{"location":"rails/generators/#migration-generators","title":"Migration Generators","text":""},{"location":"rails/generators/#ragdollmigration","title":"ragdoll:migration","text":"<p>Generates custom migrations for Ragdoll extensions.</p> <pre><code>rails generate ragdoll:migration add_custom_fields_to_documents category:string priority:integer\n</code></pre>"},{"location":"rails/generators/#generated-migration_2","title":"Generated Migration:","text":"<pre><code># db/migrate/add_custom_fields_to_ragdoll_documents.rb\nclass AddCustomFieldsToRagdollDocuments &lt; ActiveRecord::Migration[7.0]\n  def change\n    add_column :ragdoll_documents, :category, :string\n    add_column :ragdoll_documents, :priority, :integer, default: 0\n\n    add_index :ragdoll_documents, :category\n    add_index :ragdoll_documents, :priority\n  end\nend\n</code></pre>"},{"location":"rails/generators/#ragdollindexes","title":"ragdoll:indexes","text":"<p>Generates optimized database indexes for better search performance.</p> <pre><code>rails generate ragdoll:indexes\n</code></pre>"},{"location":"rails/generators/#generated-migration_3","title":"Generated Migration:","text":"<pre><code># db/migrate/add_ragdoll_performance_indexes.rb\nclass AddRagdollPerformanceIndexes &lt; ActiveRecord::Migration[7.0]\n  def change\n    # Vector similarity indexes\n    execute &lt;&lt;-SQL\n      CREATE INDEX CONCURRENTLY IF NOT EXISTS ragdoll_embeddings_cosine_idx \n      ON ragdoll_embeddings USING ivfflat (embedding vector_cosine_ops) \n      WITH (lists = 100);\n    SQL\n\n    # Composite indexes for common queries\n    add_index :ragdoll_documents, [:user_id, :status, :created_at]\n    add_index :ragdoll_documents, [:content_type, :file_size]\n    add_index :ragdoll_contents, [:document_id, :chunk_index]\n\n    # Full-text search indexes\n    execute &lt;&lt;-SQL\n      CREATE INDEX CONCURRENTLY IF NOT EXISTS ragdoll_documents_search_idx \n      ON ragdoll_documents USING gin(search_vector);\n    SQL\n  end\nend\n</code></pre>"},{"location":"rails/generators/#custom-generator-templates","title":"Custom Generator Templates","text":"<p>You can customize generator templates by copying them to your application:</p> <pre><code># Copy all generator templates\nmkdir -p lib/templates/ragdoll\ncp -r $(bundle show ragdoll-rails)/lib/generators/ragdoll/templates/* lib/templates/ragdoll/\n</code></pre>"},{"location":"rails/generators/#custom-model-template","title":"Custom Model Template:","text":"<pre><code># lib/templates/ragdoll/model/model.rb.tt\nclass &lt;%= class_name %&gt; &lt; ApplicationRecord\n  include Ragdoll::Searchable\n\n  # Associations\n  belongs_to :user, optional: true\n\n  # Validations\n&lt;% attributes.each do |attribute| -%&gt;\n  &lt;% if attribute.required? -%&gt;\n  validates :&lt;%= attribute.name %&gt;, presence: true\n  &lt;% end -%&gt;\n&lt;% end -%&gt;\n\n  # Ragdoll configuration\n  ragdoll_searchable do |config|\n    config.content_field = :&lt;%= content_field || 'content' %&gt;\n    config.title_field = :&lt;%= title_field || 'title' %&gt;\n    config.metadata_fields = [&lt;%= metadata_fields.map { |f| \":#{f}\" }.join(', ') %&gt;]\n    config.chunk_size = &lt;%= chunk_size || 1000 %&gt;\n    config.auto_process = &lt;%= auto_process.nil? ? true : auto_process %&gt;\n  end\n\n  # Scopes\n  scope :by_user, -&gt;(user) { where(user: user) }\n  scope :recent, -&gt;(days = 7) { where(created_at: days.days.ago..) }\n\n  # Instance methods\n  def display_name\n    &lt;%= title_field || 'title' %&gt; || \"Untitled &lt;%= class_name %&gt;\"\n  end\nend\n</code></pre>"},{"location":"rails/generators/#generator-usage-examples","title":"Generator Usage Examples","text":""},{"location":"rails/generators/#complete-setup-workflow","title":"Complete Setup Workflow:","text":"<pre><code># 1. Install Ragdoll\nrails generate ragdoll:install\n\n# 2. Generate a document model\nrails generate ragdoll:model Document title:string content:text category:string\n\n# 3. Generate search functionality\nrails generate ragdoll:search_controller Search\n\n# 4. Copy views for customization\nrails generate ragdoll:views --only=documents,search\n\n# 5. Add performance indexes\nrails generate ragdoll:indexes\n\n# 6. Run migrations\nrails db:migrate\n\n# 7. Set up credentials\nrails generate ragdoll:credentials\nEDITOR=\"code --wait\" rails credentials:edit\n</code></pre>"},{"location":"rails/generators/#adding-search-to-existing-model","title":"Adding Search to Existing Model:","text":"<pre><code># Add Ragdoll search to existing Article model\nrails generate ragdoll:searchable Article\n\n# Generate controller with search\nrails generate ragdoll:controller Articles --with-search\n\n# Run the migration\nrails db:migrate\n\n# Process existing articles\nrails ragdoll:process_pending\n</code></pre> <p>This comprehensive generator documentation provides everything you need to quickly set up and customize Ragdoll Rails in your application using the provided generators.</p>"},{"location":"rails/installation/","title":"Rails Installation","text":"<p>This guide walks you through installing and setting up ragdoll-rails in your Ruby on Rails application.</p>"},{"location":"rails/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ruby 3.2.0 or higher</li> <li>Rails 7.0 or higher</li> <li>PostgreSQL with pgvector extension</li> <li>Redis (for background jobs)</li> </ul>"},{"location":"rails/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"rails/installation/#1-add-to-gemfile","title":"1. Add to Gemfile","text":"<pre><code># Gemfile\ngem 'ragdoll-rails'\n\n# Optional: Background job processing\ngem 'sidekiq' # or 'resque'\n</code></pre>"},{"location":"rails/installation/#2-bundle-install","title":"2. Bundle Install","text":"<pre><code>bundle install\n</code></pre>"},{"location":"rails/installation/#3-run-the-generator","title":"3. Run the Generator","text":"<p>The installer generator sets up the basic configuration:</p> <pre><code>rails generate ragdoll:install\n</code></pre> <p>This creates: - Configuration files - Database migrations - Routes - Initializers</p>"},{"location":"rails/installation/#4-configure-database","title":"4. Configure Database","text":"<p>Ensure PostgreSQL with pgvector is set up:</p> <pre><code># Install pgvector extension\npsql your_database -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\n</code></pre>"},{"location":"rails/installation/#5-run-migrations","title":"5. Run Migrations","text":"<pre><code>rails db:migrate\n</code></pre>"},{"location":"rails/installation/#6-configure-llm-provider","title":"6. Configure LLM Provider","text":"<p>Edit the generated configuration:</p> <pre><code># config/initializers/ragdoll.rb\nRagdoll.configure do |config|\n  config.llm_provider = :openai\n  config.openai_api_key = Rails.application.credentials.openai_api_key\n  config.default_embedding_model = \"text-embedding-3-small\"\n  config.default_chat_model = \"gpt-4\"\nend\n</code></pre>"},{"location":"rails/installation/#7-mount-the-engine","title":"7. Mount the Engine","text":"<p>Add to your routes file:</p> <pre><code># config/routes.rb\nRails.application.routes.draw do\n  mount Ragdoll::Engine =&gt; '/ragdoll'\n  # Your other routes...\nend\n</code></pre>"},{"location":"rails/installation/#configuration-options","title":"Configuration Options","text":""},{"location":"rails/installation/#basic-configuration","title":"Basic Configuration","text":"<pre><code># config/initializers/ragdoll.rb\nRagdoll.configure do |config|\n  # LLM Provider Settings\n  config.llm_provider = :openai\n  config.openai_api_key = Rails.application.credentials.openai_api_key\n  config.openai_base_url = \"https://api.openai.com/v1\" # Optional\n\n  # Model Settings\n  config.default_embedding_model = \"text-embedding-3-small\"\n  config.default_chat_model = \"gpt-4\"\n  config.embedding_dimensions = 1536\n\n  # Processing Settings\n  config.chunk_size = 1000\n  config.chunk_overlap = 200\n  config.max_file_size = 50.megabytes\n\n  # Search Settings\n  config.default_search_limit = 10\n  config.similarity_threshold = 0.7\n\n  # Background Job Settings\n  config.processing_queue = :ragdoll\n  config.job_adapter = :sidekiq # or :resque, :delayed_job\nend\n</code></pre>"},{"location":"rails/installation/#environment-specific-configuration","title":"Environment-Specific Configuration","text":"<pre><code># config/environments/development.rb\nconfig.after_initialize do\n  Ragdoll.configure do |ragdoll|\n    ragdoll.log_level = :debug\n    ragdoll.async_processing = false # Process immediately in development\n  end\nend\n\n# config/environments/production.rb\nconfig.after_initialize do\n  Ragdoll.configure do |ragdoll|\n    ragdoll.log_level = :info\n    ragdoll.async_processing = true\n    ragdoll.processing_queue = :ragdoll_production\n  end\nend\n</code></pre>"},{"location":"rails/installation/#background-job-setup","title":"Background Job Setup","text":""},{"location":"rails/installation/#sidekiq-setup","title":"Sidekiq Setup","text":"<pre><code># Gemfile\ngem 'sidekiq'\n\n# config/application.rb\nconfig.active_job.queue_adapter = :sidekiq\n\n# config/initializers/sidekiq.rb\nSidekiq.configure_server do |config|\n  config.redis = { url: ENV.fetch('REDIS_URL', 'redis://localhost:6379/0') }\nend\n\nSidekiq.configure_client do |config|\n  config.redis = { url: ENV.fetch('REDIS_URL', 'redis://localhost:6379/0') }\nend\n</code></pre>"},{"location":"rails/installation/#queue-configuration","title":"Queue Configuration","text":"<pre><code># config/initializers/ragdoll.rb\nRagdoll.configure do |config|\n  config.processing_queue = :ragdoll\n  config.embedding_queue = :ragdoll_embeddings\n  config.indexing_queue = :ragdoll_indexing\nend\n</code></pre>"},{"location":"rails/installation/#asset-pipeline-setup","title":"Asset Pipeline Setup","text":""},{"location":"rails/installation/#sprockets-rails-67","title":"Sprockets (Rails 6/7)","text":"<pre><code>// app/assets/javascripts/application.js\n//= require ragdoll/application\n\n// app/assets/stylesheets/application.css\n*= require ragdoll/application\n</code></pre>"},{"location":"rails/installation/#webpackershakapacker","title":"Webpacker/Shakapacker","text":"<pre><code>// app/javascript/packs/application.js\nimport 'ragdoll/application'\n\n// app/assets/stylesheets/application.scss\n@import 'ragdoll/application';\n</code></pre>"},{"location":"rails/installation/#database-configuration","title":"Database Configuration","text":""},{"location":"rails/installation/#postgresql-with-pgvector","title":"PostgreSQL with pgvector","text":"<pre><code># config/database.yml\ndefault: &amp;default\n  adapter: postgresql\n  encoding: unicode\n  pool: &lt;%= ENV.fetch(\"RAILS_MAX_THREADS\") { 5 } %&gt;\n\ndevelopment:\n  &lt;&lt;: *default\n  database: myapp_development\n  # Ensure pgvector extension is available\n\nproduction:\n  &lt;&lt;: *default\n  database: myapp_production\n  url: &lt;%= ENV['DATABASE_URL'] %&gt;\n</code></pre>"},{"location":"rails/installation/#migration-customization","title":"Migration Customization","text":"<pre><code># Override default migration if needed\nclass CustomRagdollSetup &lt; ActiveRecord::Migration[7.0]\n  def change\n    # Enable pgvector extension\n    enable_extension 'vector'\n\n    # Run standard Ragdoll migrations\n    Ragdoll::InstallGenerator.new.create_migrations\n\n    # Add custom indexes or modifications\n    add_index :ragdoll_documents, [:user_id, :created_at]\n  end\nend\n</code></pre>"},{"location":"rails/installation/#security-configuration","title":"Security Configuration","text":""},{"location":"rails/installation/#strong-parameters","title":"Strong Parameters","text":"<pre><code># config/initializers/ragdoll.rb\nRagdoll.configure do |config|\n  # Permitted file types\n  config.allowed_file_types = %w[.pdf .doc .docx .txt .md .html]\n\n  # Maximum file size\n  config.max_file_size = 50.megabytes\n\n  # Content security\n  config.sanitize_content = true\n  config.extract_metadata = true\nend\n</code></pre>"},{"location":"rails/installation/#authentication-integration","title":"Authentication Integration","text":"<pre><code># config/initializers/ragdoll.rb\nRagdoll.configure do |config|\n  # Integrate with your authentication system\n  config.current_user_method = :current_user\n  config.authenticate_user_method = :authenticate_user!\n\n  # Authorization\n  config.authorize_document_access = -&gt;(document, user) {\n    document.user == user || user.admin?\n  }\nend\n</code></pre>"},{"location":"rails/installation/#file-storage-configuration","title":"File Storage Configuration","text":""},{"location":"rails/installation/#active-storage-setup","title":"Active Storage Setup","text":"<pre><code># config/storage.yml\nlocal:\n  service: Disk\n  root: &lt;%= Rails.root.join(\"storage\") %&gt;\n\nproduction:\n  service: S3\n  access_key_id: &lt;%= Rails.application.credentials.aws[:access_key_id] %&gt;\n  secret_access_key: &lt;%= Rails.application.credentials.aws[:secret_access_key] %&gt;\n  region: us-east-1\n  bucket: your-bucket-name\n\n# config/environments/production.rb\nconfig.active_storage.variant_processor = :mini_magick\n</code></pre>"},{"location":"rails/installation/#storage-configuration","title":"Storage Configuration","text":"<pre><code># config/initializers/ragdoll.rb\nRagdoll.configure do |config|\n  # File storage options\n  config.storage_service = :local # or :s3, :gcs, etc.\n  config.preserve_original_files = true\n  config.generate_thumbnails = true\n  config.thumbnail_sizes = { small: '150x150', medium: '300x300', large: '600x600' }\nend\n</code></pre>"},{"location":"rails/installation/#verification","title":"Verification","text":""},{"location":"rails/installation/#test-the-installation","title":"Test the Installation","text":"<pre><code># In Rails console\nrails console\n\n# Test configuration\nRagdoll.configuration.llm_provider\n# =&gt; :openai\n\n# Test model creation\ndoc = Ragdoll::Document.create!(\n  title: \"Test Document\",\n  content: \"This is a test document for verification.\"\n)\n\n# Test search\nRagdoll::Document.search(\"test\")\n</code></pre>"},{"location":"rails/installation/#health-check","title":"Health Check","text":"<p>Create a health check endpoint:</p> <pre><code># config/routes.rb\nget '/health/ragdoll', to: 'health#ragdoll'\n\n# app/controllers/health_controller.rb\nclass HealthController &lt; ApplicationController\n  def ragdoll\n    checks = {\n      database: check_database,\n      llm_provider: check_llm_provider,\n      background_jobs: check_background_jobs\n    }\n\n    render json: { status: checks.values.all? ? 'healthy' : 'unhealthy', checks: checks }\n  end\n\n  private\n\n  def check_database\n    Ragdoll::Document.connection.active?\n  rescue\n    false\n  end\n\n  def check_llm_provider\n    Ragdoll::LLMService.new.health_check\n  rescue\n    false\n  end\n\n  def check_background_jobs\n    Sidekiq.redis(&amp;:ping) == 'PONG'\n  rescue\n    false\n  end\nend\n</code></pre>"},{"location":"rails/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"rails/installation/#common-issues","title":"Common Issues","text":""},{"location":"rails/installation/#pgvector-extension-missing","title":"pgvector Extension Missing","text":"<pre><code># Install on Ubuntu/Debian\nsudo apt install postgresql-14-pgvector\n\n# Install on macOS with Homebrew\nbrew install pgvector\n\n# Enable in database\npsql your_database -c \"CREATE EXTENSION vector;\"\n</code></pre>"},{"location":"rails/installation/#asset-compilation-issues","title":"Asset Compilation Issues","text":"<pre><code># Precompile assets\nrails assets:precompile\n\n# Clear asset cache\nrails assets:clobber\n</code></pre>"},{"location":"rails/installation/#background-job-issues","title":"Background Job Issues","text":"<pre><code># Check Sidekiq status\nbundle exec sidekiq\n\n# Monitor queues\nbundle exec sidekiq-web\n</code></pre>"},{"location":"rails/installation/#configuration-validation","title":"Configuration Validation","text":"<pre><code># config/initializers/ragdoll.rb\nRagdoll.configure do |config|\n  # ... your configuration\n\n  # Validate configuration on startup\n  config.validate_on_startup = true\nend\n</code></pre>"},{"location":"rails/installation/#logging","title":"Logging","text":"<pre><code># config/initializers/ragdoll.rb\nRagdoll.configure do |config|\n  config.logger = Rails.logger\n  config.log_level = Rails.env.production? ? :info : :debug\nend\n</code></pre>"},{"location":"rails/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Detailed configuration options</li> <li>Models - Learn about the ActiveRecord models</li> <li>Controllers - Understand the provided controllers</li> <li>Examples - See implementation examples</li> </ul>"},{"location":"rails/models/","title":"Rails Models","text":"<p>The ragdoll-rails gem provides a comprehensive set of ActiveRecord models for managing documents, content, embeddings, and search functionality within your Rails application.</p>"},{"location":"rails/models/#model-overview","title":"Model Overview","text":"<p>The Ragdoll Rails models follow Rails conventions and provide a clean, intuitive API for document management and search:</p> <pre><code>erDiagram\n    Document ||--o{ Content : contains\n    Content ||--o{ Embedding : has\n    Document ||--o{ DocumentMetadata : has\n    User ||--o{ Document : owns\n    Document ||--o{ SearchResult : appears_in\n\n    Document {\n        uuid id\n        string title\n        text description\n        string file_path\n        string content_type\n        integer file_size\n        string status\n        json metadata\n        uuid user_id\n        timestamps created_at\n        timestamps updated_at\n    }\n\n    Content {\n        uuid id\n        uuid document_id\n        text content\n        integer chunk_index\n        integer start_position\n        integer end_position\n        string content_type\n        json metadata\n        timestamps created_at\n    }\n\n    Embedding {\n        uuid id\n        uuid content_id\n        vector embedding\n        string model_name\n        integer dimensions\n        timestamps created_at\n    }</code></pre>"},{"location":"rails/models/#core-models","title":"Core Models","text":""},{"location":"rails/models/#ragdolldocument","title":"Ragdoll::Document","text":"<p>The central model representing uploaded documents and their metadata.</p>"},{"location":"rails/models/#attributes","title":"Attributes","text":"<pre><code>class Ragdoll::Document &lt; ApplicationRecord\n  # Core attributes\n  attribute :id, :uuid, default: -&gt; { SecureRandom.uuid }\n  attribute :title, :string\n  attribute :description, :text\n  attribute :file_path, :string\n  attribute :content_type, :string\n  attribute :file_size, :integer\n  attribute :status, :string, default: 'pending'\n  attribute :metadata, :json, default: {}\n  attribute :user_id, :uuid\n\n  # Processing attributes\n  attribute :processed_at, :datetime\n  attribute :processing_errors, :json, default: []\n  attribute :content_hash, :string\n\n  # Search attributes\n  attribute :search_vector, :tsvector\n  attribute :language, :string, default: 'en'\nend\n</code></pre>"},{"location":"rails/models/#associations","title":"Associations","text":"<pre><code>class Ragdoll::Document &lt; ApplicationRecord\n  # User association (polymorphic by default)\n  belongs_to :user, polymorphic: true, optional: true\n\n  # Content associations\n  has_many :contents, dependent: :destroy\n  has_many :embeddings, through: :contents\n\n  # File attachment\n  has_one_attached :file\n  has_many_attached :images\n\n  # Metadata\n  has_many :document_metadata, dependent: :destroy\nend\n</code></pre>"},{"location":"rails/models/#scopes-and-finders","title":"Scopes and Finders","text":"<pre><code>class Ragdoll::Document &lt; ApplicationRecord\n  # Status scopes\n  scope :pending, -&gt; { where(status: 'pending') }\n  scope :processing, -&gt; { where(status: 'processing') }\n  scope :processed, -&gt; { where(status: 'processed') }\n  scope :failed, -&gt; { where(status: 'failed') }\n\n  # Content type scopes\n  scope :pdfs, -&gt; { where(content_type: 'application/pdf') }\n  scope :images, -&gt; { where(content_type: /^image\\//) }\n  scope :text_files, -&gt; { where(content_type: /^text\\//) }\n\n  # Date scopes\n  scope :recent, -&gt;(days = 7) { where(created_at: days.days.ago..) }\n  scope :by_date, -&gt;(date) { where(created_at: date.beginning_of_day..date.end_of_day) }\n\n  # User scopes\n  scope :by_user, -&gt;(user) { where(user: user) }\n  scope :public_documents, -&gt; { where(metadata: { visibility: 'public' }) }\nend\n</code></pre>"},{"location":"rails/models/#instance-methods","title":"Instance Methods","text":"<pre><code>class Ragdoll::Document &lt; ApplicationRecord\n  # Status methods\n  def pending?\n    status == 'pending'\n  end\n\n  def processed?\n    status == 'processed'\n  end\n\n  def failed?\n    status == 'failed'\n  end\n\n  # Content methods\n  def full_content\n    contents.order(:chunk_index).pluck(:content).join('\\n')\n  end\n\n  def content_preview(length = 500)\n    full_content.truncate(length)\n  end\n\n  # File methods\n  def file_extension\n    File.extname(file_path).downcase if file_path\n  end\n\n  def human_file_size\n    ActionController::Base.helpers.number_to_human_size(file_size)\n  end\n\n  # Metadata methods\n  def get_metadata(key)\n    metadata[key.to_s]\n  end\n\n  def set_metadata(key, value)\n    metadata[key.to_s] = value\n    save!\n  end\n\n  def add_metadata(hash)\n    self.metadata = metadata.merge(hash.stringify_keys)\n    save!\n  end\nend\n</code></pre>"},{"location":"rails/models/#class-methods","title":"Class Methods","text":"<pre><code>class Ragdoll::Document &lt; ApplicationRecord\n  # Search methods\n  def self.search(query, options = {})\n    Ragdoll::SearchService.new(query, options).call\n  end\n\n  def self.semantic_search(query, limit: 10, threshold: 0.7)\n    Ragdoll::SemanticSearchService.new(query, limit: limit, threshold: threshold).call\n  end\n\n  def self.keyword_search(query, limit: 10)\n    where(\"search_vector @@ plainto_tsquery(?)\", query).limit(limit)\n  end\n\n  # Bulk operations\n  def self.bulk_process(document_ids)\n    where(id: document_ids).find_each do |document|\n      Ragdoll::ProcessDocumentJob.perform_later(document)\n    end\n  end\n\n  def self.reindex_all\n    processed.find_each do |document|\n      Ragdoll::IndexDocumentJob.perform_later(document)\n    end\n  end\nend\n</code></pre>"},{"location":"rails/models/#ragdollcontent","title":"Ragdoll::Content","text":"<p>Represents chunked content extracted from documents.</p>"},{"location":"rails/models/#attributes_1","title":"Attributes","text":"<pre><code>class Ragdoll::Content &lt; ApplicationRecord\n  attribute :id, :uuid, default: -&gt; { SecureRandom.uuid }\n  attribute :document_id, :uuid\n  attribute :content, :text\n  attribute :chunk_index, :integer\n  attribute :start_position, :integer\n  attribute :end_position, :integer\n  attribute :content_type, :string, default: 'text/plain'\n  attribute :metadata, :json, default: {}\n  attribute :language, :string\n  attribute :word_count, :integer\n  attribute :character_count, :integer\nend\n</code></pre>"},{"location":"rails/models/#associations-and-methods","title":"Associations and Methods","text":"<pre><code>class Ragdoll::Content &lt; ApplicationRecord\n  belongs_to :document\n  has_many :embeddings, dependent: :destroy\n\n  # Scopes\n  scope :by_document, -&gt;(document) { where(document: document) }\n  scope :ordered, -&gt; { order(:chunk_index) }\n  scope :with_embeddings, -&gt; { joins(:embeddings) }\n\n  # Instance methods\n  def summary(length = 150)\n    content.truncate(length)\n  end\n\n  def has_embedding?\n    embeddings.any?\n  end\n\n  def primary_embedding\n    embeddings.first\n  end\n\n  # Content analysis\n  def calculate_statistics!\n    self.word_count = content.split.size\n    self.character_count = content.length\n    save!\n  end\nend\n</code></pre>"},{"location":"rails/models/#ragdollembedding","title":"Ragdoll::Embedding","text":"<p>Stores vector embeddings for content chunks.</p>"},{"location":"rails/models/#attributes_2","title":"Attributes","text":"<pre><code>class Ragdoll::Embedding &lt; ApplicationRecord\n  attribute :id, :uuid, default: -&gt; { SecureRandom.uuid }\n  attribute :content_id, :uuid\n  attribute :embedding, :vector\n  attribute :model_name, :string\n  attribute :dimensions, :integer\n  attribute :created_at, :datetime\nend\n</code></pre>"},{"location":"rails/models/#methods","title":"Methods","text":"<pre><code>class Ragdoll::Embedding &lt; ApplicationRecord\n  belongs_to :content\n  has_one :document, through: :content\n\n  # Validations\n  validates :embedding, presence: true\n  validates :model_name, presence: true\n  validates :dimensions, presence: true, numericality: { greater_than: 0 }\n\n  # Scopes\n  scope :by_model, -&gt;(model) { where(model_name: model) }\n  scope :recent, -&gt;(days = 30) { where(created_at: days.days.ago..) }\n\n  # Search methods\n  def self.similar_to(vector, limit: 10, threshold: 0.7)\n    where(\"embedding &lt;-&gt; ? &lt; ?\", vector, 1 - threshold)\n      .order(\"embedding &lt;-&gt; ?\", vector)\n      .limit(limit)\n  end\n\n  def self.cosine_similarity(vector, limit: 10)\n    select(\"*, (embedding &lt;=&gt; ?) AS similarity\", vector)\n      .order(\"similarity DESC\")\n      .limit(limit)\n  end\n\n  # Instance methods\n  def similarity_to(other_vector)\n    # Calculate cosine similarity\n    connection.select_value(\n      \"SELECT 1 - (embedding &lt;=&gt; ?::vector) AS similarity\",\n      other_vector\n    )\n  end\n\n  def vector_magnitude\n    Math.sqrt(embedding.sum { |x| x * x })\n  end\nend\n</code></pre>"},{"location":"rails/models/#searchable-concern","title":"Searchable Concern","text":"<p>The <code>Ragdoll::Searchable</code> concern can be included in your own models to add search capabilities:</p> <pre><code>class Article &lt; ApplicationRecord\n  include Ragdoll::Searchable\n\n  # Configure searchable attributes\n  ragdoll_searchable do |config|\n    config.content_field = :body\n    config.title_field = :title\n    config.metadata_fields = [:category, :tags, :author]\n    config.chunk_size = 1000\n    config.auto_process = true\n  end\nend\n</code></pre>"},{"location":"rails/models/#searchable-configuration","title":"Searchable Configuration","text":"<pre><code>class BlogPost &lt; ApplicationRecord\n  include Ragdoll::Searchable\n\n  ragdoll_searchable do |config|\n    # Content configuration\n    config.content_field = :content\n    config.title_field = :title\n    config.description_field = :excerpt\n\n    # Processing configuration\n    config.chunk_size = 800\n    config.chunk_overlap = 100\n    config.auto_process = true\n    config.process_on_create = true\n    config.process_on_update = true\n\n    # Metadata configuration\n    config.metadata_fields = [:category, :tags, :status]\n    config.custom_metadata = -&gt;(record) {\n      {\n        author: record.user.name,\n        published_at: record.published_at,\n        word_count: record.content.split.size\n      }\n    }\n\n    # Search configuration\n    config.searchable = true\n    config.enable_suggestions = true\n    config.boost_factor = 1.0\n  end\nend\n</code></pre>"},{"location":"rails/models/#generated-methods","title":"Generated Methods","text":"<p>When you include <code>Ragdoll::Searchable</code>, your model gains these methods:</p> <pre><code># Search methods\nBlogPost.search(\"machine learning\")\nBlogPost.semantic_search(\"AI algorithms\", limit: 5)\n\n# Processing methods\npost = BlogPost.create!(title: \"My Post\", content: \"Content here\")\npost.process_for_search!\npost.reindex!\n\n# Status methods\npost.ragdoll_processed?\npost.ragdoll_processing?\npost.ragdoll_failed?\n\n# Content methods\npost.ragdoll_contents\npost.ragdoll_embeddings\npost.ragdoll_search_preview\n</code></pre>"},{"location":"rails/models/#model-callbacks","title":"Model Callbacks","text":""},{"location":"rails/models/#document-callbacks","title":"Document Callbacks","text":"<pre><code>class Ragdoll::Document &lt; ApplicationRecord\n  # Processing callbacks\n  after_create :enqueue_processing, if: :should_auto_process?\n  after_update :enqueue_reprocessing, if: :file_changed?\n  before_destroy :cleanup_attachments\n\n  # Search callbacks\n  after_save :update_search_vector, if: :title_or_description_changed?\n  after_touch :touch_embeddings\n\n  private\n\n  def should_auto_process?\n    file.attached? &amp;&amp; Ragdoll.configuration.auto_process_documents\n  end\n\n  def file_changed?\n    saved_change_to_file_path? || file.attachment.changed?\n  end\n\n  def title_or_description_changed?\n    saved_change_to_title? || saved_change_to_description?\n  end\n\n  def cleanup_attachments\n    file.purge_later if file.attached?\n    images.purge_later if images.any?\n  end\n\n  def update_search_vector\n    self.update_column(:search_vector, \n      \"to_tsvector('#{language}', '#{title} #{description}')\"\n    )\n  end\n\n  def touch_embeddings\n    embeddings.touch_all\n  end\nend\n</code></pre>"},{"location":"rails/models/#validations","title":"Validations","text":""},{"location":"rails/models/#document-validations","title":"Document Validations","text":"<pre><code>class Ragdoll::Document &lt; ApplicationRecord\n  # Required fields\n  validates :title, presence: true, length: { maximum: 255 }\n  validates :status, presence: true, inclusion: { \n    in: %w[pending processing processed failed] \n  }\n\n  # File validations\n  validates :file, presence: true, on: :create\n  validate :file_type_allowed\n  validate :file_size_within_limit\n\n  # Content validations\n  validates :content_type, presence: true\n  validates :file_size, presence: true, numericality: { greater_than: 0 }\n\n  # User validations\n  validates :user, presence: true, if: :require_user?\n\n  private\n\n  def file_type_allowed\n    return unless file.attached?\n\n    allowed_types = Ragdoll.configuration.allowed_file_types\n    file_type = file.content_type\n\n    unless allowed_types.include?(file_type)\n      errors.add(:file, \"type #{file_type} is not allowed\")\n    end\n  end\n\n  def file_size_within_limit\n    return unless file.attached?\n\n    max_size = Ragdoll.configuration.max_file_size\n\n    if file.byte_size &gt; max_size\n      errors.add(:file, \"size exceeds maximum allowed (#{max_size} bytes)\")\n    end\n  end\n\n  def require_user?\n    Ragdoll.configuration.require_user_association\n  end\nend\n</code></pre>"},{"location":"rails/models/#custom-model-extensions","title":"Custom Model Extensions","text":""},{"location":"rails/models/#adding-custom-attributes","title":"Adding Custom Attributes","text":"<pre><code># Create a migration to add custom fields\nclass AddCustomFieldsToRagdollDocuments &lt; ActiveRecord::Migration[7.0]\n  def change\n    add_column :ragdoll_documents, :category, :string\n    add_column :ragdoll_documents, :priority, :integer, default: 0\n    add_column :ragdoll_documents, :expires_at, :datetime\n    add_index :ragdoll_documents, :category\n    add_index :ragdoll_documents, :priority\n  end\nend\n\n# Extend the model\nRagdoll::Document.class_eval do\n  # Add custom validations\n  validates :category, presence: true\n  validates :priority, inclusion: { in: 0..10 }\n\n  # Add custom scopes\n  scope :by_category, -&gt;(cat) { where(category: cat) }\n  scope :high_priority, -&gt; { where(priority: 8..10) }\n  scope :active, -&gt; { where('expires_at IS NULL OR expires_at &gt; ?', Time.current) }\n\n  # Add custom methods\n  def expired?\n    expires_at &amp;&amp; expires_at &lt; Time.current\n  end\n\n  def high_priority?\n    priority &gt;= 8\n  end\nend\n</code></pre>"},{"location":"rails/models/#custom-metadata-handling","title":"Custom Metadata Handling","text":"<pre><code>class Ragdoll::Document &lt; ApplicationRecord\n  # Typed metadata accessors\n  def self.define_metadata_accessor(name, type = :string)\n    define_method(name) do\n      value = metadata[name.to_s]\n      case type\n      when :integer then value&amp;.to_i\n      when :float then value&amp;.to_f\n      when :boolean then !!value\n      when :date then value ? Date.parse(value) : nil\n      when :datetime then value ? DateTime.parse(value) : nil\n      else value\n      end\n    end\n\n    define_method(\"#{name}=\") do |value|\n      metadata[name.to_s] = value\n    end\n  end\n\n  # Define typed metadata fields\n  define_metadata_accessor :department\n  define_metadata_accessor :confidence_score, :float\n  define_metadata_accessor :is_public, :boolean\n  define_metadata_accessor :published_at, :datetime\nend\n\n# Usage\ndocument.department = \"Engineering\"\ndocument.confidence_score = 0.95\ndocument.is_public = true\ndocument.published_at = 1.week.ago\ndocument.save!\n</code></pre>"},{"location":"rails/models/#performance-considerations","title":"Performance Considerations","text":""},{"location":"rails/models/#database-indexes","title":"Database Indexes","text":"<pre><code># Create custom indexes for better performance\nclass AddPerformanceIndexesToRagdollModels &lt; ActiveRecord::Migration[7.0]\n  def change\n    # Document indexes\n    add_index :ragdoll_documents, [:user_id, :status]\n    add_index :ragdoll_documents, [:content_type, :created_at]\n    add_index :ragdoll_documents, :file_size\n\n    # Content indexes\n    add_index :ragdoll_contents, [:document_id, :chunk_index]\n    add_index :ragdoll_contents, :word_count\n\n    # Embedding indexes\n    add_index :ragdoll_embeddings, [:content_id, :model_name]\n    add_index :ragdoll_embeddings, :created_at\n\n    # Vector similarity index (requires pgvector)\n    execute \"CREATE INDEX CONCURRENTLY IF NOT EXISTS ragdoll_embeddings_embedding_idx \n             ON ragdoll_embeddings USING ivfflat (embedding vector_cosine_ops) \n             WITH (lists = 100)\"\n  end\nend\n</code></pre>"},{"location":"rails/models/#query-optimization","title":"Query Optimization","text":"<pre><code>class Ragdoll::Document &lt; ApplicationRecord\n  # Optimized includes for search results\n  scope :with_search_data, -&gt; { \n    includes(:contents, :embeddings, :user)\n      .select('ragdoll_documents.*, COUNT(ragdoll_contents.id) as content_count')\n      .left_joins(:contents)\n      .group('ragdoll_documents.id')\n  }\n\n  # Efficient content loading\n  def preload_search_content\n    contents.includes(:embeddings).limit(3)\n  end\n\n  # Cached expensive operations\n  def cached_content_preview\n    Rails.cache.fetch(\"document:#{id}:preview\", expires_in: 1.hour) do\n      content_preview\n    end\n  end\nend\n</code></pre> <p>This comprehensive model documentation provides everything you need to work with Ragdoll Rails models effectively, from basic usage to advanced customization and performance optimization.</p>"},{"location":"rails/views-helpers/","title":"Views &amp; Helpers","text":"<p>The ragdoll-rails gem provides a comprehensive set of view templates, components, and helper methods to quickly build document management and search interfaces in your Rails application.</p>"},{"location":"rails/views-helpers/#view-templates","title":"View Templates","text":""},{"location":"rails/views-helpers/#document-views","title":"Document Views","text":""},{"location":"rails/views-helpers/#document-index-documentsindexhtmlerb","title":"Document Index (<code>documents/index.html.erb</code>)","text":"<pre><code>&lt;div class=\"ragdoll-documents-index\"&gt;\n  &lt;div class=\"documents-header\"&gt;\n    &lt;h1&gt;Documents&lt;/h1&gt;\n    &lt;div class=\"documents-actions\"&gt;\n      &lt;%= link_to \"Upload Document\", new_document_path, class: \"btn btn-primary\" %&gt;\n      &lt;%= link_to \"Bulk Upload\", bulk_upload_documents_path, class: \"btn btn-secondary\" %&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"documents-filters\"&gt;\n    &lt;%= form_with url: documents_path, method: :get, local: true, class: \"filter-form\" do |form| %&gt;\n      &lt;div class=\"filter-group\"&gt;\n        &lt;%= form.select :status, options_for_select([\n          ['All Statuses', ''],\n          ['Processed', 'processed'],\n          ['Processing', 'processing'],\n          ['Pending', 'pending'],\n          ['Failed', 'failed']\n        ], params[:status]), {}, { class: \"form-select\" } %&gt;\n      &lt;/div&gt;\n\n      &lt;div class=\"filter-group\"&gt;\n        &lt;%= form.select :content_type, content_type_options, {}, { class: \"form-select\" } %&gt;\n      &lt;/div&gt;\n\n      &lt;div class=\"filter-group\"&gt;\n        &lt;%= form.submit \"Filter\", class: \"btn btn-outline-primary\" %&gt;\n      &lt;/div&gt;\n    &lt;% end %&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"documents-grid\"&gt;\n    &lt;% @documents.each do |document| %&gt;\n      &lt;%= render 'document_card', document: document %&gt;\n    &lt;% end %&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"documents-pagination\"&gt;\n    &lt;%= paginate @documents %&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"rails/views-helpers/#document-card-documents_document_cardhtmlerb","title":"Document Card (<code>documents/_document_card.html.erb</code>)","text":"<pre><code>&lt;div class=\"document-card\" data-document-id=\"&lt;%= document.id %&gt;\"&gt;\n  &lt;div class=\"document-thumbnail\"&gt;\n    &lt;% if document.file.attached? &amp;&amp; document.file.representable? %&gt;\n      &lt;%= image_tag document.file.representation(resize_to_limit: [200, 150]), \n                    alt: document.title, class: \"thumbnail-image\" %&gt;\n    &lt;% else %&gt;\n      &lt;div class=\"file-type-icon\"&gt;\n        &lt;%= ragdoll_file_icon(document.content_type) %&gt;\n      &lt;/div&gt;\n    &lt;% end %&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"document-content\"&gt;\n    &lt;h3 class=\"document-title\"&gt;\n      &lt;%= link_to document.title, document_path(document) %&gt;\n    &lt;/h3&gt;\n\n    &lt;p class=\"document-description\"&gt;\n      &lt;%= truncate(document.description, length: 100) %&gt;\n    &lt;/p&gt;\n\n    &lt;div class=\"document-meta\"&gt;\n      &lt;span class=\"file-size\"&gt;&lt;%= human_file_size(document.file_size) %&gt;&lt;/span&gt;\n      &lt;span class=\"upload-date\"&gt;&lt;%= time_ago_in_words(document.created_at) %&gt; ago&lt;/span&gt;\n      &lt;span class=\"status-badge status-&lt;%= document.status %&gt;\"&gt;\n        &lt;%= document.status.humanize %&gt;\n      &lt;/span&gt;\n    &lt;/div&gt;\n\n    &lt;div class=\"document-actions\"&gt;\n      &lt;%= link_to \"View\", document_path(document), class: \"btn btn-sm btn-outline-primary\" %&gt;\n      &lt;% if can_edit_document?(document) %&gt;\n        &lt;%= link_to \"Edit\", edit_document_path(document), class: \"btn btn-sm btn-outline-secondary\" %&gt;\n      &lt;% end %&gt;\n      &lt;% if can_delete_document?(document) %&gt;\n        &lt;%= link_to \"Delete\", document_path(document), method: :delete,\n                    confirm: \"Are you sure?\", class: \"btn btn-sm btn-outline-danger\" %&gt;\n      &lt;% end %&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"rails/views-helpers/#document-show-documentsshowhtmlerb","title":"Document Show (<code>documents/show.html.erb</code>)","text":"<pre><code>&lt;div class=\"ragdoll-document-show\"&gt;\n  &lt;div class=\"document-header\"&gt;\n    &lt;div class=\"document-info\"&gt;\n      &lt;h1&gt;&lt;%= @document.title %&gt;&lt;/h1&gt;\n      &lt;p class=\"document-description\"&gt;&lt;%= @document.description %&gt;&lt;/p&gt;\n\n      &lt;div class=\"document-metadata\"&gt;\n        &lt;div class=\"meta-item\"&gt;\n          &lt;strong&gt;File Type:&lt;/strong&gt; &lt;%= @document.content_type %&gt;\n        &lt;/div&gt;\n        &lt;div class=\"meta-item\"&gt;\n          &lt;strong&gt;File Size:&lt;/strong&gt; &lt;%= human_file_size(@document.file_size) %&gt;\n        &lt;/div&gt;\n        &lt;div class=\"meta-item\"&gt;\n          &lt;strong&gt;Status:&lt;/strong&gt; \n          &lt;span class=\"status-badge status-&lt;%= @document.status %&gt;\"&gt;\n            &lt;%= @document.status.humanize %&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n        &lt;div class=\"meta-item\"&gt;\n          &lt;strong&gt;Uploaded:&lt;/strong&gt; &lt;%= @document.created_at.strftime(\"%B %d, %Y at %I:%M %p\") %&gt;\n        &lt;/div&gt;\n        &lt;% if @document.user %&gt;\n          &lt;div class=\"meta-item\"&gt;\n            &lt;strong&gt;Uploaded by:&lt;/strong&gt; &lt;%= @document.user.name %&gt;\n          &lt;/div&gt;\n        &lt;% end %&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;div class=\"document-actions\"&gt;\n      &lt;%= link_to \"Download\", document_download_path(@document), \n                  class: \"btn btn-primary\" %&gt;\n      &lt;% if can_edit_document?(@document) %&gt;\n        &lt;%= link_to \"Edit\", edit_document_path(@document), \n                    class: \"btn btn-secondary\" %&gt;\n      &lt;% end %&gt;\n      &lt;% if can_reprocess_document?(@document) %&gt;\n        &lt;%= link_to \"Reprocess\", reprocess_document_path(@document), \n                    method: :post, class: \"btn btn-outline-secondary\" %&gt;\n      &lt;% end %&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"document-content\"&gt;\n    &lt;div class=\"content-preview\"&gt;\n      &lt;h3&gt;Content Preview&lt;/h3&gt;\n      &lt;div class=\"preview-text\"&gt;\n        &lt;%= simple_format(@content_preview) %&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;% if @document.metadata.any? %&gt;\n      &lt;div class=\"document-custom-metadata\"&gt;\n        &lt;h3&gt;Metadata&lt;/h3&gt;\n        &lt;dl class=\"metadata-list\"&gt;\n          &lt;% @document.metadata.each do |key, value| %&gt;\n            &lt;dt&gt;&lt;%= key.humanize %&gt;&lt;/dt&gt;\n            &lt;dd&gt;&lt;%= value %&gt;&lt;/dd&gt;\n          &lt;% end %&gt;\n        &lt;/dl&gt;\n      &lt;/div&gt;\n    &lt;% end %&gt;\n  &lt;/div&gt;\n\n  &lt;% if @related_documents.any? %&gt;\n    &lt;div class=\"related-documents\"&gt;\n      &lt;h3&gt;Related Documents&lt;/h3&gt;\n      &lt;div class=\"related-grid\"&gt;\n        &lt;% @related_documents.each do |doc| %&gt;\n          &lt;%= render 'document_card', document: doc %&gt;\n        &lt;% end %&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;% end %&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"rails/views-helpers/#search-views","title":"Search Views","text":""},{"location":"rails/views-helpers/#search-interface-searchindexhtmlerb","title":"Search Interface (<code>search/index.html.erb</code>)","text":"<pre><code>&lt;div class=\"ragdoll-search\"&gt;\n  &lt;div class=\"search-header\"&gt;\n    &lt;h1&gt;Search Documents&lt;/h1&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"search-form\"&gt;\n    &lt;%= form_with url: search_path, method: :get, local: true, class: \"search-form-wrapper\" do |form| %&gt;\n      &lt;div class=\"search-input-group\"&gt;\n        &lt;%= form.text_field :q, value: params[:q], \n                           placeholder: \"Search documents...\", \n                           class: \"search-input\",\n                           data: { \n                             action: \"input-&gt;search#suggest\",\n                             search_target: \"input\"\n                           } %&gt;\n        &lt;%= form.submit \"Search\", class: \"search-btn\" %&gt;\n      &lt;/div&gt;\n\n      &lt;div class=\"search-options\"&gt;\n        &lt;%= form.select :search_type, [\n          ['Hybrid Search', 'hybrid'],\n          ['Semantic Search', 'semantic'],\n          ['Keyword Search', 'keyword']\n        ], { selected: params[:search_type] || 'hybrid' }, { class: \"search-type-select\" } %&gt;\n\n        &lt;%= form.check_box :include_facets, checked: params[:include_facets] %&gt;\n        &lt;%= form.label :include_facets, \"Show filters\" %&gt;\n      &lt;/div&gt;\n    &lt;% end %&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"search-suggestions\" data-search-target=\"suggestions\" style=\"display: none;\"&gt;\n    &lt;!-- Populated via Stimulus --&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"search-results-container\"&gt;\n    &lt;% if @results.present? %&gt;\n      &lt;div class=\"search-meta\"&gt;\n        &lt;p&gt;Found &lt;%= pluralize(@results.total_count, 'result') %&gt; \n           &lt;% if params[:q].present? %&gt;for \"&lt;%= params[:q] %&gt;\"&lt;% end %&gt;\n           in &lt;%= number_with_precision(@search_service.search_time, precision: 3) %&gt;s\n        &lt;/p&gt;\n      &lt;/div&gt;\n\n      &lt;div class=\"search-content\"&gt;\n        &lt;% if @facets.present? %&gt;\n          &lt;div class=\"search-facets\"&gt;\n            &lt;%= render 'search_facets', facets: @facets %&gt;\n          &lt;/div&gt;\n        &lt;% end %&gt;\n\n        &lt;div class=\"search-results\"&gt;\n          &lt;% @results.each do |result| %&gt;\n            &lt;%= render 'search_result', result: result %&gt;\n          &lt;% end %&gt;\n\n          &lt;div class=\"search-pagination\"&gt;\n            &lt;%= paginate @results %&gt;\n          &lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;% elsif params[:q].present? %&gt;\n      &lt;div class=\"no-results\"&gt;\n        &lt;h3&gt;No results found&lt;/h3&gt;\n        &lt;p&gt;Try adjusting your search terms or filters.&lt;/p&gt;\n\n        &lt;% if @suggestions.present? %&gt;\n          &lt;div class=\"search-suggestions\"&gt;\n            &lt;p&gt;Did you mean:&lt;/p&gt;\n            &lt;ul&gt;\n              &lt;% @suggestions.each do |suggestion| %&gt;\n                &lt;li&gt;\n                  &lt;%= link_to suggestion, search_path(q: suggestion) %&gt;\n                &lt;/li&gt;\n              &lt;% end %&gt;\n            &lt;/ul&gt;\n          &lt;/div&gt;\n        &lt;% end %&gt;\n      &lt;/div&gt;\n    &lt;% end %&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"rails/views-helpers/#search-result-search_search_resulthtmlerb","title":"Search Result (<code>search/_search_result.html.erb</code>)","text":"<pre><code>&lt;div class=\"search-result\" data-result-id=\"&lt;%= result.id %&gt;\"&gt;\n  &lt;div class=\"result-header\"&gt;\n    &lt;h3 class=\"result-title\"&gt;\n      &lt;%= link_to result.title, document_path(result) %&gt;\n    &lt;/h3&gt;\n    &lt;div class=\"result-meta\"&gt;\n      &lt;span class=\"similarity-score\"&gt;\n        &lt;%= number_to_percentage(result.similarity_score * 100, precision: 1) %&gt; match\n      &lt;/span&gt;\n      &lt;span class=\"file-type\"&gt;&lt;%= result.content_type %&gt;&lt;/span&gt;\n      &lt;span class=\"upload-date\"&gt;&lt;%= result.created_at.strftime(\"%b %d, %Y\") %&gt;&lt;/span&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"result-content\"&gt;\n    &lt;p class=\"result-description\"&gt;&lt;%= result.description %&gt;&lt;/p&gt;\n    &lt;div class=\"result-preview\"&gt;\n      &lt;%= highlight(result.content_preview, params[:q]) %&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"result-actions\"&gt;\n    &lt;%= link_to \"View Document\", document_path(result), class: \"btn btn-sm btn-primary\" %&gt;\n    &lt;%= link_to \"Download\", document_download_path(result), class: \"btn btn-sm btn-outline-secondary\" %&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"rails/views-helpers/#view-helpers","title":"View Helpers","text":""},{"location":"rails/views-helpers/#ragdollapplicationhelper","title":"Ragdoll::ApplicationHelper","text":"<pre><code>module Ragdoll::ApplicationHelper\n  # File type and size helpers\n  def ragdoll_file_icon(content_type)\n    icon_class = case content_type\n                when /^image\\// then \"bi-file-earmark-image\"\n                when /pdf/ then \"bi-file-earmark-pdf\"\n                when /word|doc/ then \"bi-file-earmark-word\"\n                when /excel|sheet/ then \"bi-file-earmark-excel\"\n                when /powerpoint|presentation/ then \"bi-file-earmark-ppt\"\n                when /^text\\// then \"bi-file-earmark-text\"\n                when /^audio\\// then \"bi-file-earmark-music\"\n                when /^video\\// then \"bi-file-earmark-play\"\n                else \"bi-file-earmark\"\n                end\n\n    content_tag :i, \"\", class: \"#{icon_class} file-icon\"\n  end\n\n  def human_file_size(size)\n    return \"0 bytes\" if size.nil? || size.zero?\n\n    number_to_human_size(size, precision: 1)\n  end\n\n  def content_type_options\n    types = Ragdoll::Document.distinct.pluck(:content_type).compact.sort\n    options = [['All Types', '']]\n\n    types.each do |type|\n      display_name = case type\n                    when /^image\\// then \"Images\"\n                    when /pdf/ then \"PDFs\"  \n                    when /word|doc/ then \"Word Documents\"\n                    when /excel|sheet/ then \"Spreadsheets\"\n                    when /powerpoint|presentation/ then \"Presentations\"\n                    when /^text\\// then \"Text Files\"\n                    else type.humanize\n                    end\n      options &lt;&lt; [display_name, type]\n    end\n\n    options_for_select(options.uniq, params[:content_type])\n  end\n\n  # Status helpers\n  def status_badge(status)\n    css_class = case status\n               when 'processed' then 'badge-success'\n               when 'processing' then 'badge-warning'\n               when 'pending' then 'badge-secondary'\n               when 'failed' then 'badge-danger'\n               else 'badge-light'\n               end\n\n    content_tag :span, status.humanize, class: \"badge #{css_class}\"\n  end\n\n  def processing_progress(document)\n    return unless document.processing?\n\n    progress = document.processing_progress || 0\n\n    content_tag :div, class: \"progress\" do\n      content_tag :div, \"\", \n                  class: \"progress-bar\", \n                  style: \"width: #{progress}%\",\n                  data: { \n                    document_id: document.id,\n                    progress: progress \n                  }\n    end\n  end\n\n  # Authorization helpers\n  def can_edit_document?(document)\n    return false unless current_user\n\n    Ragdoll.configuration.authorize_document_access&amp;.call(document, current_user) ||\n      document.user == current_user ||\n      current_user.admin?\n  end\n\n  def can_delete_document?(document)\n    can_edit_document?(document)\n  end\n\n  def can_reprocess_document?(document)\n    can_edit_document?(document) &amp;&amp; document.processed?\n  end\n\n  # Search helpers\n  def highlight_search_terms(text, query)\n    return text if query.blank?\n\n    terms = query.split(/\\s+/).reject(&amp;:blank?)\n    highlighted = text\n\n    terms.each do |term|\n      highlighted = highlighted.gsub(\n        /#{Regexp.escape(term)}/i,\n        '&lt;mark&gt;\\0&lt;/mark&gt;'\n      )\n    end\n\n    highlighted.html_safe\n  end\n\n  def search_result_snippet(content, query, length: 200)\n    return truncate(content, length: length) if query.blank?\n\n    # Find the best snippet containing search terms\n    terms = query.split(/\\s+/).reject(&amp;:blank?)\n    term_positions = []\n\n    terms.each do |term|\n      pos = content.downcase.index(term.downcase)\n      term_positions &lt;&lt; pos if pos\n    end\n\n    if term_positions.any?\n      start_pos = [term_positions.min - 50, 0].max\n      snippet = content[start_pos, length]\n      \"...\" + snippet + \"...\"\n    else\n      truncate(content, length: length)\n    end\n  end\n\n  # Metadata helpers\n  def format_metadata_value(value)\n    case value\n    when Date, DateTime, Time\n      value.strftime(\"%B %d, %Y\")\n    when TrueClass, FalseClass\n      value ? \"Yes\" : \"No\"\n    when Array\n      value.join(\", \")\n    when Hash\n      value.to_json\n    else\n      value.to_s\n    end\n  end\n\n  def metadata_icon(key)\n    icon_class = case key.to_s.downcase\n                when /author|creator|owner/ then \"bi-person\"\n                when /date|time|created|updated/ then \"bi-calendar\"\n                when /category|tag|type/ then \"bi-tag\"\n                when /size|length|count/ then \"bi-rulers\"\n                when /location|place|geo/ then \"bi-geo-alt\"\n                when /url|link|website/ then \"bi-link\"\n                when /email|mail/ then \"bi-envelope\"\n                when /phone|tel/ then \"bi-telephone\"\n                else \"bi-info-circle\"\n                end\n\n    content_tag :i, \"\", class: \"#{icon_class} metadata-icon\"\n  end\nend\n</code></pre>"},{"location":"rails/views-helpers/#ragdollsearchhelper","title":"Ragdoll::SearchHelper","text":"<pre><code>module Ragdoll::SearchHelper\n  def search_form(options = {})\n    defaults = {\n      url: search_path,\n      method: :get,\n      local: true,\n      class: \"ragdoll-search-form\"\n    }\n\n    form_with **defaults.merge(options) do |form|\n      render 'ragdoll/search/search_form', form: form\n    end\n  end\n\n  def search_filters(facets)\n    return unless facets.present?\n\n    content_tag :div, class: \"search-filters\" do\n      facets.map do |facet_name, facet_data|\n        render 'ragdoll/search/filter_group', \n               facet_name: facet_name, \n               facet_data: facet_data\n      end.join.html_safe\n    end\n  end\n\n  def similarity_score_badge(score)\n    percentage = (score * 100).round(1)\n\n    css_class = case percentage\n               when 90..100 then 'badge-success'\n               when 70..89 then 'badge-warning'\n               when 50..69 then 'badge-secondary'\n               else 'badge-light'\n               end\n\n    content_tag :span, \"#{percentage}%\", class: \"badge #{css_class}\"\n  end\n\n  def search_time_display(time_in_seconds)\n    if time_in_seconds &lt; 1\n      \"#{(time_in_seconds * 1000).round}ms\"\n    else\n      \"#{time_in_seconds.round(2)}s\"\n    end\n  end\n\n  def search_suggestions(suggestions)\n    return unless suggestions.present?\n\n    content_tag :div, class: \"search-suggestions\" do\n      content_tag :p, \"Did you mean:\" do\n        suggestions.map do |suggestion|\n          link_to suggestion, search_path(q: suggestion), class: \"suggestion-link\"\n        end.join(\" \u00b7 \").html_safe\n      end\n    end\n  end\n\n  def facet_filter_link(facet_name, facet_value, count)\n    current_filters = params[:filters] || {}\n    new_filters = current_filters.dup\n\n    if new_filters[facet_name] == facet_value\n      new_filters.delete(facet_name)\n      css_class = \"facet-link active\"\n    else\n      new_filters[facet_name] = facet_value\n      css_class = \"facet-link\"\n    end\n\n    link_to search_path(q: params[:q], filters: new_filters), class: css_class do\n      \"#{facet_value} (#{count})\"\n    end\n  end\nend\n</code></pre>"},{"location":"rails/views-helpers/#stimulus-controllers","title":"Stimulus Controllers","text":""},{"location":"rails/views-helpers/#search-controller-search_controllerjs","title":"Search Controller (<code>search_controller.js</code>)","text":"<pre><code>import { Controller } from \"@hotwired/stimulus\"\n\nexport default class extends Controller {\n  static targets = [\"input\", \"suggestions\", \"results\"]\n  static values = { \n    suggestionsUrl: String,\n    debounceDelay: { type: Number, default: 300 }\n  }\n\n  connect() {\n    this.timeout = null\n  }\n\n  suggest() {\n    clearTimeout(this.timeout)\n\n    this.timeout = setTimeout(() =&gt; {\n      const query = this.inputTarget.value.trim()\n\n      if (query.length &gt; 2) {\n        this.fetchSuggestions(query)\n      } else {\n        this.hideSuggestions()\n      }\n    }, this.debounceDelayValue)\n  }\n\n  async fetchSuggestions(query) {\n    try {\n      const response = await fetch(`${this.suggestionsUrlValue}?q=${encodeURIComponent(query)}`)\n      const suggestions = await response.json()\n\n      this.displaySuggestions(suggestions)\n    } catch (error) {\n      console.error('Failed to fetch suggestions:', error)\n    }\n  }\n\n  displaySuggestions(suggestions) {\n    if (suggestions.length === 0) {\n      this.hideSuggestions()\n      return\n    }\n\n    const suggestionsList = suggestions.map(suggestion =&gt; \n      `&lt;li class=\"suggestion-item\" data-action=\"click-&gt;search#selectSuggestion\"&gt;${suggestion}&lt;/li&gt;`\n    ).join('')\n\n    this.suggestionsTarget.innerHTML = `&lt;ul class=\"suggestions-list\"&gt;${suggestionsList}&lt;/ul&gt;`\n    this.suggestionsTarget.style.display = 'block'\n  }\n\n  hideSuggestions() {\n    this.suggestionsTarget.style.display = 'none'\n  }\n\n  selectSuggestion(event) {\n    const suggestion = event.target.textContent\n    this.inputTarget.value = suggestion\n    this.hideSuggestions()\n    this.submitSearch()\n  }\n\n  submitSearch() {\n    this.element.querySelector('form').submit()\n  }\n\n  disconnect() {\n    clearTimeout(this.timeout)\n  }\n}\n</code></pre>"},{"location":"rails/views-helpers/#upload-controller-upload_controllerjs","title":"Upload Controller (<code>upload_controller.js</code>)","text":"<pre><code>import { Controller } from \"@hotwired/stimulus\"\n\nexport default class extends Controller {\n  static targets = [\"input\", \"preview\", \"progress\", \"errors\"]\n  static values = { \n    maxFileSize: Number,\n    allowedTypes: Array,\n    uploadUrl: String\n  }\n\n  preview() {\n    const files = this.inputTarget.files\n    this.clearErrors()\n\n    if (files.length === 0) {\n      this.clearPreview()\n      return\n    }\n\n    this.validateFiles(files)\n    this.displayPreview(files)\n  }\n\n  validateFiles(files) {\n    let hasErrors = false\n\n    Array.from(files).forEach(file =&gt; {\n      if (file.size &gt; this.maxFileSizeValue) {\n        this.showError(`File \"${file.name}\" is too large. Maximum size is ${this.formatFileSize(this.maxFileSizeValue)}.`)\n        hasErrors = true\n      }\n\n      if (!this.allowedTypesValue.includes(file.type)) {\n        this.showError(`File type \"${file.type}\" is not allowed for \"${file.name}\".`)\n        hasErrors = true\n      }\n    })\n\n    return !hasErrors\n  }\n\n  displayPreview(files) {\n    const previews = Array.from(files).map(file =&gt; {\n      return `\n        &lt;div class=\"file-preview\" data-filename=\"${file.name}\"&gt;\n          &lt;div class=\"file-info\"&gt;\n            &lt;span class=\"filename\"&gt;${file.name}&lt;/span&gt;\n            &lt;span class=\"filesize\"&gt;${this.formatFileSize(file.size)}&lt;/span&gt;\n          &lt;/div&gt;\n          &lt;div class=\"file-actions\"&gt;\n            &lt;button type=\"button\" class=\"remove-file\" data-action=\"click-&gt;upload#removeFile\"&gt;\u00d7&lt;/button&gt;\n          &lt;/div&gt;\n        &lt;/div&gt;\n      `\n    }).join('')\n\n    this.previewTarget.innerHTML = previews\n  }\n\n  removeFile(event) {\n    const preview = event.target.closest('.file-preview')\n    const filename = preview.dataset.filename\n\n    // Remove from file input (requires recreating the input)\n    const dt = new DataTransfer()\n    const files = this.inputTarget.files\n\n    Array.from(files).forEach(file =&gt; {\n      if (file.name !== filename) {\n        dt.items.add(file)\n      }\n    })\n\n    this.inputTarget.files = dt.files\n    preview.remove()\n\n    if (this.inputTarget.files.length === 0) {\n      this.clearPreview()\n    }\n  }\n\n  async upload(event) {\n    event.preventDefault()\n\n    const files = this.inputTarget.files\n    if (files.length === 0) return\n\n    if (!this.validateFiles(files)) return\n\n    this.showProgress()\n\n    try {\n      for (let i = 0; i &lt; files.length; i++) {\n        await this.uploadFile(files[i], i + 1, files.length)\n      }\n\n      this.onUploadSuccess()\n    } catch (error) {\n      this.showError('Upload failed: ' + error.message)\n    } finally {\n      this.hideProgress()\n    }\n  }\n\n  async uploadFile(file, index, total) {\n    const formData = new FormData()\n    formData.append('document[file]', file)\n    formData.append('document[title]', file.name)\n\n    const response = await fetch(this.uploadUrlValue, {\n      method: 'POST',\n      body: formData,\n      headers: {\n        'X-CSRF-Token': document.querySelector('[name=\"csrf-token\"]').content\n      }\n    })\n\n    if (!response.ok) {\n      throw new Error(`Failed to upload ${file.name}`)\n    }\n\n    this.updateProgress(index, total)\n  }\n\n  showProgress() {\n    this.progressTarget.style.display = 'block'\n    this.updateProgress(0, 1)\n  }\n\n  updateProgress(current, total) {\n    const percentage = (current / total) * 100\n    const progressBar = this.progressTarget.querySelector('.progress-bar')\n    if (progressBar) {\n      progressBar.style.width = `${percentage}%`\n      progressBar.textContent = `${current} / ${total} files`\n    }\n  }\n\n  hideProgress() {\n    this.progressTarget.style.display = 'none'\n  }\n\n  onUploadSuccess() {\n    this.clearPreview()\n    this.inputTarget.value = ''\n    // Optionally redirect or reload\n    window.location.reload()\n  }\n\n  showError(message) {\n    const errorDiv = document.createElement('div')\n    errorDiv.className = 'alert alert-danger'\n    errorDiv.textContent = message\n    this.errorsTarget.appendChild(errorDiv)\n  }\n\n  clearErrors() {\n    this.errorsTarget.innerHTML = ''\n  }\n\n  clearPreview() {\n    this.previewTarget.innerHTML = ''\n  }\n\n  formatFileSize(bytes) {\n    if (bytes === 0) return '0 Bytes'\n\n    const k = 1024\n    const sizes = ['Bytes', 'KB', 'MB', 'GB']\n    const i = Math.floor(Math.log(bytes) / Math.log(k))\n\n    return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i]\n  }\n}\n</code></pre>"},{"location":"rails/views-helpers/#view-components","title":"View Components","text":""},{"location":"rails/views-helpers/#document-card-component","title":"Document Card Component","text":"<pre><code># app/components/ragdoll/document_card_component.rb\nclass Ragdoll::DocumentCardComponent &lt; ViewComponent::Base\n  def initialize(document:, show_actions: true, size: :medium)\n    @document = document\n    @show_actions = show_actions\n    @size = size\n  end\n\n  private\n\n  attr_reader :document, :show_actions, :size\n\n  def card_classes\n    base_classes = [\"ragdoll-document-card\"]\n    base_classes &lt;&lt; \"card-#{size}\"\n    base_classes &lt;&lt; \"card-#{document.status}\"\n    base_classes.join(\" \")\n  end\n\n  def thumbnail_url\n    if document.file.attached? &amp;&amp; document.file.representable?\n      rails_representation_url(document.file.representation(resize_to_limit: thumbnail_size))\n    else\n      nil\n    end\n  end\n\n  def thumbnail_size\n    case size\n    when :small then [100, 75]\n    when :large then [400, 300]\n    else [200, 150]\n    end\n  end\nend\n</code></pre> <pre><code>&lt;!-- app/components/ragdoll/document_card_component.html.erb --&gt;\n&lt;div class=\"&lt;%= card_classes %&gt;\" data-document-id=\"&lt;%= document.id %&gt;\"&gt;\n  &lt;div class=\"card-thumbnail\"&gt;\n    &lt;% if thumbnail_url %&gt;\n      &lt;%= image_tag thumbnail_url, alt: document.title, class: \"thumbnail-image\" %&gt;\n    &lt;% else %&gt;\n      &lt;div class=\"file-type-icon\"&gt;\n        &lt;%= ragdoll_file_icon(document.content_type) %&gt;\n      &lt;/div&gt;\n    &lt;% end %&gt;\n\n    &lt;div class=\"card-overlay\"&gt;\n      &lt;%= status_badge(document.status) %&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"card-content\"&gt;\n    &lt;h3 class=\"card-title\"&gt;\n      &lt;%= link_to document.title, document_path(document) %&gt;\n    &lt;/h3&gt;\n\n    &lt;% if document.description.present? %&gt;\n      &lt;p class=\"card-description\"&gt;\n        &lt;%= truncate(document.description, length: description_length) %&gt;\n      &lt;/p&gt;\n    &lt;% end %&gt;\n\n    &lt;div class=\"card-meta\"&gt;\n      &lt;span class=\"file-size\"&gt;&lt;%= human_file_size(document.file_size) %&gt;&lt;/span&gt;\n      &lt;span class=\"upload-date\"&gt;&lt;%= time_ago_in_words(document.created_at) %&gt; ago&lt;/span&gt;\n    &lt;/div&gt;\n\n    &lt;% if show_actions %&gt;\n      &lt;div class=\"card-actions\"&gt;\n        &lt;%= link_to \"View\", document_path(document), class: \"btn btn-sm btn-primary\" %&gt;\n        &lt;% if can_edit_document?(document) %&gt;\n          &lt;%= link_to \"Edit\", edit_document_path(document), class: \"btn btn-sm btn-secondary\" %&gt;\n        &lt;% end %&gt;\n      &lt;/div&gt;\n    &lt;% end %&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n</code></pre> <p>This comprehensive view and helper documentation provides everything you need to customize and extend the Ragdoll Rails UI components for your specific application needs.</p>"},{"location":"reference/llm-integration/","title":"LLM Integration","text":"<p>Ragdoll provides comprehensive LLM integration through the RubyLLM library, supporting multiple providers with flexible configuration and robust error handling. The system is designed for production use with automatic fallbacks and cost optimization strategies.</p>"},{"location":"reference/llm-integration/#multiple-provider-support-and-configuration","title":"Multiple Provider Support and Configuration","text":"<p>Ragdoll's LLM integration is built on RubyLLM, providing a unified interface to multiple LLM providers. The configuration system supports environment-based setup, automatic provider detection, and sophisticated fallback strategies.</p>"},{"location":"reference/llm-integration/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    A[Ragdoll Configuration] --&gt; B[RubyLLM Integration]\n    B --&gt; C[Provider Selection]\n    C --&gt; D[OpenAI]\n    C --&gt; E[Anthropic]\n    C --&gt; F[Google Vertex AI]\n    C --&gt; G[Azure OpenAI]\n    C --&gt; H[Ollama]\n    C --&gt; I[HuggingFace]\n    C --&gt; J[OpenRouter]\n\n    B --&gt; K[EmbeddingService]\n    B --&gt; L[TextGenerationService]\n\n    K --&gt; M[Vector Generation]\n    L --&gt; N[Summary Generation]\n    L --&gt; O[Keyword Extraction]\n\n    P[Error Handling] --&gt; Q[Provider Fallback]\n    P --&gt; R[Retry Logic]\n    P --&gt; S[Circuit Breakers]</code></pre>"},{"location":"reference/llm-integration/#configuration-structure","title":"Configuration Structure","text":"<p>The LLM configuration is centralized in the <code>ruby_llm_config</code> section:</p> <pre><code>Ragdoll::Core.configure do |config|\n  config.ruby_llm_config = {\n    openai: {\n      api_key: -&gt; { ENV[\"OPENAI_API_KEY\"] },\n      organization: -&gt; { ENV[\"OPENAI_ORGANIZATION\"] },\n      project: -&gt; { ENV[\"OPENAI_PROJECT\"] }\n    },\n    anthropic: {\n      api_key: -&gt; { ENV[\"ANTHROPIC_API_KEY\"] }\n    },\n    google: {\n      api_key: -&gt; { ENV[\"GOOGLE_API_KEY\"] },\n      project_id: -&gt; { ENV[\"GOOGLE_PROJECT_ID\"] }\n    },\n    azure: {\n      api_key: -&gt; { ENV[\"AZURE_OPENAI_API_KEY\"] },\n      endpoint: -&gt; { ENV[\"AZURE_OPENAI_ENDPOINT\"] },\n      api_version: -&gt; { ENV[\"AZURE_OPENAI_API_VERSION\"] || \"2024-02-01\" }\n    },\n    ollama: {\n      endpoint: -&gt; { ENV[\"OLLAMA_ENDPOINT\"] || \"http://localhost:11434/v1\" }\n    },\n    huggingface: {\n      api_key: -&gt; { ENV[\"HUGGINGFACE_API_KEY\"] }\n    },\n    openrouter: {\n      api_key: -&gt; { ENV[\"OPENROUTER_API_KEY\"] }\n    }\n  }\nend\n</code></pre>"},{"location":"reference/llm-integration/#supported-providers","title":"Supported Providers","text":"<p>Ragdoll supports seven major LLM providers through RubyLLM integration. Each provider is configured through environment variables with automatic validation and fallback handling.</p>"},{"location":"reference/llm-integration/#openai","title":"OpenAI","text":"<p>Models Supported: - GPT-4 series (gpt-4o, gpt-4-turbo, gpt-4) - GPT-3.5-turbo series - Text embedding models (text-embedding-3-small, text-embedding-3-large) - Image understanding via GPT-4 Vision</p> <p>Configuration: <pre><code>config.ruby_llm_config[:openai] = {\n  api_key: -&gt; { ENV[\"OPENAI_API_KEY\"] },\n  organization: -&gt; { ENV[\"OPENAI_ORGANIZATION\"] },  # Optional\n  project: -&gt; { ENV[\"OPENAI_PROJECT\"] }             # Optional\n}\n\n# Model selection\nconfig.models = {\n  default: \"openai/gpt-4o\",\n  summary: \"openai/gpt-4o\",\n  keywords: \"openai/gpt-4o\",\n  embedding: {\n    text: \"text-embedding-3-small\"\n  }\n}\n</code></pre></p> <p>Rate Limiting &amp; Optimization: - Automatic retry with exponential backoff - Request batching for embeddings - Token usage optimization - Cost monitoring through usage tracking</p>"},{"location":"reference/llm-integration/#anthropic","title":"Anthropic","text":"<p>Models Supported: - Claude 3 series (claude-3-opus, claude-3-sonnet, claude-3-haiku) - Claude 2 series for cost optimization - Long context capabilities (up to 200K tokens)</p> <p>Configuration: <pre><code>config.ruby_llm_config[:anthropic] = {\n  api_key: -&gt; { ENV[\"ANTHROPIC_API_KEY\"] }\n}\n\n# Using Anthropic models\nconfig.models[:default] = \"anthropic/claude-3-sonnet\"\nconfig.models[:summary] = \"anthropic/claude-3-haiku\"  # Cost optimization\n</code></pre></p> <p>Best Practices: - Use Claude 3 Haiku for simple tasks (cost-effective) - Use Claude 3 Sonnet for balanced performance - Use Claude 3 Opus for complex reasoning tasks - Leverage long context for document analysis</p>"},{"location":"reference/llm-integration/#google-vertex-ai","title":"Google Vertex AI","text":"<p>Models Supported: - Gemini Pro and Gemini Pro Vision - PaLM 2 models - Embedding models (textembedding-gecko) - Multi-modal capabilities</p> <p>Configuration: <pre><code>config.ruby_llm_config[:google] = {\n  api_key: -&gt; { ENV[\"GOOGLE_API_KEY\"] },\n  project_id: -&gt; { ENV[\"GOOGLE_PROJECT_ID\"] }\n}\n\n# Regional configuration\nENV[\"GOOGLE_VERTEX_REGION\"] = \"us-central1\"\n</code></pre></p> <p>Service Account Setup: 1. Create service account in Google Cloud Console 2. Download JSON key file 3. Set <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable 4. Enable Vertex AI API for your project</p>"},{"location":"reference/llm-integration/#azure-openai","title":"Azure OpenAI","text":"<p>Enterprise Features: - Private endpoints and VNet integration - Managed identity authentication - Content filtering and safety - Compliance certifications (SOC 2, HIPAA)</p> <p>Configuration: <pre><code>config.ruby_llm_config[:azure] = {\n  api_key: -&gt; { ENV[\"AZURE_OPENAI_API_KEY\"] },\n  endpoint: -&gt; { ENV[\"AZURE_OPENAI_ENDPOINT\"] },\n  api_version: -&gt; { ENV[\"AZURE_OPENAI_API_VERSION\"] || \"2024-02-01\" }\n}\n</code></pre></p> <p>Model Deployment: <pre><code># Example environment variables\nexport AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com/\"\nexport AZURE_OPENAI_API_KEY=\"your-api-key\"\nexport AZURE_OPENAI_API_VERSION=\"2024-02-01\"\n</code></pre></p>"},{"location":"reference/llm-integration/#ollama-local-models","title":"Ollama (Local Models)","text":"<p>Supported Models: - Llama 2 and Code Llama - Mistral and Mixtral models - Phi-3 and other local models - Custom fine-tuned models</p> <p>Configuration: <pre><code>config.ruby_llm_config[:ollama] = {\n  endpoint: -&gt; { ENV[\"OLLAMA_ENDPOINT\"] || \"http://localhost:11434/v1\" }\n}\n\n# No API key required for local deployment\nconfig.embedding_config[:provider] = :ollama\n</code></pre></p> <p>Performance Optimization: - GPU acceleration with CUDA/Metal - Memory management for large models - Model quantization for efficiency - Concurrent request handling</p> <p>Resource Requirements: - Minimum 8GB RAM for 7B models - 16GB+ RAM for 13B models - GPU recommended for production use - SSD storage for model files</p>"},{"location":"reference/llm-integration/#huggingface","title":"HuggingFace","text":"<p>Model Hub Integration: - 100,000+ models available - Custom model deployment - Inference API integration - Transformers library compatibility</p> <p>Configuration: <pre><code>config.ruby_llm_config[:huggingface] = {\n  api_key: -&gt; { ENV[\"HUGGINGFACE_API_KEY\"] }\n}\n\n# Model selection\nconfig.models[:embedding][:text] = \"sentence-transformers/all-MiniLM-L6-v2\"\n</code></pre></p> <p>Performance Tuning: - Model caching for faster inference - Batch processing for embeddings - Custom inference endpoints - Auto-scaling with serverless</p>"},{"location":"reference/llm-integration/#openrouter","title":"OpenRouter","text":"<p>Provider Routing: - Access to 20+ LLM providers - Automatic provider selection - Cost optimization routing - Real-time pricing updates</p> <p>Configuration: <pre><code>config.ruby_llm_config[:openrouter] = {\n  api_key: -&gt; { ENV[\"OPENROUTER_API_KEY\"] }\n}\n\n# Cost-optimized model selection\nconfig.models[:default] = \"openrouter/anthropic/claude-3-haiku\"\nconfig.models[:summary] = \"openrouter/meta-llama/llama-2-7b-chat\"\n</code></pre></p> <p>Fallback Strategies: - Primary provider \u2192 Secondary provider \u2192 Local fallback - Cost-based provider selection - Geographic routing for compliance - Real-time availability checking</p>"},{"location":"reference/llm-integration/#configuration-patterns","title":"Configuration Patterns","text":"<p>Ragdoll provides flexible configuration patterns that support everything from simple single-provider setups to complex multi-provider environments with cost optimization and failover.</p>"},{"location":"reference/llm-integration/#provider-selection","title":"Provider Selection","text":"<p>Model-Specific Provider Configuration: <pre><code>Ragdoll::Core.configure do |config|\n  # Different providers for different tasks\n  config.models = {\n    default: \"openai/gpt-4o\",           # OpenAI for general tasks\n    summary: \"anthropic/claude-3-haiku\", # Anthropic for summaries\n    keywords: \"openai/gpt-3.5-turbo\",   # OpenAI for keywords\n    embedding: {\n      text: \"text-embedding-3-small\",   # OpenAI embeddings\n      image: \"openai/clip-vit-large\",   # OpenAI image embeddings\n      audio: \"openai/whisper-1\"         # OpenAI audio processing\n    }\n  }\nend\n</code></pre></p> <p>Automatic Provider Detection: The system uses <code>parse_provider_model</code> to automatically detect providers:</p> <pre><code># Format: \"provider/model\" -&gt; automatic provider detection\nconfig.models[:default] = \"openai/gpt-4o\"\nconfig.models[:summary] = \"anthropic/claude-3-sonnet\"\n\n# Format: \"model\" -&gt; RubyLLM determines provider\nconfig.models[:embedding][:text] = \"text-embedding-3-small\"\n\n# Configuration parsing\nparsed = config.parse_provider_model(\"openai/gpt-4o\")\n# =&gt; { provider: :openai, model: \"gpt-4o\" }\n\nparsed = config.parse_provider_model(\"gpt-4o\")\n# =&gt; { provider: nil, model: \"gpt-4o\" } # RubyLLM auto-detects\n</code></pre> <p>Fallback Provider Chains: <pre><code>class CustomTextGenerationService &lt; Ragdoll::TextGenerationService\n  private\n\n  def generate_with_fallback(prompt, models)\n    models.each do |model_string|\n      begin\n        parsed = @configuration.parse_provider_model(model_string)\n        return generate_with_model(prompt, parsed[:model])\n      rescue StandardError =&gt; e\n        Rails.logger.warn \"Provider #{parsed[:provider]} failed: #{e.message}\"\n        next\n      end\n    end\n\n    # Ultimate fallback to basic processing\n    generate_basic_summary(prompt, 300)\n  end\nend\n\n# Usage with fallback chain\nfallback_models = [\n  \"openai/gpt-4o\",           # Primary\n  \"anthropic/claude-3-sonnet\", # Secondary\n  \"ollama/llama2\"            # Local fallback\n]\n</code></pre></p> <p>Cost-Based Selection: <pre><code># Cost optimization configuration\nconfig.cost_optimization = {\n  enable: true,\n  thresholds: {\n    summary: { max_cost_per_request: 0.01 },\n    keywords: { max_cost_per_request: 0.005 },\n    embedding: { max_cost_per_1k_tokens: 0.0001 }\n  },\n  fallback_models: {\n    summary: [\"anthropic/claude-3-haiku\", \"ollama/llama2\"],\n    keywords: [\"openai/gpt-3.5-turbo\", \"ollama/mistral\"]\n  }\n}\n</code></pre></p>"},{"location":"reference/llm-integration/#api-key-management","title":"API Key Management","text":"<p>Environment Variable Setup: <pre><code># OpenAI\nexport OPENAI_API_KEY=\"sk-...\"\nexport OPENAI_ORGANIZATION=\"org-...\"\nexport OPENAI_PROJECT=\"proj_...\"\n\n# Anthropic\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n\n# Google\nexport GOOGLE_API_KEY=\"AIza...\"\nexport GOOGLE_PROJECT_ID=\"my-project-id\"\nexport GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/service-account.json\"\n\n# Azure\nexport AZURE_OPENAI_API_KEY=\"...\"\nexport AZURE_OPENAI_ENDPOINT=\"https://my-resource.openai.azure.com/\"\nexport AZURE_OPENAI_API_VERSION=\"2024-02-01\"\n\n# HuggingFace\nexport HUGGINGFACE_API_KEY=\"hf_...\"\n\n# OpenRouter\nexport OPENROUTER_API_KEY=\"sk-or-...\"\n</code></pre></p> <p>Secure Key Storage: <pre><code># Using Rails credentials\nconfig.ruby_llm_config[:openai] = {\n  api_key: -&gt; { Rails.application.credentials.openai[:api_key] },\n  organization: -&gt; { Rails.application.credentials.openai[:organization] }\n}\n\n# Using Vault or similar secret management\nconfig.ruby_llm_config[:openai] = {\n  api_key: -&gt; { VaultClient.get_secret(\"openai/api_key\") }\n}\n\n# Using AWS Secrets Manager\nconfig.ruby_llm_config[:openai] = {\n  api_key: -&gt; {\n    AWS::SecretsManager::Client.new.get_secret_value(\n      secret_id: \"prod/ragdoll/openai_api_key\"\n    ).secret_string\n  }\n}\n</code></pre></p> <p>Key Rotation Strategies: <pre><code>class APIKeyRotationService\n  def self.rotate_keys\n    # Implement key rotation logic\n    providers = [:openai, :anthropic, :google]\n\n    providers.each do |provider|\n      current_key = get_current_key(provider)\n      new_key = generate_new_key(provider)\n\n      # Test new key\n      if test_api_key(provider, new_key)\n        update_key_in_secret_store(provider, new_key)\n        schedule_old_key_revocation(provider, current_key)\n      end\n    end\n  end\n\n  private\n\n  def self.test_api_key(provider, key)\n    # Test API key with minimal request\n    test_config = { provider =&gt; { api_key: key } }\n    service = Ragdoll::EmbeddingService.new\n    service.generate_embedding(\"test\")\n    true\n  rescue\n    false\n  end\nend\n</code></pre></p> <p>Multi-Tenant Key Management: <pre><code>class MultiTenantConfiguration\n  def self.for_tenant(tenant_id)\n    Ragdoll::Core.configure do |config|\n      tenant_keys = fetch_tenant_keys(tenant_id)\n\n      config.ruby_llm_config = {\n        openai: {\n          api_key: -&gt; { tenant_keys[:openai_api_key] },\n          organization: -&gt; { tenant_keys[:openai_organization] }\n        },\n        anthropic: {\n          api_key: -&gt; { tenant_keys[:anthropic_api_key] }\n        }\n      }\n\n      # Tenant-specific model preferences\n      config.models = tenant_model_preferences(tenant_id)\n    end\n  end\n\n  private\n\n  def self.fetch_tenant_keys(tenant_id)\n    # Fetch from secure tenant key store\n    TenantKeyStore.get_keys(tenant_id)\n  end\nend\n</code></pre></p>"},{"location":"reference/llm-integration/#model-selection-strategies","title":"Model Selection Strategies","text":"<p>Ragdoll implements intelligent model selection based on task requirements, performance characteristics, and cost considerations. The system supports both automatic and manual model selection strategies.</p>"},{"location":"reference/llm-integration/#task-specific-models","title":"Task-Specific Models","text":"<p>Embedding Model Selection: <pre><code>config.models[:embedding] = {\n  text: \"text-embedding-3-small\",      # Fast, cost-effective for most text\n  image: \"clip-vit-large-patch14\",     # Best for image understanding\n  audio: \"whisper-embedding-v1\",      # Audio content embeddings\n  code: \"text-embedding-3-large\",     # Higher accuracy for code\n  multilingual: \"multilingual-e5-large\" # Multi-language support\n}\n\n# Dynamic embedding model selection\nclass SmartEmbeddingService &lt; Ragdoll::EmbeddingService\n  def generate_embedding(text, content_type: :text)\n    model = select_embedding_model(text, content_type)\n    super(text, model: model)\n  end\n\n  private\n\n  def select_embedding_model(text, content_type)\n    case content_type\n    when :code\n      detect_programming_language(text) ? \"text-embedding-3-large\" : \"text-embedding-3-small\"\n    when :multilingual\n      detect_language(text) != \"en\" ? \"multilingual-e5-large\" : \"text-embedding-3-small\"\n    when :long_document\n      text.length &gt; 5000 ? \"text-embedding-3-large\" : \"text-embedding-3-small\"\n    else\n      Ragdoll.config.models[:embedding][content_type]\n    end\n  end\nend\n</code></pre></p> <p>Summary Generation Models: <pre><code># Task-specific summary model configuration\nconfig.summarization_config = {\n  enable: true,\n  models: {\n    short_content: \"openai/gpt-3.5-turbo\",    # &lt; 1000 tokens\n    medium_content: \"anthropic/claude-3-haiku\", # 1000-5000 tokens\n    long_content: \"anthropic/claude-3-sonnet\",  # &gt; 5000 tokens\n    technical: \"openai/gpt-4o\",                # Technical documents\n    creative: \"anthropic/claude-3-opus\"        # Creative content\n  },\n  max_length: 300,\n  quality_threshold: 0.8\n}\n\n# Intelligent model selection in TextGenerationService\ndef select_summary_model(content)\n  token_count = estimate_token_count(content)\n  content_type = detect_content_type(content)\n\n  case\n  when token_count &lt; 1000\n    @configuration.summarization_config[:models][:short_content]\n  when technical_content?(content)\n    @configuration.summarization_config[:models][:technical]\n  when creative_content?(content)\n    @configuration.summarization_config[:models][:creative]\n  else\n    @configuration.summarization_config[:models][:medium_content]\n  end\nend\n</code></pre></p> <p>Keyword Extraction Models: <pre><code># Specialized keyword extraction configuration\nconfig.keyword_extraction = {\n  models: {\n    general: \"openai/gpt-3.5-turbo\",\n    technical: \"openai/gpt-4o\",\n    academic: \"anthropic/claude-3-sonnet\",\n    multilingual: \"google/gemini-pro\"\n  },\n  max_keywords: 20,\n  confidence_threshold: 0.7\n}\n\n# Context-aware keyword extraction\nclass AdvancedKeywordExtraction &lt; Ragdoll::TextGenerationService\n  def extract_keywords(text, context: :general)\n    model = select_keyword_model(text, context)\n\n    prompt = build_contextual_keyword_prompt(text, context)\n\n    extract_with_model(text, model, prompt)\n  end\n\n  private\n\n  def select_keyword_model(text, context)\n    case context\n    when :technical\n      detect_technical_terms(text) ? \"openai/gpt-4o\" : \"openai/gpt-3.5-turbo\"\n    when :academic\n      \"anthropic/claude-3-sonnet\"\n    when :multilingual\n      \"google/gemini-pro\"\n    else\n      \"openai/gpt-3.5-turbo\"\n    end\n  end\nend\n</code></pre></p> <p>Classification Models: <pre><code># Document classification configuration\nconfig.classification = {\n  models: {\n    content_type: \"openai/gpt-3.5-turbo\",\n    sentiment: \"anthropic/claude-3-haiku\",\n    topic: \"openai/gpt-4o\",\n    language: \"google/gemini-pro\"\n  },\n  categories: {\n    content_type: [\"technical\", \"business\", \"academic\", \"creative\"],\n    sentiment: [\"positive\", \"negative\", \"neutral\"],\n    topic: [\"technology\", \"finance\", \"healthcare\", \"education\"]\n  }\n}\n</code></pre></p>"},{"location":"reference/llm-integration/#performance-vs-cost","title":"Performance vs Cost","text":"<p>Model Performance Comparison: <pre><code># Performance benchmarking data\nMODEL_PERFORMANCE = {\n  \"openai/gpt-4o\" =&gt; {\n    accuracy: 0.95,\n    speed: \"medium\",\n    cost_per_1k_tokens: 0.03,\n    best_for: [\"complex_reasoning\", \"technical_analysis\"]\n  },\n  \"anthropic/claude-3-sonnet\" =&gt; {\n    accuracy: 0.92,\n    speed: \"fast\",\n    cost_per_1k_tokens: 0.015,\n    best_for: [\"document_analysis\", \"summarization\"]\n  },\n  \"openai/gpt-3.5-turbo\" =&gt; {\n    accuracy: 0.87,\n    speed: \"very_fast\",\n    cost_per_1k_tokens: 0.002,\n    best_for: [\"simple_tasks\", \"bulk_processing\"]\n  }\n}\n\nclass PerformanceCostOptimizer\n  def select_optimal_model(task_type, content_length, quality_requirement)\n    candidates = MODEL_PERFORMANCE.select do |model, stats|\n      stats[:best_for].include?(task_type.to_s) &amp;&amp;\n      stats[:accuracy] &gt;= quality_requirement\n    end\n\n    # Sort by cost-effectiveness (accuracy/cost ratio)\n    candidates.min_by do |model, stats|\n      estimated_cost = calculate_cost(content_length, stats[:cost_per_1k_tokens])\n      estimated_cost / stats[:accuracy]\n    end.first\n  end\nend\n</code></pre></p> <p>Cost Optimization Strategies: <pre><code># Adaptive cost optimization\nconfig.cost_optimization = {\n  enable: true,\n  budget_limits: {\n    daily: 100.00,    # Daily budget limit\n    monthly: 2000.00  # Monthly budget limit\n  },\n  strategies: {\n    batch_processing: true,     # Batch similar requests\n    caching: true,             # Cache similar requests\n    model_downgrade: true,     # Use cheaper models when possible\n    queue_management: true     # Queue non-urgent requests\n  }\n}\n\nclass CostOptimizationService\n  def optimize_request(request_type, content, urgency: :normal)\n    current_spend = calculate_daily_spend\n    remaining_budget = config.cost_optimization[:budget_limits][:daily] - current_spend\n\n    if remaining_budget &lt; 10.00 &amp;&amp; urgency != :high\n      # Use cheaper model or queue request\n      return queue_request(request_type, content)\n    end\n\n    # Check for cached similar requests\n    if cached_result = check_cache(request_type, content)\n      return cached_result\n    end\n\n    # Select cost-effective model\n    model = select_cost_effective_model(request_type, remaining_budget)\n    process_request(request_type, content, model)\n  end\nend\n</code></pre></p> <p>Quality Thresholds: <pre><code># Quality control configuration\nconfig.quality_control = {\n  minimum_thresholds: {\n    embedding_similarity: 0.7,\n    summary_coherence: 0.8,\n    keyword_relevance: 0.75\n  },\n  validation_methods: {\n    semantic_coherence: true,\n    factual_accuracy: true,\n    language_quality: true\n  },\n  retry_on_low_quality: true,\n  max_retries: 2\n}\n\nclass QualityValidator\n  def validate_summary(summary, original_content)\n    scores = {\n      coherence: calculate_coherence_score(summary),\n      relevance: calculate_relevance_score(summary, original_content),\n      completeness: calculate_completeness_score(summary, original_content)\n    }\n\n    overall_score = scores.values.sum / scores.length\n\n    if overall_score &lt; config.quality_control[:minimum_thresholds][:summary_coherence]\n      raise QualityThresholdError, \"Summary quality below threshold: #{overall_score}\"\n    end\n\n    { valid: true, scores: scores, overall_score: overall_score }\n  end\nend\n</code></pre></p> <p>Batch Processing Optimization: <pre><code># Efficient batch processing\nclass BatchOptimizedEmbeddingService &lt; Ragdoll::EmbeddingService\n  MAX_BATCH_SIZE = 100\n  OPTIMAL_BATCH_SIZE = 50\n\n  def generate_embeddings_optimized(texts)\n    # Group texts by optimal batch size\n    batches = texts.each_slice(OPTIMAL_BATCH_SIZE).to_a\n\n    results = []\n    batches.each_with_index do |batch, index|\n      # Add delay between batches to respect rate limits\n      sleep(0.1) if index &gt; 0\n\n      batch_results = generate_embeddings_batch(batch)\n      results.concat(batch_results)\n\n      # Progress tracking\n      progress = ((index + 1) * 100.0 / batches.length).round(1)\n      puts \"Batch processing: #{progress}% complete\"\n    end\n\n    results\n  end\nend\n</code></pre></p>"},{"location":"reference/llm-integration/#error-handling","title":"Error Handling","text":"<p>Ragdoll implements comprehensive error handling with automatic retries, intelligent fallbacks, and circuit breaker patterns to ensure reliable LLM integration in production environments.</p>"},{"location":"reference/llm-integration/#provider-failures","title":"Provider Failures","text":"<p>Automatic Retry Strategies: <pre><code>class RobustLLMService\n  MAX_RETRIES = 3\n  RETRY_DELAYS = [1, 2, 4] # Exponential backoff in seconds\n\n  def generate_with_retry(content, model, max_retries: MAX_RETRIES)\n    attempt = 0\n\n    begin\n      attempt += 1\n      result = generate_content(content, model)\n\n      # Reset success counter on successful request\n      reset_failure_count(model)\n      return result\n\n    rescue RateLimitError =&gt; e\n      if attempt &lt;= max_retries\n        delay = calculate_rate_limit_delay(e)\n        Rails.logger.warn \"Rate limited, retrying in #{delay}s (attempt #{attempt})\"\n        sleep(delay)\n        retry\n      else\n        handle_rate_limit_failure(model, e)\n      end\n\n    rescue APIError =&gt; e\n      if retryable_error?(e) &amp;&amp; attempt &lt;= max_retries\n        delay = RETRY_DELAYS[attempt - 1] || 4\n        Rails.logger.warn \"API error, retrying in #{delay}s (attempt #{attempt}): #{e.message}\"\n        sleep(delay)\n        retry\n      else\n        handle_api_failure(model, e)\n      end\n\n    rescue StandardError =&gt; e\n      Rails.logger.error \"Unexpected error with #{model}: #{e.message}\"\n      increment_failure_count(model)\n      raise e\n    end\n  end\n\n  private\n\n  def retryable_error?(error)\n    case error\n    when NetworkError, TimeoutError, TemporaryServerError\n      true\n    when AuthenticationError, InvalidModelError\n      false\n    else\n      error.message.include?(\"temporary\") || error.message.include?(\"retry\")\n    end\n  end\nend\n</code></pre></p> <p>Provider Fallback: <pre><code>class ProviderFallbackService\n  def initialize\n    @provider_health = Hash.new(0) # Track failure counts\n    @circuit_breakers = {} # Circuit breaker states\n  end\n\n  def generate_with_fallback(content, task_type)\n    providers = get_provider_chain(task_type)\n\n    providers.each do |provider_config|\n      next if circuit_breaker_open?(provider_config[:provider])\n\n      begin\n        result = attempt_generation(content, provider_config)\n        record_success(provider_config[:provider])\n        return result\n\n      rescue StandardError =&gt; e\n        record_failure(provider_config[:provider], e)\n        Rails.logger.warn \"Provider #{provider_config[:provider]} failed: #{e.message}\"\n\n        # Continue to next provider\n        next\n      end\n    end\n\n    # All providers failed, use basic fallback\n    Rails.logger.error \"All LLM providers failed, using basic fallback\"\n    generate_basic_fallback(content, task_type)\n  end\n\n  private\n\n  def get_provider_chain(task_type)\n    case task_type\n    when :summary\n      [\n        { provider: :openai, model: \"gpt-4o\", priority: 1 },\n        { provider: :anthropic, model: \"claude-3-sonnet\", priority: 2 },\n        { provider: :ollama, model: \"llama2\", priority: 3 }\n      ]\n    when :embedding\n      [\n        { provider: :openai, model: \"text-embedding-3-small\", priority: 1 },\n        { provider: :huggingface, model: \"sentence-transformers/all-MiniLM-L6-v2\", priority: 2 }\n      ]\n    end.sort_by { |config| config[:priority] }\n  end\nend\n</code></pre></p> <p>Error Classification: <pre><code>module ErrorClassification\n  class LLMError &lt; StandardError; end\n  class RateLimitError &lt; LLMError; end\n  class AuthenticationError &lt; LLMError; end\n  class QuotaExceededError &lt; LLMError; end\n  class ModelUnavailableError &lt; LLMError; end\n  class InvalidRequestError &lt; LLMError; end\n  class NetworkError &lt; LLMError; end\n  class TimeoutError &lt; LLMError; end\n\n  def classify_error(error_response)\n    case error_response\n    when /rate limit/i, /too many requests/i\n      RateLimitError.new(error_response)\n    when /unauthorized/i, /invalid api key/i\n      AuthenticationError.new(error_response)\n    when /quota exceeded/i, /billing/i\n      QuotaExceededError.new(error_response)\n    when /model.*not available/i, /model.*not found/i\n      ModelUnavailableError.new(error_response)\n    when /timeout/i, /connection/i\n      NetworkError.new(error_response)\n    else\n      LLMError.new(error_response)\n    end\n  end\nend\n</code></pre></p> <p>Circuit Breaker Patterns: <pre><code>class CircuitBreaker\n  FAILURE_THRESHOLD = 5\n  RECOVERY_TIMEOUT = 300 # 5 minutes\n  HALF_OPEN_MAX_CALLS = 3\n\n  def initialize(provider)\n    @provider = provider\n    @failure_count = 0\n    @last_failure_time = nil\n    @state = :closed # :closed, :open, :half_open\n    @half_open_calls = 0\n  end\n\n  def call(&amp;block)\n    case @state\n    when :closed\n      execute_closed(&amp;block)\n    when :open\n      execute_open(&amp;block)\n    when :half_open\n      execute_half_open(&amp;block)\n    end\n  end\n\n  private\n\n  def execute_closed(&amp;block)\n    begin\n      result = block.call\n      reset_failure_count\n      result\n    rescue StandardError =&gt; e\n      record_failure\n      if @failure_count &gt;= FAILURE_THRESHOLD\n        @state = :open\n        @last_failure_time = Time.current\n        Rails.logger.error \"Circuit breaker opened for #{@provider}\"\n      end\n      raise e\n    end\n  end\n\n  def execute_open(&amp;block)\n    if Time.current - @last_failure_time &gt; RECOVERY_TIMEOUT\n      @state = :half_open\n      @half_open_calls = 0\n      Rails.logger.info \"Circuit breaker half-open for #{@provider}\"\n      execute_half_open(&amp;block)\n    else\n      raise CircuitBreakerOpenError, \"Circuit breaker is open for #{@provider}\"\n    end\n  end\n\n  def execute_half_open(&amp;block)\n    begin\n      result = block.call\n      @half_open_calls += 1\n\n      if @half_open_calls &gt;= HALF_OPEN_MAX_CALLS\n        @state = :closed\n        reset_failure_count\n        Rails.logger.info \"Circuit breaker closed for #{@provider}\"\n      end\n\n      result\n    rescue StandardError =&gt; e\n      @state = :open\n      @last_failure_time = Time.current\n      Rails.logger.error \"Circuit breaker reopened for #{@provider}\"\n      raise e\n    end\n  end\nend\n</code></pre></p>"},{"location":"reference/llm-integration/#rate-limiting","title":"Rate Limiting","text":"<p>Request Throttling: <pre><code>class RateLimitManager\n  def initialize\n    @request_timestamps = Hash.new { |h, k| h[k] = [] }\n    @rate_limits = {\n      openai: { requests_per_minute: 60, tokens_per_minute: 150_000 },\n      anthropic: { requests_per_minute: 50, tokens_per_minute: 100_000 },\n      google: { requests_per_minute: 100, tokens_per_minute: 200_000 }\n    }\n  end\n\n  def throttle_request(provider, estimated_tokens)\n    cleanup_old_timestamps(provider)\n\n    current_requests = @request_timestamps[provider].length\n    current_tokens = calculate_current_token_usage(provider)\n\n    limits = @rate_limits[provider]\n\n    # Check request rate limit\n    if current_requests &gt;= limits[:requests_per_minute]\n      delay = calculate_request_delay(provider)\n      Rails.logger.info \"Rate limiting: waiting #{delay}s for #{provider}\"\n      sleep(delay)\n    end\n\n    # Check token rate limit\n    if current_tokens + estimated_tokens &gt; limits[:tokens_per_minute]\n      delay = calculate_token_delay(provider, estimated_tokens)\n      Rails.logger.info \"Token rate limiting: waiting #{delay}s for #{provider}\"\n      sleep(delay)\n    end\n\n    # Record this request\n    @request_timestamps[provider] &lt;&lt; Time.current\n  end\n\n  private\n\n  def cleanup_old_timestamps(provider)\n    cutoff = Time.current - 60 # Remove timestamps older than 1 minute\n    @request_timestamps[provider].reject! { |timestamp| timestamp &lt; cutoff }\n  end\nend\n</code></pre></p> <p>Queue Management: <pre><code>class LLMRequestQueue\n  def initialize\n    @queues = {\n      high_priority: [],\n      normal_priority: [],\n      low_priority: []\n    }\n    @processing = false\n  end\n\n  def enqueue_request(request, priority: :normal_priority)\n    @queues[priority] &lt;&lt; {\n      request: request,\n      timestamp: Time.current,\n      retries: 0\n    }\n\n    process_queue unless @processing\n  end\n\n  def process_queue\n    @processing = true\n\n    while (item = get_next_item)\n      begin\n        result = process_request(item[:request])\n        item[:request][:callback].call(result) if item[:request][:callback]\n\n      rescue RateLimitError =&gt; e\n        if item[:retries] &lt; 3\n          item[:retries] += 1\n          delay = extract_retry_delay(e) || 60\n\n          # Re-queue with delay\n          Thread.new do\n            sleep(delay)\n            @queues[:normal_priority] &lt;&lt; item\n          end\n        else\n          handle_failed_request(item, e)\n        end\n\n      rescue StandardError =&gt; e\n        handle_failed_request(item, e)\n      end\n    end\n\n    @processing = false\n  end\n\n  private\n\n  def get_next_item\n    # Process high priority first, then normal, then low\n    [:high_priority, :normal_priority, :low_priority].each do |priority|\n      return @queues[priority].shift unless @queues[priority].empty?\n    end\n    nil\n  end\nend\n</code></pre></p> <p>Backoff Strategies: <pre><code>class AdaptiveBackoffStrategy\n  def initialize\n    @base_delay = 1.0\n    @max_delay = 300.0 # 5 minutes\n    @backoff_multiplier = 2.0\n    @jitter_factor = 0.1\n  end\n\n  def calculate_delay(attempt, error_type = :generic)\n    base = case error_type\n           when :rate_limit\n             @base_delay * 2 # Longer delays for rate limits\n           when :server_error\n             @base_delay * 1.5\n           else\n             @base_delay\n           end\n\n    # Exponential backoff with jitter\n    delay = base * (@backoff_multiplier ** (attempt - 1))\n    delay = [@max_delay, delay].min\n\n    # Add jitter to prevent thundering herd\n    jitter = delay * @jitter_factor * rand\n    delay + jitter\n  end\n\n  def calculate_rate_limit_delay(error_response)\n    # Try to extract suggested delay from error response\n    if match = error_response.match(/retry.*?(\\d+).*?second/i)\n      match[1].to_i + rand(5) # Add small jitter\n    elsif match = error_response.match(/retry.*?(\\d+).*?minute/i)\n      (match[1].to_i * 60) + rand(30)\n    else\n      60 + rand(30) # Default 60-90 seconds\n    end\n  end\nend\n</code></pre></p> <p>Cost Management: <pre><code>class CostManager\n  def initialize\n    @daily_spend = 0.0\n    @monthly_spend = 0.0\n    @cost_per_provider = Hash.new(0.0)\n    @cost_tracking_enabled = true\n  end\n\n  def track_request_cost(provider, tokens_used, model)\n    return unless @cost_tracking_enabled\n\n    cost = calculate_cost(provider, tokens_used, model)\n\n    @daily_spend += cost\n    @monthly_spend += cost\n    @cost_per_provider[provider] += cost\n\n    # Check budget limits\n    check_budget_limits(cost)\n\n    # Log cost tracking\n    Rails.logger.info \"LLM cost tracking: #{provider}/#{model} - #{tokens_used} tokens - $#{cost.round(4)}\"\n  end\n\n  private\n\n  def calculate_cost(provider, tokens, model)\n    rate = PRICING_TABLE[provider][model] || 0.002 # Fallback rate\n    (tokens / 1000.0) * rate\n  end\n\n  def check_budget_limits\n    daily_limit = Ragdoll.config.cost_optimization[:budget_limits][:daily]\n    monthly_limit = Ragdoll.config.cost_optimization[:budget_limits][:monthly]\n\n    if @daily_spend &gt; daily_limit * 0.9 # 90% of limit\n      Rails.logger.warn \"Approaching daily budget limit: $#{@daily_spend}/$#{daily_limit}\"\n\n      if @daily_spend &gt; daily_limit\n        raise BudgetExceededError, \"Daily budget limit exceeded: $#{@daily_spend}\"\n      end\n    end\n  end\nend\n</code></pre></p>"},{"location":"reference/llm-integration/#best-practices","title":"Best Practices","text":"<p>Implementing LLM integration effectively requires careful consideration of provider characteristics, cost optimization, security, and quality assurance. These best practices are derived from production deployments and real-world experience.</p>"},{"location":"reference/llm-integration/#provider-specific-optimization","title":"Provider-Specific Optimization","text":"<p>OpenAI Optimization: <pre><code># OpenAI-specific optimizations\nconfig.openai_optimization = {\n  # Use streaming for long responses\n  enable_streaming: true,\n\n  # Optimize token usage\n  token_optimization: {\n    max_tokens: 4000,\n    temperature: 0.3,    # Lower for consistent results\n    top_p: 0.9,         # Nucleus sampling\n    frequency_penalty: 0.1,\n    presence_penalty: 0.1\n  },\n\n  # Batch embeddings for cost efficiency\n  embedding_batch_size: 100,\n\n  # Use cheaper models for simple tasks\n  model_selection: {\n    simple_tasks: \"gpt-3.5-turbo\",\n    complex_tasks: \"gpt-4o\",\n    embeddings: \"text-embedding-3-small\"\n  }\n}\n\nclass OptimizedOpenAIService\n  def generate_summary(content)\n    # Use gpt-3.5-turbo for short content, gpt-4o for complex content\n    model = content.length &lt; 2000 ? \"gpt-3.5-turbo\" : \"gpt-4o\"\n\n    # Optimize prompt for OpenAI\n    prompt = build_openai_optimized_prompt(content)\n\n    RubyLLM.chat.with_model(model)\n           .with_temperature(0.3)\n           .with_max_tokens(300)\n           .add_message(role: \"user\", content: prompt)\n           .complete\n  end\n\n  private\n\n  def build_openai_optimized_prompt(content)\n    # OpenAI responds well to structured prompts\n    &lt;&lt;~PROMPT\n      Task: Create a concise summary of the following content.\n\n      Requirements:\n      - Maximum 250 words\n      - Focus on key points and main themes\n      - Use clear, professional language\n\n      Content:\n      #{content}\n\n      Summary:\n    PROMPT\n  end\nend\n</code></pre></p> <p>Anthropic (Claude) Optimization: <pre><code># Claude-specific optimizations\nconfig.anthropic_optimization = {\n  # Claude handles long context very well\n  max_context_length: 100_000,\n\n  # Optimize for Claude's strengths\n  use_cases: {\n    document_analysis: \"claude-3-sonnet\",    # Excellent for documents\n    creative_writing: \"claude-3-opus\",      # Best creative capabilities\n    cost_effective: \"claude-3-haiku\"        # Fast and cheap\n  },\n\n  # Claude-optimized parameters\n  generation_params: {\n    temperature: 0.1,     # Claude is naturally creative\n    max_tokens: 1000\n  }\n}\n\nclass OptimizedClaudeService\n  def analyze_document(content)\n    # Claude excels at document analysis\n    prompt = build_claude_optimized_prompt(content)\n\n    RubyLLM.chat.with_model(\"claude-3-sonnet\")\n           .with_temperature(0.1)\n           .add_message(role: \"user\", content: prompt)\n           .complete\n  end\n\n  private\n\n  def build_claude_optimized_prompt(content)\n    # Claude prefers conversational, detailed prompts\n    &lt;&lt;~PROMPT\n      I need you to analyze this document and provide insights. Please:\n\n      1. Identify the main themes and topics\n      2. Extract key facts and data points\n      3. Summarize the document's purpose and conclusions\n      4. Note any important technical details\n\n      Here's the document to analyze:\n\n      #{content}\n\n      Please provide a thorough analysis following the structure above.\n    PROMPT\n  end\nend\n</code></pre></p> <p>Local Model (Ollama) Optimization: <pre><code># Ollama-specific optimizations\nconfig.ollama_optimization = {\n  # Optimize for local hardware\n  gpu_acceleration: true,\n  memory_management: {\n    model_cache_size: \"8GB\",\n    concurrent_requests: 4\n  },\n\n  # Model selection for different tasks\n  models: {\n    general: \"llama2:7b\",\n    code: \"codellama:7b\",\n    embedding: \"nomic-embed-text\"\n  },\n\n  # Performance tuning\n  generation_params: {\n    temperature: 0.7,\n    num_ctx: 4096,      # Context window\n    num_predict: 512,   # Max prediction tokens\n    repeat_penalty: 1.1\n  }\n}\n\nclass OptimizedOllamaService\n  def initialize\n    @model_cache = {}\n    @request_queue = Queue.new\n    setup_worker_threads\n  end\n\n  def generate_text(prompt, model: \"llama2:7b\")\n    # Warm up model if not cached\n    warm_up_model(model) unless @model_cache[model]\n\n    # Use optimized prompt for local models\n    optimized_prompt = optimize_for_local_model(prompt)\n\n    RubyLLM.chat.with_model(model)\n           .with_temperature(0.7)\n           .add_message(role: \"user\", content: optimized_prompt)\n           .complete\n  end\n\n  private\n\n  def optimize_for_local_model(prompt)\n    # Local models benefit from more structured prompts\n    \"### Instruction:\\n#{prompt}\\n\\n### Response:\"\n  end\nend\n</code></pre></p>"},{"location":"reference/llm-integration/#model-fine-tuning-approaches","title":"Model Fine-Tuning Approaches","text":"<p>Domain-Specific Fine-Tuning: <pre><code>class FineTuningManager\n  def self.prepare_training_data(domain:, documents:)\n    training_examples = []\n\n    documents.each do |doc|\n      # Create training examples from document content\n      examples = case domain\n                 when :legal\n                   extract_legal_examples(doc)\n                 when :medical\n                   extract_medical_examples(doc)\n                 when :technical\n                   extract_technical_examples(doc)\n                 end\n\n      training_examples.concat(examples)\n    end\n\n    # Format for OpenAI fine-tuning\n    format_for_openai_training(training_examples)\n  end\n\n  def self.create_fine_tuned_model(training_file:, base_model: \"gpt-3.5-turbo\")\n    # Submit fine-tuning job to OpenAI\n    client = OpenAI::Client.new\n\n    response = client.fine_tuning.create(\n      training_file: training_file,\n      model: base_model,\n      hyperparameters: {\n        n_epochs: 3,\n        batch_size: 16,\n        learning_rate: 0.0001\n      }\n    )\n\n    response[\"id\"]\n  end\nend\n</code></pre></p>"},{"location":"reference/llm-integration/#prompt-engineering","title":"Prompt Engineering","text":"<p>Effective Prompt Patterns: <pre><code>module PromptEngineering\n  # Chain-of-thought prompting\n  def self.build_cot_prompt(question, context: nil)\n    prompt = \"Let's approach this step-by-step:\\n\\n\"\n    prompt += \"Context: #{context}\\n\\n\" if context\n    prompt += \"Question: #{question}\\n\\n\"\n    prompt += \"Let's think through this carefully:\\n\"\n    prompt += \"1) First, I'll identify the key elements...\\n\"\n    prompt += \"2) Then, I'll analyze the relationships...\\n\"\n    prompt += \"3) Finally, I'll draw conclusions...\\n\\n\"\n    prompt += \"Step-by-step analysis:\"\n    prompt\n  end\n\n  # Few-shot prompting\n  def self.build_few_shot_prompt(task, examples, input)\n    prompt = \"Here are some examples of #{task}:\\n\\n\"\n\n    examples.each_with_index do |example, index|\n      prompt += \"Example #{index + 1}:\\n\"\n      prompt += \"Input: #{example[:input]}\\n\"\n      prompt += \"Output: #{example[:output]}\\n\\n\"\n    end\n\n    prompt += \"Now, please #{task} for the following:\\n\"\n    prompt += \"Input: #{input}\\n\"\n    prompt += \"Output:\"\n    prompt\n  end\n\n  # Role-based prompting\n  def self.build_role_prompt(role, task, content)\n    &lt;&lt;~PROMPT\n      You are a #{role} with extensive experience in your field.\n\n      Your task: #{task}\n\n      Please approach this with your professional expertise and provide:\n      - Clear, accurate analysis\n      - Relevant professional insights\n      - Actionable recommendations\n\n      Content to analyze:\n      #{content}\n\n      Your professional analysis:\n    PROMPT\n  end\nend\n\n# Usage examples\nclass SmartPromptService\n  include PromptEngineering\n\n  def generate_technical_summary(content)\n    prompt = build_role_prompt(\n      \"senior technical writer\",\n      \"create a concise technical summary\",\n      content\n    )\n\n    generate_with_prompt(prompt)\n  end\n\n  def extract_key_insights(content)\n    examples = [\n      {\n        input: \"Quarterly revenue increased by 15% due to strong product sales...\",\n        output: \"Revenue growth: +15%, Driver: Strong product sales, Time: Quarterly\"\n      },\n      {\n        input: \"Customer satisfaction scores improved following the new support system...\",\n        output: \"Customer satisfaction: Improved, Cause: New support system, Impact: Positive\"\n      }\n    ]\n\n    prompt = build_few_shot_prompt(\"extract key insights\", examples, content)\n    generate_with_prompt(prompt)\n  end\nend\n</code></pre></p>"},{"location":"reference/llm-integration/#quality-assessment","title":"Quality Assessment","text":"<p>Automated Quality Metrics: <pre><code>class QualityAssessmentService\n  def assess_summary_quality(summary, original_content)\n    metrics = {}\n\n    # Semantic similarity\n    metrics[:semantic_similarity] = calculate_semantic_similarity(summary, original_content)\n\n    # Information coverage\n    metrics[:coverage] = calculate_information_coverage(summary, original_content)\n\n    # Coherence score\n    metrics[:coherence] = calculate_coherence_score(summary)\n\n    # Factual accuracy\n    metrics[:factual_accuracy] = verify_factual_accuracy(summary, original_content)\n\n    # Length appropriateness\n    metrics[:length_score] = assess_length_appropriateness(summary, original_content)\n\n    # Overall quality score\n    metrics[:overall_score] = calculate_overall_score(metrics)\n\n    metrics\n  end\n\n  def assess_keyword_quality(keywords, content)\n    {\n      relevance: calculate_keyword_relevance(keywords, content),\n      coverage: calculate_keyword_coverage(keywords, content),\n      specificity: calculate_keyword_specificity(keywords),\n      uniqueness: calculate_keyword_uniqueness(keywords)\n    }\n  end\n\n  private\n\n  def calculate_semantic_similarity(text1, text2)\n    embedding1 = embedding_service.generate_embedding(text1)\n    embedding2 = embedding_service.generate_embedding(text2)\n\n    cosine_similarity(embedding1, embedding2)\n  end\n\n  def calculate_information_coverage(summary, original)\n    # Extract key concepts from both texts\n    original_concepts = extract_key_concepts(original)\n    summary_concepts = extract_key_concepts(summary)\n\n    # Calculate coverage ratio\n    covered_concepts = original_concepts &amp; summary_concepts\n    covered_concepts.length.to_f / original_concepts.length\n  end\nend\n\n# Quality monitoring and alerting\nclass QualityMonitor\n  QUALITY_THRESHOLDS = {\n    semantic_similarity: 0.75,\n    information_coverage: 0.80,\n    coherence: 0.85,\n    factual_accuracy: 0.90\n  }\n\n  def monitor_quality(result, type)\n    quality_scores = assess_quality(result, type)\n\n    # Check for quality issues\n    issues = []\n    QUALITY_THRESHOLDS.each do |metric, threshold|\n      if quality_scores[metric] &lt; threshold\n        issues &lt;&lt; \"#{metric}: #{quality_scores[metric]} (threshold: #{threshold})\"\n      end\n    end\n\n    # Alert if quality issues found\n    if issues.any?\n      alert_quality_issues(type, issues, quality_scores)\n    end\n\n    quality_scores\n  end\n\n  private\n\n  def alert_quality_issues(type, issues, scores)\n    Rails.logger.warn \"Quality issues detected for #{type}:\"\n    issues.each { |issue| Rails.logger.warn \"  - #{issue}\" }\n\n    # Send notification if configured\n    if Rails.env.production?\n      QualityAlertMailer.quality_degradation(\n        type: type,\n        issues: issues,\n        scores: scores\n      ).deliver_now\n    end\n  end\nend\n</code></pre></p>"},{"location":"reference/llm-integration/#production-deployment-checklist","title":"Production Deployment Checklist","text":"<pre><code># Production readiness checklist\nclass ProductionReadinessChecker\n  def self.check_readiness\n    checks = {\n      api_keys_configured: check_api_keys,\n      rate_limits_configured: check_rate_limits,\n      error_handling_enabled: check_error_handling,\n      monitoring_setup: check_monitoring,\n      cost_controls_active: check_cost_controls,\n      quality_thresholds_set: check_quality_thresholds,\n      backup_providers_configured: check_backup_providers\n    }\n\n    failed_checks = checks.select { |check, passed| !passed }\n\n    if failed_checks.any?\n      raise ProductionReadinessError, \"Failed checks: #{failed_checks.keys.join(', ')}\"\n    end\n\n    true\n  end\n\n  private\n\n  def self.check_api_keys\n    required_keys = %w[OPENAI_API_KEY ANTHROPIC_API_KEY]\n    required_keys.all? { |key| ENV[key].present? }\n  end\n\n  def self.check_rate_limits\n    Ragdoll.config.respond_to?(:rate_limits) &amp;&amp;\n    Ragdoll.config.rate_limits.present?\n  end\n\n  def self.check_cost_controls\n    Ragdoll.config.cost_optimization[:enable] &amp;&amp;\n    Ragdoll.config.cost_optimization[:budget_limits].present?\n  end\nend\n</code></pre> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"reference/metadata-schemas/","title":"Metadata Schemas","text":"<p>Ragdoll implements a sophisticated metadata schema system that ensures consistent, structured metadata generation across different content types. The system uses JSON Schema-based definitions to guide LLM-powered metadata extraction and validate the resulting structured data.</p>"},{"location":"reference/metadata-schemas/#structured-content-analysis-and-validation","title":"Structured Content Analysis and Validation","text":"<p>The metadata schema system provides:</p> <ul> <li>Content-Type Specific Schemas: Tailored metadata structures for text, images, audio, PDFs, and multi-modal content</li> <li>LLM-Guided Generation: Schema-aware prompts ensure consistent metadata format and quality</li> <li>Validation Framework: Automatic validation of generated metadata against defined schemas</li> <li>Extensible Architecture: Easy addition of new content types and custom schema definitions</li> <li>Dual Metadata System: Separation of LLM-generated content metadata and system file metadata</li> <li>Quality Assurance: Built-in fallback strategies and error handling for robust metadata generation</li> </ul>"},{"location":"reference/metadata-schemas/#schema-architecture","title":"Schema Architecture","text":"<p>Ragdoll uses a dual metadata architecture that separates concerns between AI-generated content insights and technical file properties:</p>"},{"location":"reference/metadata-schemas/#dual-metadata-system","title":"Dual Metadata System","text":"<p>LLM-Generated Content Metadata (<code>metadata</code> column) <pre><code># Stored in document.metadata (JSON column)\n{\n  \"summary\": \"This research paper explores machine learning applications...\",\n  \"keywords\": [\"machine learning\", \"neural networks\", \"AI\"],\n  \"classification\": \"research\",\n  \"topics\": [\"artificial intelligence\", \"computer science\"],\n  \"sentiment\": \"neutral\",\n  \"complexity_level\": \"advanced\",\n  \"reading_time_minutes\": 25,\n  \"language\": \"en\",\n  \"tags\": [\"AI\", \"research\", \"academic\"]\n}\n</code></pre></p> <p>System-Generated File Metadata (<code>file_metadata</code> column) <pre><code># Stored in document.file_metadata (JSON column)\n{\n  \"file_size\": 2048576,\n  \"file_type\": \"pdf\",\n  \"page_count\": 15,\n  \"creation_date\": \"2024-01-15T10:30:00Z\",\n  \"modification_date\": \"2024-01-20T14:15:30Z\",\n  \"author\": \"Dr. Jane Smith\",\n  \"title\": \"Advanced ML Techniques\",\n  \"encoding\": \"UTF-8\",\n  \"extraction_method\": \"pdf-reader\",\n  \"processing_time_ms\": 1250\n}\n</code></pre></p> <p>Schema Separation Rationale: 1. Semantic vs Technical: Content metadata focuses on meaning, file metadata on technical properties 2. LLM vs System Generated: Different generation methods require different validation approaches 3. Update Patterns: Content metadata may be regenerated, file metadata is typically static 4. Search Optimization: Separate indexes for semantic search vs file property filtering 5. Schema Evolution: Content schemas evolve with AI capabilities, file schemas remain stable</p> <p>Integration Patterns: <pre><code># Combined search across both metadata types\nDocument.search_combined(\n  content_query: \"machine learning\",  # Searches content metadata\n  file_filters: { file_type: 'pdf', page_count: 10..50 }  # Filters file metadata\n)\n\n# Unified metadata access\ndocument.combined_metadata  # Merges both metadata types\ndocument.searchable_content # Optimized for search indexing\n</code></pre></p>"},{"location":"reference/metadata-schemas/#content-type-schemas","title":"Content Type Schemas","text":"<p>Ragdoll provides specialized schemas for different content types:</p> <pre><code>graph TD\n    A[Base Schema] --&gt; B[Text Schema]\n    A --&gt; C[Image Schema]\n    A --&gt; D[Audio Schema]\n    A --&gt; E[PDF Schema]\n    A --&gt; F[Mixed Schema]\n\n    B --&gt; B1[\"summary, keywords, classification\"]\n    C --&gt; C1[\"description, objects, scene_type\"]\n    D --&gt; D1[\"transcript_summary, speakers, content_type\"]\n    E --&gt; E1[\"document_type, structure, reading_time\"]\n    F --&gt; F1[\"content_types, cohesion_analysis\"]</code></pre> <p>Schema Inheritance Pattern: <pre><code>module MetadataSchemas\n  # Base fields common to all content types\n  BASE_PROPERTIES = {\n    summary: { type: \"string\", description: \"Content summary\" },\n    keywords: { type: \"array\", items: { type: \"string\" } },\n    classification: { type: \"string\" },\n    tags: { type: \"array\", items: { type: \"string\" } }\n  }.freeze\n\n  # Content-specific extensions\n  TEXT_EXTENSIONS = {\n    reading_time_minutes: { type: \"integer\" },\n    complexity_level: { type: \"string\", enum: %w[beginner intermediate advanced expert] },\n    sentiment: { type: \"string\", enum: %w[positive negative neutral mixed] }\n  }.freeze\n\n  # Combined schema\n  TEXT_SCHEMA = {\n    type: \"object\",\n    properties: BASE_PROPERTIES.merge(TEXT_EXTENSIONS),\n    required: %w[summary keywords classification]\n  }.freeze\nend\n</code></pre></p>"},{"location":"reference/metadata-schemas/#standard-metadata-fields","title":"Standard Metadata Fields","text":"<p>Ragdoll defines comprehensive metadata fields organized by function and content type:</p>"},{"location":"reference/metadata-schemas/#content-analysis-fields","title":"Content Analysis Fields","text":""},{"location":"reference/metadata-schemas/#summary-generation","title":"Summary Generation","text":"<pre><code># Text content summary\nsummary: {\n  type: \"string\",\n  description: \"Concise summary of the text content (2-3 sentences)\",\n  min_length: 50,\n  max_length: 500,\n  required: true\n}\n\n# Image content summary\nsummary: {\n  type: \"string\",\n  description: \"Brief summary of the image content (1 sentence)\",\n  max_length: 200,\n  required: true\n}\n\n# Multi-modal summary\nsummary: {\n  type: \"string\",\n  description: \"Overall summary combining all content types in the document\",\n  max_length: 600,\n  required: true\n}\n</code></pre>"},{"location":"reference/metadata-schemas/#keyword-extraction","title":"Keyword Extraction","text":"<pre><code># Standard keyword field (all content types)\nkeywords: {\n  type: \"array\",\n  items: { type: \"string\" },\n  description: \"Relevant keywords and phrases extracted from content\",\n  maxItems: 10,\n  minItems: 3,\n  required: true,\n  validation: {\n    min_word_length: 3,\n    no_stopwords: true,\n    unique_only: true\n  }\n}\n\n# Enhanced keywords with confidence scores\nkeywords_enhanced: {\n  type: \"array\",\n  items: {\n    type: \"object\",\n    properties: {\n      term: { type: \"string\" },\n      confidence: { type: \"number\", minimum: 0, maximum: 1 },\n      category: { type: \"string\", enum: %w[concept entity action descriptor] }\n    }\n  },\n  maxItems: 15\n}\n</code></pre>"},{"location":"reference/metadata-schemas/#topic-classification","title":"Topic Classification","text":"<pre><code># Primary classification\nclassification: {\n  type: \"string\",\n  enum: {\n    text: %w[research article blog documentation technical legal financial marketing other],\n    image: %w[technical diagram photo artwork chart screenshot document other],\n    audio: %w[educational entertainment business technical musical interview podcast other],\n    pdf: %w[academic business legal technical manual report presentation other]\n  },\n  required: true\n}\n\n# Detailed topics array\ntopics: {\n  type: \"array\",\n  items: { type: \"string\" },\n  description: \"Main topics discussed in the document\",\n  maxItems: 5,\n  validation: {\n    hierarchical: true,  # Support parent::child topic structure\n    taxonomy_validation: true\n  }\n}\n\n# Classification confidence\nclassification_confidence: {\n  type: \"number\",\n  minimum: 0,\n  maximum: 1,\n  description: \"Confidence score for the assigned classification\"\n}\n</code></pre>"},{"location":"reference/metadata-schemas/#sentiment-analysis","title":"Sentiment Analysis","text":"<pre><code># Basic sentiment (text content)\nsentiment: {\n  type: \"string\",\n  enum: %w[positive negative neutral mixed],\n  description: \"Overall sentiment of the text\"\n}\n\n# Detailed sentiment analysis\nsentiment_detailed: {\n  type: \"object\",\n  properties: {\n    overall: { type: \"string\", enum: %w[positive negative neutral mixed] },\n    confidence: { type: \"number\", minimum: 0, maximum: 1 },\n    emotional_tone: {\n      type: \"array\",\n      items: { type: \"string\" },\n      enum: %w[joy sadness anger fear disgust surprise trust anticipation]\n    },\n    subjectivity: { type: \"string\", enum: %w[objective subjective] }\n  }\n}\n\n# Mood for images and audio\nmood: {\n  type: \"string\",\n  enum: {\n    image: %w[professional casual formal technical artistic dramatic serene energetic other],\n    audio: %w[formal casual energetic calm professional educational entertaining informative other]\n  },\n  description: \"Overall mood or tone of the content\"\n}\n</code></pre>"},{"location":"reference/metadata-schemas/#content-categorization","title":"Content Categorization","text":"<pre><code># Complexity assessment\ncomplexity_level: {\n  type: \"string\",\n  enum: %w[beginner intermediate advanced expert],\n  description: \"Complexity/difficulty level of the content\",\n  scoring_criteria: {\n    beginner: \"Basic concepts, simple language, introductory material\",\n    intermediate: \"Some specialized knowledge required, moderate complexity\",\n    advanced: \"Specialized knowledge required, complex concepts\",\n    expert: \"Deep expertise required, highly technical content\"\n  }\n}\n\n# Reading time estimation\nreading_time_minutes: {\n  type: \"integer\",\n  minimum: 1,\n  maximum: 600,  # 10 hours max\n  description: \"Estimated reading time in minutes\",\n  calculation: \"Based on 200-250 words per minute average reading speed\"\n}\n\n# Language detection\nlanguage: {\n  type: \"string\",\n  pattern: \"^[a-z]{2}(-[A-Z]{2})?$\",  # ISO 639-1 format\n  description: \"Primary language of the content\",\n  examples: [\"en\", \"es\", \"fr\", \"de\", \"zh-CN\"]\n}\n\n# User-defined tags\ntags: {\n  type: \"array\",\n  items: { type: \"string\" },\n  description: \"User-defined or AI-suggested tags for organization\",\n  maxItems: 20,\n  validation: {\n    no_spaces: false,  # Allow multi-word tags\n    lowercase: true,\n    unique_only: true\n  }\n}\n</code></pre>"},{"location":"reference/metadata-schemas/#technical-metadata","title":"Technical Metadata","text":""},{"location":"reference/metadata-schemas/#processing-parameters","title":"Processing Parameters","text":"<pre><code># Content processing metadata (stored in file_metadata)\nprocessing_metadata: {\n  extraction_method: { type: \"string\" },  # \"pdf-reader\", \"docx\", \"image-magick\"\n  processing_time_ms: { type: \"integer\" },\n  embedding_model: { type: \"string\" },\n  embedding_dimensions: { type: \"integer\" },\n  chunk_count: { type: \"integer\" },\n  chunk_strategy: { type: \"string\" },\n  content_hash: { type: \"string\" },  # For change detection\n  last_processed_at: { type: \"string\", format: \"date-time\" }\n}\n</code></pre>"},{"location":"reference/metadata-schemas/#quality-metrics","title":"Quality Metrics","text":"<pre><code># Content quality assessment\nquality_metrics: {\n  content_completeness: { type: \"number\", minimum: 0, maximum: 1 },\n  extraction_confidence: { type: \"number\", minimum: 0, maximum: 1 },\n  metadata_completeness: { type: \"number\", minimum: 0, maximum: 1 },\n  validation_score: { type: \"number\", minimum: 0, maximum: 1 },\n  overall_quality: { type: \"number\", minimum: 0, maximum: 1 }\n}\n</code></pre>"},{"location":"reference/metadata-schemas/#performance-data","title":"Performance Data","text":"<pre><code># Performance tracking metadata\nperformance_data: {\n  file_size_bytes: { type: \"integer\" },\n  processing_duration_ms: { type: \"integer\" },\n  embedding_generation_time_ms: { type: \"integer\" },\n  metadata_generation_time_ms: { type: \"integer\" },\n  memory_usage_mb: { type: \"number\" },\n  cpu_usage_percent: { type: \"number\" },\n  api_calls_made: { type: \"integer\" },\n  cost_estimate_usd: { type: \"number\" }\n}\n</code></pre>"},{"location":"reference/metadata-schemas/#schema-validation","title":"Schema Validation","text":"<p>Ragdoll implements comprehensive validation to ensure metadata quality and consistency:</p>"},{"location":"reference/metadata-schemas/#validation-rules","title":"Validation Rules","text":""},{"location":"reference/metadata-schemas/#required-fields-validation","title":"Required Fields Validation","text":"<pre><code># Schema-based required field checking\ndef self.validate_metadata(document_type, metadata)\n  schema = schema_for(document_type)\n  required_fields = schema[:required] || []\n  errors = []\n\n  required_fields.each do |field|\n    unless metadata.key?(field) &amp;&amp; !metadata[field].to_s.strip.empty?\n      errors &lt;&lt; \"Missing required field: #{field}\"\n    end\n  end\n\n  errors\nend\n\n# Content-specific required fields\nTEXT_SCHEMA[:required] = %w[summary keywords classification]\nIMAGE_SCHEMA[:required] = %w[description summary scene_type classification]\nAUDIO_SCHEMA[:required] = %w[summary content_type classification]\nPDF_SCHEMA[:required] = %w[summary document_type classification]\nMIXED_SCHEMA[:required] = %w[summary content_types primary_content_type classification]\n</code></pre>"},{"location":"reference/metadata-schemas/#data-type-constraints","title":"Data Type Constraints","text":"<pre><code># Type validation with coercion\nclass MetadataValidator\n  def self.validate_field_type(field_name, value, field_schema)\n    expected_type = field_schema[:type]\n\n    case expected_type\n    when 'string'\n      validate_string_field(field_name, value, field_schema)\n    when 'array'\n      validate_array_field(field_name, value, field_schema)\n    when 'integer'\n      validate_integer_field(field_name, value, field_schema)\n    when 'number'\n      validate_number_field(field_name, value, field_schema)\n    when 'boolean'\n      validate_boolean_field(field_name, value, field_schema)\n    when 'object'\n      validate_object_field(field_name, value, field_schema)\n    else\n      [\"Unknown field type: #{expected_type}\"]\n    end\n  end\n\n  private\n\n  def self.validate_string_field(field_name, value, schema)\n    errors = []\n\n    unless value.is_a?(String)\n      return [\"#{field_name} must be a string, got #{value.class}\"]\n    end\n\n    # Length constraints\n    if schema[:minLength] &amp;&amp; value.length &lt; schema[:minLength]\n      errors &lt;&lt; \"#{field_name} must be at least #{schema[:minLength]} characters\"\n    end\n\n    if schema[:maxLength] &amp;&amp; value.length &gt; schema[:maxLength]\n      errors &lt;&lt; \"#{field_name} must be no more than #{schema[:maxLength]} characters\"\n    end\n\n    # Enum validation\n    if schema[:enum] &amp;&amp; !schema[:enum].include?(value)\n      errors &lt;&lt; \"#{field_name} must be one of: #{schema[:enum].join(', ')}\"\n    end\n\n    # Pattern validation\n    if schema[:pattern] &amp;&amp; !value.match?(Regexp.new(schema[:pattern]))\n      errors &lt;&lt; \"#{field_name} does not match required pattern\"\n    end\n\n    errors\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#format-validation","title":"Format Validation","text":"<pre><code># Specialized format validators\nclass FormatValidators\n  # Language code validation (ISO 639-1)\n  def self.validate_language_code(code)\n    valid_codes = %w[en es fr de it pt ru ja ko zh ar hi]\n    return true if valid_codes.include?(code)\n    return true if code.match?(/^[a-z]{2}-[A-Z]{2}$/)  # en-US format\n    false\n  end\n\n  # Keyword validation\n  def self.validate_keywords(keywords)\n    errors = []\n\n    return [\"Keywords must be an array\"] unless keywords.is_a?(Array)\n\n    keywords.each_with_index do |keyword, index|\n      unless keyword.is_a?(String)\n        errors &lt;&lt; \"Keyword at index #{index} must be a string\"\n        next\n      end\n\n      if keyword.length &lt; 3\n        errors &lt;&lt; \"Keyword '#{keyword}' must be at least 3 characters\"\n      end\n\n      if keyword.length &gt; 50\n        errors &lt;&lt; \"Keyword '#{keyword}' must be no more than 50 characters\"\n      end\n\n      if keyword.match?(/^\\d+$/)  # Only numbers\n        errors &lt;&lt; \"Keyword '#{keyword}' cannot be only numbers\"\n      end\n    end\n\n    # Check for duplicates\n    duplicates = keywords.group_by(&amp;:downcase).select { |k, v| v.size &gt; 1 }.keys\n    if duplicates.any?\n      errors &lt;&lt; \"Duplicate keywords found: #{duplicates.join(', ')}\"\n    end\n\n    errors\n  end\n\n  # URL validation for image sources\n  def self.validate_url(url)\n    return true if url.nil? || url.empty?\n\n    begin\n      uri = URI.parse(url)\n      uri.is_a?(URI::HTTP) || uri.is_a?(URI::HTTPS)\n    rescue URI::InvalidURIError\n      false\n    end\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#range-constraints","title":"Range Constraints","text":"<pre><code># Numeric range validation\nclass RangeValidator\n  def self.validate_reading_time(minutes)\n    errors = []\n\n    unless minutes.is_a?(Integer)\n      return [\"Reading time must be an integer\"]\n    end\n\n    if minutes &lt; 1\n      errors &lt;&lt; \"Reading time must be at least 1 minute\"\n    end\n\n    if minutes &gt; 600  # 10 hours\n      errors &lt;&lt; \"Reading time cannot exceed 600 minutes (10 hours)\"\n    end\n\n    # Warn for unusual values\n    if minutes &gt; 120  # 2 hours\n      errors &lt;&lt; \"Warning: Reading time of #{minutes} minutes seems unusually high\"\n    end\n\n    errors\n  end\n\n  def self.validate_confidence_score(score)\n    return [\"Confidence score must be a number\"] unless score.is_a?(Numeric)\n    return [\"Confidence score must be between 0 and 1\"] unless (0..1).cover?(score)\n    []\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#error-handling","title":"Error Handling","text":""},{"location":"reference/metadata-schemas/#validation-error-reporting","title":"Validation Error Reporting","text":"<pre><code>class ValidationErrorReporter\n  def self.generate_detailed_report(document_type, metadata, errors)\n    {\n      document_type: document_type,\n      validation_status: errors.empty? ? 'passed' : 'failed',\n      error_count: errors.length,\n      errors: errors.map { |error| format_error(error) },\n      metadata_completeness: calculate_completeness(document_type, metadata),\n      suggestions: generate_suggestions(document_type, errors),\n      schema_version: get_schema_version(document_type)\n    }\n  end\n\n  private\n\n  def self.format_error(error)\n    {\n      message: error,\n      severity: determine_severity(error),\n      field: extract_field_name(error),\n      suggestion: suggest_fix(error)\n    }\n  end\n\n  def self.determine_severity(error)\n    case error\n    when /Missing required field/\n      'critical'\n    when /must be one of/\n      'error'\n    when /Warning:/\n      'warning'\n    else\n      'error'\n    end\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#schema-compatibility-checking","title":"Schema Compatibility Checking","text":"<pre><code>class SchemaCompatibilityChecker\n  def self.check_compatibility(old_metadata, new_schema)\n    compatibility_report = {\n      compatible: true,\n      issues: [],\n      migration_required: false,\n      breaking_changes: []\n    }\n\n    # Check for removed required fields\n    old_schema = infer_schema_from_metadata(old_metadata)\n    new_required = new_schema[:required] || []\n    old_required = old_schema[:required] || []\n\n    removed_required = old_required - new_required\n    added_required = new_required - old_required\n\n    if removed_required.any?\n      compatibility_report[:issues] &lt;&lt; {\n        type: 'removed_required_fields',\n        fields: removed_required,\n        impact: 'breaking_change'\n      }\n      compatibility_report[:compatible] = false\n    end\n\n    if added_required.any?\n      compatibility_report[:issues] &lt;&lt; {\n        type: 'new_required_fields',\n        fields: added_required,\n        impact: 'migration_required'\n      }\n      compatibility_report[:migration_required] = true\n    end\n\n    compatibility_report\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#fallback-strategies","title":"Fallback Strategies","text":"<pre><code>class MetadataFallbackHandler\n  def self.apply_fallbacks(document_type, invalid_metadata, errors)\n    fallback_metadata = invalid_metadata.dup\n\n    errors.each do |error|\n      case error\n      when /Missing required field: summary/\n        fallback_metadata['summary'] = generate_fallback_summary(invalid_metadata)\n      when /Missing required field: keywords/\n        fallback_metadata['keywords'] = extract_fallback_keywords(invalid_metadata)\n      when /Missing required field: classification/\n        fallback_metadata['classification'] = infer_classification(document_type, invalid_metadata)\n      when /Invalid language code/\n        fallback_metadata['language'] = 'en'  # Default to English\n      when /Reading time.*unreasonable/\n        fallback_metadata['reading_time_minutes'] = estimate_reading_time(invalid_metadata)\n      end\n    end\n\n    # Re-validate with fallbacks applied\n    new_errors = validate_metadata(document_type, fallback_metadata)\n\n    {\n      fallback_metadata: fallback_metadata,\n      remaining_errors: new_errors,\n      fallbacks_applied: errors.length - new_errors.length\n    }\n  end\n\n  private\n\n  def self.generate_fallback_summary(metadata)\n    # Generate basic summary from available content\n    if metadata['description']\n      metadata['description'][0..200] + (metadata['description'].length &gt; 200 ? '...' : '')\n    elsif metadata['title']\n      \"Document: #{metadata['title']}\"\n    else\n      \"Content summary not available\"\n    end\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#quality-thresholds","title":"Quality Thresholds","text":"<pre><code>class QualityThresholdManager\n  QUALITY_THRESHOLDS = {\n    minimum_acceptable: 0.6,\n    good_quality: 0.8,\n    excellent_quality: 0.95\n  }.freeze\n\n  def self.assess_metadata_quality(document_type, metadata)\n    schema = MetadataSchemas.schema_for(document_type)\n    total_possible_fields = schema[:properties].keys.length\n\n    scores = {\n      completeness: calculate_completeness_score(metadata, schema),\n      accuracy: calculate_accuracy_score(metadata, schema),\n      richness: calculate_richness_score(metadata, schema),\n      consistency: calculate_consistency_score(metadata)\n    }\n\n    overall_score = (\n      scores[:completeness] * 0.3 +\n      scores[:accuracy] * 0.4 +\n      scores[:richness] * 0.2 +\n      scores[:consistency] * 0.1\n    )\n\n    {\n      overall_score: overall_score,\n      quality_level: determine_quality_level(overall_score),\n      component_scores: scores,\n      meets_threshold: overall_score &gt;= QUALITY_THRESHOLDS[:minimum_acceptable],\n      recommendations: generate_quality_recommendations(overall_score, scores)\n    }\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#custom-schemas","title":"Custom Schemas","text":"<p>Ragdoll supports creating custom metadata schemas for specialized content types and domain-specific requirements:</p>"},{"location":"reference/metadata-schemas/#schema-definition","title":"Schema Definition","text":""},{"location":"reference/metadata-schemas/#json-schema-format","title":"JSON Schema Format","text":"<pre><code># Custom schema for legal documents\nLEGAL_DOCUMENT_SCHEMA = {\n  type: \"object\",\n  schema_version: \"1.0.0\",\n  schema_id: \"legal_document_v1\",\n  description: \"Metadata schema for legal documents and contracts\",\n\n  properties: {\n    # Required base fields (inherited)\n    summary: {\n      type: \"string\",\n      description: \"Legal document summary focusing on key provisions\",\n      minLength: 100,\n      maxLength: 1000\n    },\n\n    # Legal-specific fields\n    document_type: {\n      type: \"string\",\n      enum: %w[contract agreement policy statute regulation ordinance brief motion other],\n      description: \"Type of legal document\"\n    },\n\n    jurisdiction: {\n      type: \"string\",\n      description: \"Legal jurisdiction (e.g., 'US-CA', 'UK', 'EU')\",\n      pattern: \"^[A-Z]{2}(-[A-Z]{2})?$\"\n    },\n\n    legal_areas: {\n      type: \"array\",\n      items: {\n        type: \"string\",\n        enum: %w[contract corporate employment intellectual_property real_estate family criminal civil other]\n      },\n      maxItems: 5,\n      description: \"Areas of law covered by this document\"\n    },\n\n    parties: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          name: { type: \"string\" },\n          role: { type: \"string\", enum: %w[plaintiff defendant buyer seller lessor lessee employer employee other] },\n          entity_type: { type: \"string\", enum: %w[individual corporation llc partnership government other] }\n        },\n        required: %w[name role]\n      },\n      maxItems: 10\n    },\n\n    effective_date: {\n      type: \"string\",\n      format: \"date\",\n      description: \"Date when the document becomes effective\"\n    },\n\n    expiration_date: {\n      type: \"string\",\n      format: \"date\",\n      description: \"Date when the document expires (if applicable)\"\n    },\n\n    key_terms: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          term: { type: \"string\" },\n          definition: { type: \"string\" },\n          importance: { type: \"string\", enum: %w[critical important standard] }\n        },\n        required: %w[term definition]\n      },\n      maxItems: 20\n    },\n\n    compliance_requirements: {\n      type: \"array\",\n      items: { type: \"string\" },\n      description: \"Regulatory or legal compliance requirements\"\n    },\n\n    risk_level: {\n      type: \"string\",\n      enum: %w[low medium high critical],\n      description: \"Risk assessment level for the document\"\n    }\n  },\n\n  required: %w[summary document_type jurisdiction legal_areas],\n\n  # Custom validation rules\n  custom_validators: %w[validate_jurisdiction_format validate_date_consistency validate_party_roles],\n\n  # Schema metadata\n  created_by: \"Legal Team\",\n  created_at: \"2024-01-15\",\n  compatible_with: [\"base_schema_v1\"],\n  extends: \"base_document_schema\"\n}.freeze\n</code></pre>"},{"location":"reference/metadata-schemas/#field-type-definitions","title":"Field Type Definitions","text":"<pre><code># Extended field type system\nCUSTOM_FIELD_TYPES = {\n  # Geographic types\n  \"coordinates\" =&gt; {\n    type: \"object\",\n    properties: {\n      latitude: { type: \"number\", minimum: -90, maximum: 90 },\n      longitude: { type: \"number\", minimum: -180, maximum: 180 }\n    },\n    required: %w[latitude longitude]\n  },\n\n  # Monetary types\n  \"currency_amount\" =&gt; {\n    type: \"object\",\n    properties: {\n      amount: { type: \"number\", minimum: 0 },\n      currency: { type: \"string\", pattern: \"^[A-Z]{3}$\" },  # ISO 4217\n      formatted: { type: \"string\" }\n    },\n    required: %w[amount currency]\n  },\n\n  # Person/entity types\n  \"person\" =&gt; {\n    type: \"object\",\n    properties: {\n      name: { type: \"string\" },\n      email: { type: \"string\", format: \"email\" },\n      role: { type: \"string\" },\n      organization: { type: \"string\" }\n    },\n    required: %w[name]\n  },\n\n  # Date range types\n  \"date_range\" =&gt; {\n    type: \"object\",\n    properties: {\n      start_date: { type: \"string\", format: \"date\" },\n      end_date: { type: \"string\", format: \"date\" },\n      duration_days: { type: \"integer\", minimum: 0 }\n    },\n    required: %w[start_date end_date]\n  }\n}.freeze\n</code></pre>"},{"location":"reference/metadata-schemas/#validation-rule-specification","title":"Validation Rule Specification","text":"<pre><code># Custom validation rules for specialized schemas\nclass CustomValidators\n  def self.validate_jurisdiction_format(value)\n    # US state codes, country codes, or international codes\n    valid_patterns = [\n      /^US-[A-Z]{2}$/,    # US-CA, US-NY\n      /^[A-Z]{2}$/,       # US, UK, DE\n      /^EU$/,             # European Union\n      /^UN$/              # United Nations\n    ]\n\n    return [] if valid_patterns.any? { |pattern| value.match?(pattern) }\n    [\"Invalid jurisdiction format: #{value}\"]\n  end\n\n  def self.validate_date_consistency(metadata)\n    errors = []\n\n    if metadata['effective_date'] &amp;&amp; metadata['expiration_date']\n      effective = Date.parse(metadata['effective_date'])\n      expiration = Date.parse(metadata['expiration_date'])\n\n      if effective &gt; expiration\n        errors &lt;&lt; \"Effective date cannot be after expiration date\"\n      end\n\n      if expiration &lt; Date.current\n        errors &lt;&lt; \"Warning: Document appears to be expired\"\n      end\n    end\n\n    errors\n  rescue Date::Error\n    [\"Invalid date format in date fields\"]\n  end\n\n  def self.validate_party_roles(parties)\n    return [] unless parties.is_a?(Array)\n\n    errors = []\n    role_counts = parties.group_by { |p| p['role'] }.transform_values(&amp;:count)\n\n    # Business logic validation\n    if role_counts['buyer'] &amp;&amp; role_counts['seller']\n      unless role_counts['buyer'] == role_counts['seller']\n        errors &lt;&lt; \"Number of buyers must equal number of sellers in transaction\"\n      end\n    end\n\n    # Check for conflicting roles\n    parties.each do |party|\n      name = party['name']\n      conflicting_parties = parties.select { |p| p['name'] == name &amp;&amp; p['role'] != party['role'] }\n\n      if conflicting_parties.any?\n        errors &lt;&lt; \"Party '#{name}' has conflicting roles: #{[party['role']] + conflicting_parties.map { |p| p['role'] }}\"\n      end\n    end\n\n    errors\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#documentation-requirements","title":"Documentation Requirements","text":"<pre><code># Schema documentation template\nSCHEMA_DOCUMENTATION_TEMPLATE = {\n  schema_info: {\n    name: \"Legal Document Schema\",\n    version: \"1.0.0\",\n    description: \"Comprehensive metadata schema for legal documents\",\n    use_cases: [\n      \"Contract analysis and management\",\n      \"Legal compliance tracking\",\n      \"Document classification and search\"\n    ],\n    target_documents: [\"contracts\", \"agreements\", \"policies\", \"regulations\"]\n  },\n\n  field_documentation: {\n    \"jurisdiction\" =&gt; {\n      description: \"Legal jurisdiction where document applies\",\n      examples: [\"US-CA\", \"UK\", \"EU\"],\n      validation_notes: \"Must follow ISO country codes or US state format\",\n      business_impact: \"Critical for determining applicable laws and regulations\"\n    },\n\n    \"parties\" =&gt; {\n      description: \"All parties involved in the legal document\",\n      examples: [\n        { name: \"Acme Corp\", role: \"buyer\", entity_type: \"corporation\" },\n        { name: \"John Smith\", role: \"seller\", entity_type: \"individual\" }\n      ],\n      validation_notes: \"Must include name and role for each party\",\n      business_impact: \"Essential for contract management and obligation tracking\"\n    }\n  },\n\n  usage_guidelines: {\n    best_practices: [\n      \"Always validate jurisdiction format before processing\",\n      \"Include all parties mentioned in the document\",\n      \"Use standardized role names for consistency\"\n    ],\n    common_mistakes: [\n      \"Forgetting to include all parties\",\n      \"Using non-standard jurisdiction codes\",\n      \"Inconsistent date formats\"\n    ]\n  }\n}.freeze\n</code></pre>"},{"location":"reference/metadata-schemas/#schema-registration","title":"Schema Registration","text":""},{"location":"reference/metadata-schemas/#schema-loading-mechanisms","title":"Schema Loading Mechanisms","text":"<pre><code>class CustomSchemaRegistry\n  @@registered_schemas = {}\n  @@schema_versions = {}\n\n  def self.register_schema(schema_id, schema_definition)\n    validate_schema_format!(schema_definition)\n\n    version = schema_definition[:schema_version] || '1.0.0'\n\n    @@registered_schemas[schema_id] = schema_definition\n    @@schema_versions[schema_id] ||= []\n    @@schema_versions[schema_id] &lt;&lt; version\n\n    # Register custom validators if present\n    if schema_definition[:custom_validators]\n      register_custom_validators(schema_id, schema_definition[:custom_validators])\n    end\n\n    Rails.logger.info \"Registered custom schema: #{schema_id} v#{version}\"\n  end\n\n  def self.load_schemas_from_directory(directory_path)\n    Dir.glob(File.join(directory_path, '*.rb')).each do |schema_file|\n      load_schema_file(schema_file)\n    end\n  end\n\n  def self.get_schema(schema_id, version: 'latest')\n    base_schema = @@registered_schemas[schema_id]\n    return nil unless base_schema\n\n    if version == 'latest'\n      base_schema\n    else\n      get_schema_version(schema_id, version)\n    end\n  end\n\n  private\n\n  def self.validate_schema_format!(schema)\n    required_keys = %w[type properties required]\n    missing_keys = required_keys - schema.keys.map(&amp;:to_s)\n\n    if missing_keys.any?\n      raise \"Schema missing required keys: #{missing_keys.join(', ')}\"\n    end\n\n    unless schema[:type] == 'object'\n      raise \"Schema type must be 'object', got '#{schema[:type]}'\"\n    end\n  end\nend\n\n# Usage\nCustomSchemaRegistry.register_schema('legal_document', LEGAL_DOCUMENT_SCHEMA)\nCustomSchemaRegistry.load_schemas_from_directory(Rails.root.join('config', 'schemas'))\n</code></pre>"},{"location":"reference/metadata-schemas/#version-management","title":"Version Management","text":"<pre><code>class SchemaVersionManager\n  def self.create_new_version(schema_id, updates)\n    current_schema = CustomSchemaRegistry.get_schema(schema_id)\n    return nil unless current_schema\n\n    current_version = current_schema[:schema_version] || '1.0.0'\n    new_version = increment_version(current_version, updates[:breaking_changes])\n\n    new_schema = current_schema.deep_merge(updates).merge(\n      schema_version: new_version,\n      previous_version: current_version,\n      migration_guide: generate_migration_guide(current_schema, updates)\n    )\n\n    CustomSchemaRegistry.register_schema(\"#{schema_id}_v#{new_version.gsub('.', '_')}\", new_schema)\n\n    new_schema\n  end\n\n  def self.increment_version(current_version, has_breaking_changes)\n    major, minor, patch = current_version.split('.').map(&amp;:to_i)\n\n    if has_breaking_changes\n      \"#{major + 1}.0.0\"\n    elsif updates[:new_fields]&amp;.any?\n      \"#{major}.#{minor + 1}.0\"\n    else\n      \"#{major}.#{minor}.#{patch + 1}\"\n    end\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#migration-strategies","title":"Migration Strategies","text":"<pre><code>class SchemaMigrator\n  def self.migrate_metadata(old_metadata, from_schema, to_schema)\n    migration_plan = create_migration_plan(from_schema, to_schema)\n    migrated_metadata = old_metadata.dup\n\n    migration_plan[:field_mappings].each do |old_field, new_field|\n      if migrated_metadata.key?(old_field)\n        migrated_metadata[new_field] = migrated_metadata.delete(old_field)\n      end\n    end\n\n    migration_plan[:field_transformations].each do |field, transformer|\n      if migrated_metadata.key?(field)\n        migrated_metadata[field] = apply_transformation(migrated_metadata[field], transformer)\n      end\n    end\n\n    migration_plan[:default_values].each do |field, default|\n      migrated_metadata[field] ||= default\n    end\n\n    {\n      migrated_metadata: migrated_metadata,\n      migration_warnings: validate_migrated_metadata(migrated_metadata, to_schema),\n      migration_log: migration_plan[:log]\n    }\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#domain-specific-schemas","title":"Domain-Specific Schemas","text":"<p>Ragdoll provides specialized schemas for common document domains:</p>"},{"location":"reference/metadata-schemas/#academic-papers","title":"Academic Papers","text":""},{"location":"reference/metadata-schemas/#citation-extraction-schema","title":"Citation Extraction Schema","text":"<pre><code>ACADEMIC_PAPER_SCHEMA = {\n  type: \"object\",\n  properties: {\n    # Standard fields\n    summary: {\n      type: \"string\",\n      description: \"Abstract or executive summary of the research\"\n    },\n\n    # Academic-specific fields\n    paper_type: {\n      type: \"string\",\n      enum: %w[research_paper review_article conference_paper thesis dissertation preprint technical_report other],\n      description: \"Type of academic document\"\n    },\n\n    research_areas: {\n      type: \"array\",\n      items: { type: \"string\" },\n      description: \"Research disciplines and fields covered\",\n      maxItems: 5\n    },\n\n    authors: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          name: { type: \"string\" },\n          affiliation: { type: \"string\" },\n          email: { type: \"string\", format: \"email\" },\n          orcid: { type: \"string\", pattern: \"^\\\\d{4}-\\\\d{4}-\\\\d{4}-\\\\d{3}[0-9X]$\" },\n          corresponding_author: { type: \"boolean\" }\n        },\n        required: %w[name]\n      },\n      minItems: 1\n    },\n\n    publication_info: {\n      type: \"object\",\n      properties: {\n        journal: { type: \"string\" },\n        conference: { type: \"string\" },\n        volume: { type: \"string\" },\n        issue: { type: \"string\" },\n        pages: { type: \"string\" },\n        publication_date: { type: \"string\", format: \"date\" },\n        doi: { type: \"string\", pattern: \"^10\\\\.\\\\d+/.+\" },\n        isbn: { type: \"string\" },\n        publisher: { type: \"string\" }\n      }\n    },\n\n    citations: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          title: { type: \"string\" },\n          authors: { type: \"string\" },\n          year: { type: \"integer\", minimum: 1900, maximum: 2030 },\n          source: { type: \"string\" },\n          doi: { type: \"string\" },\n          citation_type: { type: \"string\", enum: %w[foundational supportive comparative critical methodological] }\n        },\n        required: %w[title authors year]\n      }\n    },\n\n    methodology: {\n      type: \"object\",\n      properties: {\n        research_methods: {\n          type: \"array\",\n          items: { type: \"string\", enum: %w[experimental observational survey case_study meta_analysis theoretical computational qualitative quantitative mixed_methods] }\n        },\n        data_sources: { type: \"array\", items: { type: \"string\" } },\n        sample_size: { type: \"integer\", minimum: 0 },\n        statistical_methods: { type: \"array\", items: { type: \"string\" } }\n      }\n    },\n\n    funding: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          agency: { type: \"string\" },\n          grant_number: { type: \"string\" },\n          amount: { type: \"number\" },\n          currency: { type: \"string\" }\n        },\n        required: %w[agency]\n      }\n    },\n\n    peer_review_status: {\n      type: \"string\",\n      enum: %w[peer_reviewed non_peer_reviewed preprint under_review],\n      description: \"Peer review status of the publication\"\n    },\n\n    impact_metrics: {\n      type: \"object\",\n      properties: {\n        citation_count: { type: \"integer\", minimum: 0 },\n        h_index_contribution: { type: \"number\" },\n        altmetric_score: { type: \"number\" },\n        download_count: { type: \"integer\", minimum: 0 }\n      }\n    }\n  },\n\n  required: %w[summary paper_type research_areas authors]\n}.freeze\n</code></pre>"},{"location":"reference/metadata-schemas/#legal-documents","title":"Legal Documents","text":""},{"location":"reference/metadata-schemas/#legal-classification-schema","title":"Legal Classification Schema","text":"<pre><code>LEGAL_DOCUMENT_SCHEMA = {\n  type: \"object\",\n  properties: {\n    # Standard fields adapted for legal content\n    summary: {\n      type: \"string\",\n      description: \"Legal summary focusing on key provisions and obligations\",\n      minLength: 100\n    },\n\n    # Legal-specific classification\n    document_type: {\n      type: \"string\",\n      enum: %w[contract agreement policy statute regulation ordinance brief motion pleading judgment settlement nda license other],\n      description: \"Type of legal document\"\n    },\n\n    jurisdiction: {\n      type: \"object\",\n      properties: {\n        primary: { type: \"string\", description: \"Primary jurisdiction (US-CA, UK, EU)\" },\n        additional: { type: \"array\", items: { type: \"string\" } },\n        court_level: { type: \"string\", enum: %w[federal state local municipal international] }\n      },\n      required: %w[primary]\n    },\n\n    legal_areas: {\n      type: \"array\",\n      items: {\n        type: \"string\",\n        enum: %w[contract corporate employment intellectual_property real_estate family criminal civil tax immigration environmental securities banking healthcare privacy data_protection other]\n      },\n      maxItems: 5\n    },\n\n    parties: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          name: { type: \"string\" },\n          role: {\n            type: \"string\",\n            enum: %w[plaintiff defendant buyer seller lessor lessee employer employee licensor licensee grantor grantee other]\n          },\n          entity_type: {\n            type: \"string\",\n            enum: %w[individual corporation llc partnership government nonprofit trust estate other]\n          },\n          representation: { type: \"string\", description: \"Legal representation/counsel\" }\n        },\n        required: %w[name role entity_type]\n      }\n    },\n\n    key_provisions: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          provision_type: {\n            type: \"string\",\n            enum: %w[payment termination liability confidentiality indemnification warranty limitation_of_liability force_majeure governing_law dispute_resolution other]\n          },\n          description: { type: \"string\" },\n          critical: { type: \"boolean\" },\n          page_reference: { type: \"string\" }\n        },\n        required: %w[provision_type description]\n      }\n    },\n\n    compliance_requirements: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          regulation: { type: \"string\" },\n          requirement: { type: \"string\" },\n          deadline: { type: \"string\", format: \"date\" },\n          responsible_party: { type: \"string\" },\n          penalty: { type: \"string\" }\n        },\n        required: %w[regulation requirement]\n      }\n    },\n\n    risk_analysis: {\n      type: \"object\",\n      properties: {\n        overall_risk_level: { type: \"string\", enum: %w[low medium high critical] },\n        financial_exposure: { type: \"string\" },\n        operational_risks: { type: \"array\", items: { type: \"string\" } },\n        legal_risks: { type: \"array\", items: { type: \"string\" } },\n        mitigation_strategies: { type: \"array\", items: { type: \"string\" } }\n      }\n    },\n\n    important_dates: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          date: { type: \"string\", format: \"date\" },\n          description: { type: \"string\" },\n          date_type: { type: \"string\", enum: %w[effective_date expiration_date deadline milestone notification_date other] },\n          recurring: { type: \"boolean\" },\n          recurrence_pattern: { type: \"string\" }\n        },\n        required: %w[date description date_type]\n      }\n    }\n  },\n\n  required: %w[summary document_type jurisdiction legal_areas parties]\n}.freeze\n</code></pre>"},{"location":"reference/metadata-schemas/#technical-documentation","title":"Technical Documentation","text":""},{"location":"reference/metadata-schemas/#api-documentation-schema","title":"API Documentation Schema","text":"<pre><code>TECHNICAL_DOCUMENTATION_SCHEMA = {\n  type: \"object\",\n  properties: {\n    # Standard technical fields\n    summary: {\n      type: \"string\",\n      description: \"Technical summary of the documentation content\"\n    },\n\n    documentation_type: {\n      type: \"string\",\n      enum: %w[api_reference user_guide developer_guide installation_guide troubleshooting architecture_document code_documentation release_notes other],\n      description: \"Type of technical documentation\"\n    },\n\n    # API-specific fields\n    api_info: {\n      type: \"object\",\n      properties: {\n        api_name: { type: \"string\" },\n        version: { type: \"string\", pattern: \"^\\\\d+\\\\.\\\\d+\\\\.\\\\d+\" },\n        base_url: { type: \"string\", format: \"uri\" },\n        authentication_methods: {\n          type: \"array\",\n          items: { type: \"string\", enum: %w[api_key oauth2 jwt basic_auth bearer_token none] }\n        },\n        supported_formats: {\n          type: \"array\",\n          items: { type: \"string\", enum: %w[json xml yaml csv] }\n        },\n        rate_limits: {\n          type: \"object\",\n          properties: {\n            requests_per_minute: { type: \"integer\" },\n            requests_per_hour: { type: \"integer\" },\n            requests_per_day: { type: \"integer\" }\n          }\n        }\n      }\n    },\n\n    endpoints: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          path: { type: \"string\" },\n          method: { type: \"string\", enum: %w[GET POST PUT DELETE PATCH HEAD OPTIONS] },\n          description: { type: \"string\" },\n          parameters: {\n            type: \"array\",\n            items: {\n              type: \"object\",\n              properties: {\n                name: { type: \"string\" },\n                type: { type: \"string\" },\n                required: { type: \"boolean\" },\n                description: { type: \"string\" },\n                example: { type: \"string\" }\n              },\n              required: %w[name type required]\n            }\n          },\n          response_codes: {\n            type: \"array\",\n            items: {\n              type: \"object\",\n              properties: {\n                code: { type: \"integer\" },\n                description: { type: \"string\" },\n                example: { type: \"string\" }\n              }\n            }\n          }\n        },\n        required: %w[path method description]\n      }\n    },\n\n    code_examples: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          language: { type: \"string\", enum: %w[javascript python ruby java php curl bash other] },\n          code: { type: \"string\" },\n          description: { type: \"string\" },\n          output_example: { type: \"string\" }\n        },\n        required: %w[language code]\n      }\n    },\n\n    dependencies: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          name: { type: \"string\" },\n          version: { type: \"string\" },\n          type: { type: \"string\", enum: %w[runtime development build peer optional] },\n          purpose: { type: \"string\" },\n          license: { type: \"string\" }\n        },\n        required: %w[name version type]\n      }\n    },\n\n    version_info: {\n      type: \"object\",\n      properties: {\n        current_version: { type: \"string\" },\n        supported_versions: { type: \"array\", items: { type: \"string\" } },\n        deprecated_versions: { type: \"array\", items: { type: \"string\" } },\n        version_history: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            properties: {\n              version: { type: \"string\" },\n              release_date: { type: \"string\", format: \"date\" },\n              changes: { type: \"array\", items: { type: \"string\" } },\n              breaking_changes: { type: \"boolean\" }\n            }\n          }\n        }\n      }\n    },\n\n    technical_requirements: {\n      type: \"object\",\n      properties: {\n        minimum_versions: {\n          type: \"object\",\n          patternProperties: {\n            \".*\": { type: \"string\" }  # e.g., \"node\": \"&gt;=14.0.0\", \"python\": \"&gt;=3.8\"\n          }\n        },\n        supported_platforms: {\n          type: \"array\",\n          items: { type: \"string\", enum: %w[windows macos linux docker kubernetes web mobile] }\n        },\n        hardware_requirements: {\n          type: \"object\",\n          properties: {\n            min_memory_mb: { type: \"integer\" },\n            min_disk_space_mb: { type: \"integer\" },\n            cpu_cores: { type: \"integer\" }\n          }\n        }\n      }\n    },\n\n    troubleshooting: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          problem: { type: \"string\" },\n          solution: { type: \"string\" },\n          error_codes: { type: \"array\", items: { type: \"string\" } },\n          category: { type: \"string\", enum: %w[installation configuration authentication authorization performance security other] }\n        },\n        required: %w[problem solution]\n      }\n    }\n  },\n\n  required: %w[summary documentation_type]\n}.freeze\n</code></pre>"},{"location":"reference/metadata-schemas/#code-documentation-schema","title":"Code Documentation Schema","text":"<pre><code>CODE_DOCUMENTATION_SCHEMA = {\n  type: \"object\",\n  properties: {\n    summary: {\n      type: \"string\",\n      description: \"Summary of the code functionality and purpose\"\n    },\n\n    code_type: {\n      type: \"string\",\n      enum: %w[class function module library framework application script configuration other],\n      description: \"Type of code being documented\"\n    },\n\n    programming_language: {\n      type: \"string\",\n      enum: %w[javascript typescript python ruby java csharp cpp c php go rust swift kotlin scala other],\n      description: \"Primary programming language\"\n    },\n\n    functions_and_methods: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          name: { type: \"string\" },\n          description: { type: \"string\" },\n          parameters: {\n            type: \"array\",\n            items: {\n              type: \"object\",\n              properties: {\n                name: { type: \"string\" },\n                type: { type: \"string\" },\n                required: { type: \"boolean\" },\n                default_value: { type: \"string\" },\n                description: { type: \"string\" }\n              }\n            }\n          },\n          return_type: { type: \"string\" },\n          return_description: { type: \"string\" },\n          exceptions: { type: \"array\", items: { type: \"string\" } },\n          complexity: { type: \"string\", enum: %w[O(1) O(log n) O(n) O(n log n) O(n\u00b2) O(2^n) other] }\n        },\n        required: %w[name description]\n      }\n    },\n\n    design_patterns: {\n      type: \"array\",\n      items: { type: \"string\", enum: %w[singleton factory observer strategy command adapter decorator facade mvc mvp mvvm repository unit_of_work other] }\n    },\n\n    testing_info: {\n      type: \"object\",\n      properties: {\n        test_coverage: { type: \"number\", minimum: 0, maximum: 100 },\n        testing_frameworks: { type: \"array\", items: { type: \"string\" } },\n        test_types: { type: \"array\", items: { type: \"string\", enum: %w[unit integration e2e performance security accessibility] } }\n      }\n    }\n  },\n\n  required: %w[summary code_type programming_language]\n}.freeze\n</code></pre>"},{"location":"reference/metadata-schemas/#llm-metadata-generation","title":"LLM Metadata Generation","text":"<p>Ragdoll uses advanced prompt engineering and LLM integration for high-quality metadata generation:</p>"},{"location":"reference/metadata-schemas/#generation-process","title":"Generation Process","text":""},{"location":"reference/metadata-schemas/#content-analysis-pipeline","title":"Content Analysis Pipeline","text":"<pre><code>class Ragdoll::MetadataGenerator\n  def generate_for_document(document)\n    content_analyzer = ContentAnalyzer.new(document)\n\n    # Stage 1: Content preprocessing\n    preprocessed_content = content_analyzer.preprocess\n\n    # Stage 2: Schema selection\n    schema = MetadataSchemas.schema_for(document.document_type)\n\n    # Stage 3: Model selection\n    model = select_optimal_model(document, schema)\n\n    # Stage 4: Prompt generation\n    prompt = generate_schema_aware_prompt(preprocessed_content, schema)\n\n    # Stage 5: LLM generation\n    raw_metadata = call_llm_with_retry(model, prompt)\n\n    # Stage 6: Validation and cleanup\n    validated_metadata = validate_and_clean(raw_metadata, schema)\n\n    # Stage 7: Quality assessment\n    quality_score = assess_metadata_quality(validated_metadata, schema)\n\n    {\n      metadata: validated_metadata,\n      quality_score: quality_score,\n      model_used: model,\n      processing_time: Time.current - start_time\n    }\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#model-selection-strategies","title":"Model Selection Strategies","text":"<pre><code>class ModelSelector\n  MODEL_CAPABILITIES = {\n    'openai/gpt-4o' =&gt; {\n      strengths: [:complex_analysis, :technical_content, :multilingual],\n      optimal_for: [:academic_papers, :legal_documents, :technical_docs],\n      cost: :high,\n      speed: :medium,\n      max_tokens: 128000\n    },\n\n    'openai/gpt-4o-mini' =&gt; {\n      strengths: [:general_analysis, :speed, :cost_effective],\n      optimal_for: [:text_documents, :simple_classification],\n      cost: :low,\n      speed: :fast,\n      max_tokens: 128000\n    },\n\n    'anthropic/claude-3-sonnet-20240229' =&gt; {\n      strengths: [:detailed_analysis, :accuracy, :reasoning],\n      optimal_for: [:complex_documents, :legal_analysis],\n      cost: :medium,\n      speed: :medium,\n      max_tokens: 200000\n    },\n\n    'anthropic/claude-3-haiku-20240307' =&gt; {\n      strengths: [:speed, :simple_tasks, :cost_effective],\n      optimal_for: [:keyword_extraction, :basic_classification],\n      cost: :low,\n      speed: :very_fast,\n      max_tokens: 200000\n    }\n  }.freeze\n\n  def self.select_optimal_model(document, schema)\n    content_complexity = analyze_content_complexity(document)\n    schema_complexity = analyze_schema_complexity(schema)\n\n    # Score models based on document and schema requirements\n    model_scores = MODEL_CAPABILITIES.map do |model, capabilities|\n      score = 0\n\n      # Content type optimization\n      if capabilities[:optimal_for].include?(document.document_type.to_sym)\n        score += 30\n      end\n\n      # Complexity matching\n      case content_complexity\n      when :high\n        score += 20 if capabilities[:strengths].include?(:complex_analysis)\n      when :medium\n        score += 15 if capabilities[:cost] == :medium\n      when :low\n        score += 25 if capabilities[:speed] == :fast\n      end\n\n      # Schema complexity matching\n      if schema_complexity == :high &amp;&amp; capabilities[:strengths].include?(:detailed_analysis)\n        score += 15\n      end\n\n      # Cost optimization (configurable weight)\n      cost_weight = Ragdoll.config.metadata_generation[:cost_optimization_weight] || 0.2\n      case capabilities[:cost]\n      when :low then score += (20 * cost_weight)\n      when :medium then score += (10 * cost_weight)\n      when :high then score -= (10 * cost_weight)\n      end\n\n      [model, score]\n    end\n\n    # Return the highest scoring model\n    model_scores.max_by { |model, score| score }.first\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#quality-assessment","title":"Quality Assessment","text":"<pre><code>class MetadataQualityAssessor\n  def self.assess_quality(metadata, schema, original_content)\n    scores = {\n      completeness: assess_completeness(metadata, schema),\n      accuracy: assess_accuracy(metadata, original_content),\n      consistency: assess_consistency(metadata),\n      relevance: assess_relevance(metadata, original_content),\n      specificity: assess_specificity(metadata)\n    }\n\n    # Weighted overall score\n    overall_score = (\n      scores[:completeness] * 0.25 +\n      scores[:accuracy] * 0.30 +\n      scores[:consistency] * 0.15 +\n      scores[:relevance] * 0.20 +\n      scores[:specificity] * 0.10\n    )\n\n    {\n      overall_score: overall_score,\n      component_scores: scores,\n      quality_level: determine_quality_level(overall_score),\n      recommendations: generate_improvement_recommendations(scores)\n    }\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#prompt-engineering","title":"Prompt Engineering","text":""},{"location":"reference/metadata-schemas/#schema-aware-prompts","title":"Schema-Aware Prompts","text":"<pre><code>class SchemaAwarePromptBuilder\n  def build_prompt(content, schema)\n    prompt_parts = []\n\n    # System instruction\n    prompt_parts &lt;&lt; build_system_instruction(schema)\n\n    # Schema specification\n    prompt_parts &lt;&lt; build_schema_specification(schema)\n\n    # Content context\n    prompt_parts &lt;&lt; build_content_context(content)\n\n    # Examples (few-shot learning)\n    prompt_parts &lt;&lt; build_examples(schema)\n\n    # Final instruction\n    prompt_parts &lt;&lt; build_final_instruction(schema)\n\n    prompt_parts.join(\"\\n\\n\")\n  end\n\n  private\n\n  def build_system_instruction(schema)\n    &lt;&lt;~PROMPT\n      You are an expert content analyst specializing in extracting structured metadata from documents.\n\n      Your task is to analyze the provided content and generate metadata that strictly follows the given JSON schema.\n\n      Key requirements:\n      - All required fields must be present\n      - Enum values must match exactly (case-sensitive)\n      - Array fields must not exceed maxItems limits\n      - Provide accurate, relevant, and specific information\n      - If information is not available, use appropriate defaults or null values\n      - Maintain consistency across related fields\n    PROMPT\n  end\n\n  def build_schema_specification(schema)\n    &lt;&lt;~PROMPT\n      ## JSON Schema Specification\n\n      ```json\n      #{JSON.pretty_generate(schema)}\n      ```\n\n      Required fields: #{schema[:required]&amp;.join(', ') || 'none'}\n    PROMPT\n  end\n\n  def build_content_context(content)\n    # Truncate content if too long for model context\n    truncated_content = truncate_for_model(content)\n\n    &lt;&lt;~PROMPT\n      ## Content to Analyze\n\n      ```\n      #{truncated_content}\n      ```\n    PROMPT\n  end\n\n  def build_examples(schema)\n    example_metadata = generate_example_metadata(schema)\n\n    &lt;&lt;~PROMPT\n      ## Example Output Format\n\n      ```json\n      #{JSON.pretty_generate(example_metadata)}\n      ```\n    PROMPT\n  end\n\n  def build_final_instruction(schema)\n    &lt;&lt;~PROMPT\n      ## Instructions\n\n      Analyze the provided content and generate metadata following the exact schema format.\n\n      Return only valid JSON that matches the schema - no additional text or explanations.\n\n      Focus on:\n      #{build_focus_points(schema)}\n    PROMPT\n  end\n\n  def build_focus_points(schema)\n    focus_points = []\n\n    schema[:properties].each do |field, field_schema|\n      case field\n      when :summary, \"summary\"\n        focus_points &lt;&lt; \"- Create a concise, informative summary (#{field_schema[:description]})\"\n      when :keywords, \"keywords\"\n        focus_points &lt;&lt; \"- Extract relevant, specific keywords (max #{field_schema[:maxItems] || 10})\"\n      when :classification, \"classification\"\n        focus_points &lt;&lt; \"- Choose the most appropriate classification from: #{field_schema[:enum]&amp;.join(', ')}\"\n      end\n    end\n\n    focus_points.join(\"\\n\")\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#consistency-optimization","title":"Consistency Optimization","text":"<pre><code>class ConsistencyOptimizer\n  def self.optimize_for_consistency(prompt, previous_metadata = nil)\n    optimized_prompt = prompt.dup\n\n    if previous_metadata\n      # Add consistency instruction\n      consistency_instruction = build_consistency_instruction(previous_metadata)\n      optimized_prompt += \"\\n\\n#{consistency_instruction}\"\n    end\n\n    # Add terminology consistency rules\n    optimized_prompt += \"\\n\\n#{build_terminology_rules}\"\n\n    optimized_prompt\n  end\n\n  private\n\n  def self.build_consistency_instruction(previous_metadata)\n    &lt;&lt;~INSTRUCTION\n      ## Consistency Requirements\n\n      Maintain consistency with previously generated metadata:\n\n      - Use similar classification categories when appropriate\n      - Maintain consistent keyword terminology\n      - Keep similar complexity levels for related content\n      - Use consistent language and tone in summaries\n\n      Previous metadata reference:\n      ```json\n      #{JSON.pretty_generate(previous_metadata.sample(3))}  # Sample for reference\n      ```\n    INSTRUCTION\n  end\n\n  def self.build_terminology_rules\n    &lt;&lt;~RULES\n      ## Terminology Standards\n\n      - Use consistent technical terms (e.g., \"API\" not \"api\" or \"Api\")\n      - Standardize compound terms (e.g., \"machine learning\" not \"machine-learning\")\n      - Use full forms for important concepts (e.g., \"artificial intelligence\" before \"AI\")\n      - Maintain consistent capitalization in proper nouns\n      - Use industry-standard terminology when available\n    RULES\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#error-reduction-techniques","title":"Error Reduction Techniques","text":"<pre><code>class ErrorReductionStrategies\n  def self.apply_error_reduction(prompt, schema)\n    enhanced_prompt = prompt.dup\n\n    # Add common error prevention\n    enhanced_prompt += \"\\n\\n#{build_error_prevention_rules(schema)}\"\n\n    # Add validation reminders\n    enhanced_prompt += \"\\n\\n#{build_validation_reminders(schema)}\"\n\n    enhanced_prompt\n  end\n\n  private\n\n  def self.build_error_prevention_rules(schema)\n    rules = []\n\n    # Enum validation rules\n    enum_fields = schema[:properties].select { |k, v| v[:enum] }\n    if enum_fields.any?\n      rules &lt;&lt; \"CRITICAL: Enum fields must use exact values from the schema:\"\n      enum_fields.each do |field, field_schema|\n        rules &lt;&lt; \"- #{field}: #{field_schema[:enum].join(' | ')}\"\n      end\n    end\n\n    # Array length rules\n    array_fields = schema[:properties].select { |k, v| v[:type] == 'array' &amp;&amp; v[:maxItems] }\n    if array_fields.any?\n      rules &lt;&lt; \"\\nArray length limits:\"\n      array_fields.each do |field, field_schema|\n        rules &lt;&lt; \"- #{field}: maximum #{field_schema[:maxItems]} items\"\n      end\n    end\n\n    # Required field reminders\n    if schema[:required]&amp;.any?\n      rules &lt;&lt; \"\\nRequired fields (cannot be null or empty):\"\n      rules &lt;&lt; \"- #{schema[:required].join(', ')}\"\n    end\n\n    rules.join(\"\\n\")\n  end\n\n  def self.build_validation_reminders(schema)\n    &lt;&lt;~REMINDERS\n      ## Pre-submission Checklist\n\n      Before returning your response, verify:\n      \u2713 All required fields are present and non-empty\n      \u2713 Enum values match exactly (check spelling and case)\n      \u2713 Arrays don't exceed maximum item limits\n      \u2713 JSON is valid and properly formatted\n      \u2713 Field types match schema requirements\n      \u2713 No additional fields outside the schema\n    REMINDERS\n  end\nend\n</code></pre>"},{"location":"reference/metadata-schemas/#multi-language-support","title":"Multi-Language Support","text":"<pre><code>class MultiLanguagePromptBuilder &lt; SchemaAwarePromptBuilder\n  LANGUAGE_INSTRUCTIONS = {\n    'es' =&gt; {\n      system_instruction: \"Eres un analista experto en contenido especializado en extraer metadatos estructurados de documentos.\",\n      focus_keywords: \"Extrae palabras clave relevantes y espec\u00edficas en espa\u00f1ol\",\n      summary_instruction: \"Crea un resumen conciso e informativo en espa\u00f1ol\"\n    },\n    'fr' =&gt; {\n      system_instruction: \"Vous \u00eates un analyste de contenu expert sp\u00e9cialis\u00e9 dans l'extraction de m\u00e9tadonn\u00e9es structur\u00e9es de documents.\",\n      focus_keywords: \"Extraire des mots-cl\u00e9s pertinents et sp\u00e9cifiques en fran\u00e7ais\",\n      summary_instruction: \"Cr\u00e9er un r\u00e9sum\u00e9 concis et informatif en fran\u00e7ais\"\n    },\n    'de' =&gt; {\n      system_instruction: \"Sie sind ein Experte f\u00fcr Inhaltsanalyse, der auf die Extraktion strukturierter Metadaten aus Dokumenten spezialisiert ist.\",\n      focus_keywords: \"Extrahieren Sie relevante und spezifische Schl\u00fcsselw\u00f6rter auf Deutsch\",\n      summary_instruction: \"Erstellen Sie eine pr\u00e4gnante und informative Zusammenfassung auf Deutsch\"\n    }\n  }.freeze\n\n  def build_prompt(content, schema, language = 'en')\n    if language == 'en'\n      super(content, schema)\n    else\n      build_multilingual_prompt(content, schema, language)\n    end\n  end\n\n  private\n\n  def build_multilingual_prompt(content, schema, language)\n    instructions = LANGUAGE_INSTRUCTIONS[language] || LANGUAGE_INSTRUCTIONS['en']\n\n    prompt_parts = [\n      instructions[:system_instruction],\n      build_schema_specification(schema),\n      build_content_context(content),\n      build_language_specific_instructions(language, instructions),\n      build_final_instruction(schema)\n    ]\n\n    prompt_parts.join(\"\\n\\n\")\n  end\n\n  def build_language_specific_instructions(language, instructions)\n    &lt;&lt;~INSTRUCTIONS\n      ## Language-Specific Requirements\n\n      - Content language: #{language.upcase}\n      - #{instructions[:summary_instruction]}\n      - #{instructions[:focus_keywords]}\n      - Maintain cultural context and terminology appropriate for #{language.upcase}\n      - Use native language conventions for formatting and style\n    INSTRUCTIONS\n  end\nend\n</code></pre> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"user-guide/document-processing/","title":"Document Processing","text":"<p>Ragdoll provides comprehensive document processing capabilities that handle multiple file formats, extract content, and generate intelligent metadata using LLM providers. The system uses a multi-modal Single Table Inheritance (STI) architecture to store different content types efficiently.</p>"},{"location":"user-guide/document-processing/#file-parsing-metadata-extraction-and-content-analysis","title":"File Parsing, Metadata Extraction, and Content Analysis","text":"<p>The document processing pipeline transforms raw files into searchable, embedded content through several stages:</p> <ul> <li>File Format Detection: Automatic detection and routing to appropriate parsers</li> <li>Content Extraction: Specialized extraction for text, images, and metadata</li> <li>Multi-Modal STI Storage: Stores content in specialized models (TextContent, ImageContent, AudioContent)</li> <li>LLM-Powered Metadata: AI-generated summaries, keywords, and structured metadata</li> <li>Vector Embeddings: Semantic search capabilities through embedding generation</li> <li>Error Handling: Robust error recovery and validation at each stage</li> </ul>"},{"location":"user-guide/document-processing/#supported-file-types","title":"Supported File Types","text":"<p>Ragdoll supports a wide range of file formats through specialized parsers:</p>"},{"location":"user-guide/document-processing/#text-documents","title":"Text Documents","text":"<p>PDF Processing (<code>pdf-reader</code> gem) - Full text extraction from all pages - Metadata extraction (title, author, subject, creator, producer) - Creation and modification dates - Page count and page-by-page processing - Handles malformed PDFs with graceful error recovery - Supports password-protected PDFs</p> <p>DOCX Processing (<code>docx</code> gem) - Paragraph text extraction with formatting preservation - Table content extraction with structure maintained - Core document properties (title, author, subject, description) - Keywords and metadata from document properties - Creation and modification timestamps - Word and paragraph count statistics</p> <p>HTML and Markdown Parsing - Script and style tag removal for clean content - HTML tag stripping with whitespace normalization - Markdown files processed as plain text - File size and encoding detection - Preserves content structure and readability</p> <p>Plain Text Handling - UTF-8 encoding with fallback to ISO-8859-1 - File size and encoding metadata - Direct content preservation without modification - Supports <code>.txt</code>, <code>.md</code>, <code>.markdown</code> extensions</p>"},{"location":"user-guide/document-processing/#image-documents","title":"Image Documents","text":"<p>Supported Formats <pre><code>.jpg, .jpeg, .png, .gif, .bmp, .webp, .svg, .ico, .tiff, .tif\n</code></pre></p> <p>Image Processing (ImageMagick/RMagick) - Automatic dimension extraction (width \u00d7 height) - File size and format metadata - AI-powered description generation using vision models - Content stored as descriptive text for search indexing - Error-resilient processing with fallback descriptions</p> <p>Vision AI Integration - Integrated with <code>Ragdoll::ImageDescriptionService</code> - Configurable vision models (GPT-4 Vision, Claude 3, etc.) - Generates contextual descriptions for semantic search - Fallback to filename-based descriptions if AI fails</p>"},{"location":"user-guide/document-processing/#audio-documents-planned","title":"Audio Documents (Planned)","text":"<p>Supported Formats (Future) <pre><code>.mp3, .wav, .flac, .m4a, .ogg, .aac\n</code></pre></p> <p>Audio Processing (Planned) - Speech-to-text transcription using Whisper or similar - Audio metadata extraction (duration, bitrate, codec) - Transcript storage in AudioContent models - Speaker identification and timestamp extraction - Background job processing for long audio files</p>"},{"location":"user-guide/document-processing/#processing-pipeline","title":"Processing Pipeline","text":"<p>The document processing workflow follows a structured six-stage pipeline:</p>"},{"location":"user-guide/document-processing/#1-file-upload-and-validation","title":"1. File Upload and Validation","text":"<pre><code># File path processing\ndocument = DocumentProcessor.create_document_from_file(\n  'path/to/document.pdf',\n  title: 'Custom Title',\n  metadata: { source: 'import' }\n)\n\n# Upload processing (Shrine compatible)\ndocument = DocumentProcessor.create_document_from_upload(\n  uploaded_file,\n  title: 'Uploaded Document',\n  metadata: { user_id: 123 }\n)\n</code></pre> <p>Validation Steps: - File existence and accessibility verification - File size limits (configurable) - Format detection via extension and MIME type - Permission checks for file access - Malware scanning (if configured)</p>"},{"location":"user-guide/document-processing/#2-format-detection-and-routing","title":"2. Format Detection and Routing","text":"<p>Primary Detection Method: <pre><code>def self.determine_document_type(file_path)\n  case File.extname(file_path).downcase\n  when \".pdf\" then \"pdf\"\n  when \".docx\" then \"docx\"\n  when \".txt\" then \"text\"\n  when \".md\", \".markdown\" then \"markdown\"\n  when \".html\", \".htm\" then \"html\"\n  when /\\.(jpg|jpeg|png|gif|bmp|webp|svg|ico|tiff|tif)$/i then \"image\"\n  else \"text\"  # Default fallback\n  end\nend\n</code></pre></p> <p>Secondary Detection (MIME Type): - Used for uploaded files without reliable extensions - Content-type header analysis - Magic number detection for binary files - Fallback to text processing for unknown types</p>"},{"location":"user-guide/document-processing/#3-content-extraction","title":"3. Content Extraction","text":"<p>Multi-Modal STI Architecture: <pre><code>graph TD\n    A[Document] --&gt; B[TextContent]\n    A --&gt; C[ImageContent]\n    A --&gt; D[AudioContent]\n    B --&gt; E[Text Embeddings]\n    C --&gt; F[Image Embeddings]\n    D --&gt; G[Audio Embeddings]</code></pre></p> <p>Content Storage Strategy: - TextContent: Raw text, processed text, word/character counts - ImageContent: AI description, dimensions, file metadata - AudioContent: Transcript, duration, speaker info (planned) - Polymorphic Embeddings: Linked to each content type</p>"},{"location":"user-guide/document-processing/#4-metadata-generation-with-llm","title":"4. Metadata Generation with LLM","text":"<p>AI-Powered Analysis: <pre><code># Automatic metadata generation\ndoc.generate_metadata!\n\n# Generated metadata includes:\n# - summary: Concise document summary\n# - keywords: Extracted key terms\n# - classification: Document category\n# - description: Detailed description\n# - tags: Topical tags\n</code></pre></p> <p>Schema Validation: - Document type-specific schemas - Required field validation - Format and length constraints - Error handling with fallback values</p>"},{"location":"user-guide/document-processing/#5-content-chunking-for-embeddings","title":"5. Content Chunking for Embeddings","text":"<p>TextChunker Integration: <pre><code># Configurable chunking strategy\nconfig.chunking[:text][:max_tokens] = 1000\nconfig.chunking[:text][:overlap] = 200\nconfig.chunking[:text][:strategy] = 'sentence_boundary'\n</code></pre></p> <p>Chunking Strategies: - Sentence Boundary: Respects sentence structure - Token-Based: Fixed token count with overlap - Paragraph-Based: Natural paragraph breaks - Semantic Chunking: Content-aware splitting (planned)</p>"},{"location":"user-guide/document-processing/#6-database-storage-and-indexing","title":"6. Database Storage and Indexing","text":"<p>PostgreSQL Storage: - Documents: Main document metadata and status - Contents: STI-based content storage - Embeddings: Vector storage with pgvector - Full-text Indexes: PostgreSQL GIN indexes - JSON Metadata: Structured metadata with indexes</p> <p>Index Strategy: <pre><code>-- Full-text search index\nCREATE INDEX idx_documents_fulltext ON ragdoll_documents \nUSING gin(to_tsvector('english', title || ' ' || \n  COALESCE(metadata-&gt;&gt;'summary', '') || ' ' || \n  COALESCE(metadata-&gt;&gt;'keywords', '')));\n\n-- Vector similarity index\nCREATE INDEX idx_embeddings_vector ON ragdoll_embeddings \nUSING ivfflat (embedding_vector vector_cosine_ops);\n</code></pre></p>"},{"location":"user-guide/document-processing/#metadata-generation","title":"Metadata Generation","text":"<p>Ragdoll uses AI-powered metadata extraction to enhance document searchability and organization:</p>"},{"location":"user-guide/document-processing/#llm-based-content-analysis","title":"LLM-Based Content Analysis","text":"<p>MetadataGenerator Service: <pre><code>generator = Ragdoll::MetadataGenerator.new\nmetadata = generator.generate_for_document(document)\n\n# Example generated metadata:\n{\n  \"summary\" =&gt; \"This technical document explains the implementation...\",\n  \"keywords\" =&gt; [\"API\", \"authentication\", \"security\", \"OAuth\"],\n  \"classification\" =&gt; \"technical_documentation\",\n  \"description\" =&gt; \"Comprehensive guide to API security practices\",\n  \"tags\" =&gt; [\"development\", \"security\", \"best-practices\"],\n  \"sentiment\" =&gt; \"neutral\",\n  \"complexity\" =&gt; \"intermediate\",\n  \"estimated_reading_time\" =&gt; 15\n}\n</code></pre></p> <p>Configurable LLM Models: <pre><code># Different models for different tasks\nconfig.summarization_config[:model] = 'openai/gpt-4o'\nconfig.keywords_config[:model] = 'anthropic/claude-3-haiku-20240307'\nconfig.classification_config[:model] = 'openai/gpt-4o-mini'\n</code></pre></p>"},{"location":"user-guide/document-processing/#schema-validation","title":"Schema Validation","text":"<p>Document Type-Specific Schemas: <pre><code># Text document schema\nMetadataSchemas::TEXT_SCHEMA = {\n  summary: { type: :string, required: true, max_length: 500 },\n  keywords: { type: :array, items: :string, max_items: 20 },\n  classification: { type: :string, enum: CLASSIFICATIONS },\n  description: { type: :string, max_length: 1000 },\n  tags: { type: :array, items: :string, max_items: 10 }\n}\n\n# Image document schema\nMetadataSchemas::IMAGE_SCHEMA = {\n  description: { type: :string, required: true },\n  objects_detected: { type: :array, items: :string },\n  scene_type: { type: :string },\n  colors: { type: :array, items: :string },\n  text_content: { type: :string }  # OCR results\n}\n</code></pre></p> <p>Validation Process: <pre><code>errors = MetadataSchemas.validate_metadata(document_type, metadata)\nif errors.any?\n  Rails.logger.warn \"Metadata validation errors: #{errors.join(', ')}\"\n  # Apply fallback values for failed fields\nend\n</code></pre></p>"},{"location":"user-guide/document-processing/#summary-generation","title":"Summary Generation","text":"<p>Configurable Summary Strategy: <pre><code>config.summarization_config.tap do |c|\n  c[:enable] = true\n  c[:model] = 'openai/gpt-4o'\n  c[:max_length] = 300\n  c[:style] = 'concise'  # concise, detailed, bullet_points\n  c[:include_keywords] = true\nend\n</code></pre></p> <p>Content-Aware Summarization: - Technical Documents: Focus on key concepts and procedures - Legal Documents: Highlight important clauses and obligations - Academic Papers: Emphasize methodology and findings - General Content: Extract main themes and conclusions</p>"},{"location":"user-guide/document-processing/#keyword-extraction","title":"Keyword Extraction","text":"<p>Multi-Strategy Keyword Extraction: <pre><code># LLM-based extraction\nllm_keywords = extract_keywords_with_llm(content)\n\n# Statistical extraction (TF-IDF)\nstats_keywords = extract_keywords_statistical(content)\n\n# Hybrid approach combining both\nfinal_keywords = merge_keyword_strategies(\n  llm_keywords, \n  stats_keywords,\n  weights: { llm: 0.7, statistical: 0.3 }\n)\n</code></pre></p> <p>Keyword Quality Filtering: - Minimum length requirements (&gt;3 characters) - Stop word removal - Duplicate detection and merging - Relevance scoring - Maximum keyword limits (configurable)</p>"},{"location":"user-guide/document-processing/#classification-and-tagging","title":"Classification and Tagging","text":"<p>Hierarchical Classification: <pre><code>CLASSIFICATIONS = {\n  'technical_documentation' =&gt; {\n    'api_documentation' =&gt; ['rest', 'graphql', 'rpc'],\n    'user_guides' =&gt; ['tutorial', 'how-to', 'reference'],\n    'architecture' =&gt; ['design', 'patterns', 'infrastructure']\n  },\n  'business_documents' =&gt; {\n    'contracts' =&gt; ['nda', 'service_agreement', 'license'],\n    'reports' =&gt; ['financial', 'quarterly', 'analysis'],\n    'procedures' =&gt; ['policy', 'workflow', 'compliance']\n  }\n}\n</code></pre></p> <p>Smart Tagging System: <pre><code># Auto-generated tags based on content analysis\nauto_tags = [\n  content_based_tags,      # From text analysis\n  format_based_tags,       # From document format\n  metadata_based_tags,     # From existing metadata\n  context_based_tags       # From file location/name\n].flatten.uniq\n\n# User-defined tags (preserved and merged)\nfinal_tags = (user_tags + auto_tags).uniq\n</code></pre></p> <p>Tag Confidence Scoring: - High confidence: Direct content matches - Medium confidence: Contextual indicators - Low confidence: Statistical correlations - Threshold-based filtering for quality control</p>"},{"location":"user-guide/document-processing/#background-processing","title":"Background Processing","text":"<p>Ragdoll uses background jobs for resource-intensive processing operations:</p>"},{"location":"user-guide/document-processing/#activejob-integration","title":"ActiveJob Integration","text":"<p>Available Background Jobs: <pre><code># Text extraction job\nRagdoll::ExtractTextJob.perform_later(document_id)\n\n# Embedding generation job\nRagdoll::GenerateEmbeddingsJob.perform_later(content_id, content_type)\n\n# Summary generation job\nRagdoll::GenerateSummaryJob.perform_later(document_id)\n\n# Keyword extraction job\nRagdoll::ExtractKeywordsJob.perform_later(document_id)\n</code></pre></p> <p>Job Configuration: <pre><code>config.background_processing_config.tap do |c|\n  c[:enable] = true\n  c[:queue_name] = 'ragdoll_processing'\n  c[:job_timeout] = 300.seconds\n  c[:max_retry_attempts] = 3\n  c[:retry_backoff] = :exponential\nend\n</code></pre></p>"},{"location":"user-guide/document-processing/#job-queues-and-workers","title":"Job Queues and Workers","text":"<p>Queue Priority System: <pre><code># High priority: User-facing operations\nqueue_as :ragdoll_high_priority, priority: 10\n\n# Medium priority: Batch processing\nqueue_as :ragdoll_medium_priority, priority: 5\n\n# Low priority: Background optimization\nqueue_as :ragdoll_low_priority, priority: 1\n</code></pre></p> <p>Worker Scaling Configuration: <pre><code># Development: Single worker\nbundle exec sidekiq -q ragdoll_processing\n\n# Production: Multiple workers with priority queues\nbundle exec sidekiq -q ragdoll_high_priority:3 -q ragdoll_medium_priority:2 -q ragdoll_low_priority:1\n</code></pre></p>"},{"location":"user-guide/document-processing/#error-handling-and-retries","title":"Error Handling and Retries","text":"<p>Retry Strategy: <pre><code>class ProcessDocumentJob &lt; ApplicationJob\n  retry_on StandardError, wait: :exponentially_longer, attempts: 3\n  retry_on ActiveRecord::Deadlocked, wait: 5.seconds, attempts: 3\n\n  discard_on ActiveJob::DeserializationError\n  discard_on Ragdoll::Core::UnsupportedFormatError\n\n  def perform(document_id)\n    document = Document.find(document_id)\n    document.process_content!\n  rescue =&gt; e\n    document&amp;.update(status: 'error', error_message: e.message)\n    raise\n  end\nend\n</code></pre></p> <p>Error Recovery: - Automatic retries with exponential backoff - Dead letter queue for failed jobs - Error notification system - Manual job retry capabilities - Partial processing recovery</p>"},{"location":"user-guide/document-processing/#progress-tracking","title":"Progress Tracking","text":"<p>Job Status Monitoring: <pre><code># Document processing status\ndocument.status  # 'pending', 'processing', 'processed', 'error'\n\n# Detailed progress tracking\nprocessing_info = {\n  stage: 'embedding_generation',\n  progress: 75,\n  total_steps: 4,\n  current_step: 3,\n  estimated_completion: 2.minutes.from_now\n}\n\ndocument.update(processing_info: processing_info)\n</code></pre></p> <p>Real-time Updates: <pre><code># WebSocket integration for live progress\nActionCable.server.broadcast(\n  \"document_#{document.id}\",\n  {\n    event: 'processing_update',\n    progress: 50,\n    message: 'Generating embeddings...'\n  }\n)\n</code></pre></p>"},{"location":"user-guide/document-processing/#scaling-considerations","title":"Scaling Considerations","text":"<p>Horizontal Scaling: <pre><code># Docker Compose example\nservices:\n  ragdoll_worker_1:\n    build: .\n    command: bundle exec sidekiq -q ragdoll_high_priority:2\n    environment:\n      - REDIS_URL=redis://redis:6379/0\n\n  ragdoll_worker_2:\n    build: .\n    command: bundle exec sidekiq -q ragdoll_medium_priority:3 -q ragdoll_low_priority:1\n    environment:\n      - REDIS_URL=redis://redis:6379/0\n</code></pre></p> <p>Resource Management: <pre><code># Memory-aware job processing\nclass ProcessLargeDocumentJob &lt; ApplicationJob\n  def perform(document_id)\n    # Process in chunks to manage memory\n    document = Document.find(document_id)\n\n    if document.file_size &gt; 50.megabytes\n      process_in_chunks(document)\n    else\n      process_normally(document)\n    end\n  ensure\n    GC.start  # Force garbage collection\n  end\nend\n</code></pre></p> <p>Performance Monitoring: <pre><code># Job performance metrics\nclass JobMetrics\n  def self.track_job_performance(job_name, &amp;block)\n    start_time = Time.current\n    result = block.call\n    duration = Time.current - start_time\n\n    Rails.logger.info \"Job #{job_name} completed in #{duration}s\"\n\n    # Send to monitoring service\n    StatsD.histogram('job.duration', duration, tags: [\"job:#{job_name}\"])\n\n    result\n  end\nend\n</code></pre></p>"},{"location":"user-guide/document-processing/#configuration-options","title":"Configuration Options","text":"<p>Ragdoll provides extensive configuration options for document processing:</p>"},{"location":"user-guide/document-processing/#chunk-size-and-overlap-settings","title":"Chunk Size and Overlap Settings","text":"<p>Text Chunking Configuration: <pre><code>Ragdoll::Core.configure do |config|\n  config.chunking[:text].tap do |c|\n    c[:max_tokens] = 1000           # Maximum tokens per chunk\n    c[:overlap] = 200               # Token overlap between chunks\n    c[:strategy] = 'sentence'       # 'sentence', 'paragraph', 'token'\n    c[:min_chunk_size] = 100        # Minimum viable chunk size\n    c[:preserve_paragraphs] = true  # Respect paragraph boundaries\n    c[:split_on_headers] = true     # Split at header boundaries\n  end\nend\n</code></pre></p> <p>Content-Type Specific Chunking: <pre><code># PDF documents (technical content)\nconfig.chunking[:pdf][:max_tokens] = 1500\nconfig.chunking[:pdf][:overlap] = 300\nconfig.chunking[:pdf][:preserve_page_breaks] = true\n\n# HTML documents (web content)\nconfig.chunking[:html][:max_tokens] = 800\nconfig.chunking[:html][:overlap] = 150\nconfig.chunking[:html][:preserve_structure] = true\n\n# Code documents\nconfig.chunking[:code][:max_tokens] = 2000\nconfig.chunking[:code][:overlap] = 100\nconfig.chunking[:code][:preserve_functions] = true\n</code></pre></p>"},{"location":"user-guide/document-processing/#model-selection-for-metadata-generation","title":"Model Selection for Metadata Generation","text":"<p>LLM Model Configuration: <pre><code>config.ruby_llm_config.tap do |llm|\n  # Primary models for different tasks\n  llm[:openai][:api_key] = ENV['OPENAI_API_KEY']\n  llm[:anthropic][:api_key] = ENV['ANTHROPIC_API_KEY']\n  llm[:google][:api_key] = ENV['GOOGLE_API_KEY']\nend\n\n# Task-specific model assignment\nconfig.models.tap do |m|\n  m[:summarization] = 'openai/gpt-4o'           # Best for summaries\n  m[:keywords] = 'anthropic/claude-3-haiku'     # Fast keyword extraction\n  m[:classification] = 'openai/gpt-4o-mini'     # Cost-effective classification\n  m[:description] = 'google/gemini-1.5-pro'    # Detailed descriptions\nend\n</code></pre></p> <p>Embedding Model Configuration: <pre><code>config.embedding_config.tap do |e|\n  # Text embeddings\n  e[:text][:model] = 'openai/text-embedding-3-large'\n  e[:text][:dimensions] = 3072\n  e[:text][:batch_size] = 100\n\n  # Image embeddings (planned)\n  e[:image][:model] = 'openai/clip-vit-large-patch14'\n  e[:image][:dimensions] = 768\n\n  # Audio embeddings (planned)\n  e[:audio][:model] = 'openai/whisper-embedding-v1'\n  e[:audio][:dimensions] = 1024\nend\n</code></pre></p>"},{"location":"user-guide/document-processing/#processing-timeouts","title":"Processing Timeouts","text":"<p>Timeout Configuration: <pre><code>config.processing_timeouts.tap do |t|\n  # Per-operation timeouts\n  t[:file_parsing] = 120.seconds        # File content extraction\n  t[:text_extraction] = 60.seconds      # Text processing\n  t[:image_analysis] = 180.seconds      # Vision AI processing\n  t[:metadata_generation] = 300.seconds # LLM metadata creation\n  t[:embedding_generation] = 240.seconds # Vector embedding creation\n\n  # Document size-based scaling\n  t[:scaling_factor] = 1.5              # Multiply timeout by this for large docs\n  t[:large_document_threshold] = 10.megabytes\nend\n</code></pre></p> <p>Background Job Timeouts: <pre><code>config.background_processing_config.tap do |bg|\n  bg[:job_timeout] = 600.seconds        # Maximum job execution time\n  bg[:queue_timeout] = 3600.seconds     # Maximum time in queue\n  bg[:retry_timeout] = 1800.seconds     # Time between retries\nend\n</code></pre></p>"},{"location":"user-guide/document-processing/#quality-thresholds","title":"Quality Thresholds","text":"<p>Content Quality Filters: <pre><code>config.quality_thresholds.tap do |q|\n  # Minimum content requirements\n  q[:min_text_length] = 50              # Minimum characters for processing\n  q[:min_word_count] = 10               # Minimum words for meaningful content\n  q[:max_empty_lines_ratio] = 0.5       # Maximum ratio of empty lines\n\n  # Metadata quality requirements\n  q[:min_summary_length] = 20           # Minimum summary length\n  q[:max_summary_length] = 500          # Maximum summary length\n  q[:min_keywords_count] = 3            # Minimum number of keywords\n  q[:max_keywords_count] = 20           # Maximum number of keywords\n\n  # Embedding quality thresholds\n  q[:min_embedding_similarity] = 0.1    # Minimum similarity for relevance\n  q[:duplicate_threshold] = 0.95        # Similarity threshold for duplicates\nend\n</code></pre></p> <p>Language Detection and Filtering: <pre><code>config.language_config.tap do |l|\n  l[:enabled] = true\n  l[:supported_languages] = ['en', 'es', 'fr', 'de', 'it']\n  l[:confidence_threshold] = 0.8         # Minimum language detection confidence\n  l[:fallback_language] = 'en'          # Default when detection fails\n  l[:skip_unsupported] = false          # Process unsupported languages as text\nend\n</code></pre></p>"},{"location":"user-guide/document-processing/#advanced-processing-options","title":"Advanced Processing Options","text":"<p>Performance Optimization: <pre><code>config.performance_config.tap do |p|\n  # Parallel processing\n  p[:parallel_processing] = true\n  p[:max_parallel_jobs] = 4\n  p[:chunk_processing_batch_size] = 50\n\n  # Memory management\n  p[:memory_limit] = 2.gigabytes\n  p[:gc_frequency] = 100               # GC every N operations\n  p[:temp_file_cleanup] = true\n\n  # Caching\n  p[:cache_parsed_content] = true\n  p[:cache_embeddings] = true\n  p[:cache_ttl] = 1.hour\nend\n</code></pre></p> <p>Error Handling Configuration: <pre><code>config.error_handling.tap do |e|\n  e[:continue_on_parse_error] = true    # Continue processing other content\n  e[:retry_failed_chunks] = true       # Retry failed chunk processing\n  e[:max_retry_attempts] = 3           # Maximum retry attempts\n  e[:fallback_to_text] = true          # Fallback to text processing\n  e[:notify_on_errors] = true          # Send error notifications\nend\n</code></pre></p>"},{"location":"user-guide/document-processing/#environment-specific-configuration","title":"Environment-Specific Configuration","text":"<p>Development Settings: <pre><code>if Rails.env.development?\n  config.chunking[:text][:max_tokens] = 500    # Smaller chunks for faster processing\n  config.processing_timeouts[:metadata_generation] = 60.seconds\n  config.background_processing_config[:enable] = false  # Synchronous processing\nend\n</code></pre></p> <p>Production Settings: <pre><code>if Rails.env.production?\n  config.chunking[:text][:max_tokens] = 1500   # Larger chunks for efficiency\n  config.background_processing_config[:enable] = true\n  config.performance_config[:parallel_processing] = true\n  config.quality_thresholds[:min_text_length] = 100\nend\n</code></pre></p> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"user-guide/embedding-system/","title":"Embedding System","text":"<p>Ragdoll provides a sophisticated embedding system that transforms text, images, and audio content into high-dimensional vector representations for semantic search. The system leverages PostgreSQL's pgvector extension for efficient similarity search and supports multiple embedding models across different providers.</p>"},{"location":"user-guide/embedding-system/#vector-generation-and-similarity-search","title":"Vector Generation and Similarity Search","text":"<p>The embedding system operates through a comprehensive pipeline that handles:</p> <ul> <li>Multi-Modal Vector Generation: Supports text, image, and audio content embedding</li> <li>Provider Agnostic Architecture: Works with OpenAI, Anthropic, Google, Azure, Ollama, and HuggingFace</li> <li>PostgreSQL pgvector Integration: High-performance vector storage and similarity search</li> <li>Usage Analytics: Tracks embedding usage for intelligent result ranking</li> <li>Configurable Chunking: Optimal content segmentation for embedding generation</li> <li>Batch Processing: Efficient bulk embedding generation with background jobs</li> </ul>"},{"location":"user-guide/embedding-system/#embedding-models","title":"Embedding Models","text":"<p>Ragdoll supports a wide range of embedding models optimized for different content types:</p>"},{"location":"user-guide/embedding-system/#text-embeddings","title":"Text Embeddings","text":"<p>OpenAI Models (Recommended) <pre><code>config.embedding_config[:text][:model] = 'openai/text-embedding-3-large'  # 3072 dimensions\nconfig.embedding_config[:text][:model] = 'openai/text-embedding-3-small'  # 1536 dimensions\nconfig.embedding_config[:text][:model] = 'openai/text-embedding-ada-002'  # 1536 dimensions (legacy)\n</code></pre></p> <p>Features: - High semantic accuracy for English and multilingual content - Optimized for retrieval and similarity tasks - Consistent performance across diverse document types - Built-in rate limiting and error handling</p> <p>Azure OpenAI Embeddings <pre><code>config.ruby_llm_config[:azure][:api_key] = ENV['AZURE_OPENAI_API_KEY']\nconfig.ruby_llm_config[:azure][:endpoint] = ENV['AZURE_OPENAI_ENDPOINT']\nconfig.embedding_config[:text][:model] = 'azure/text-embedding-ada-002'\n</code></pre></p> <p>Google Vertex AI Embeddings <pre><code>config.ruby_llm_config[:google][:api_key] = ENV['GOOGLE_API_KEY']\nconfig.embedding_config[:text][:model] = 'google/textembedding-gecko@003'\nconfig.embedding_config[:text][:model] = 'google/text-embedding-004'  # Latest model\n</code></pre></p> <p>Local/Ollama Embeddings <pre><code>config.ruby_llm_config[:ollama][:endpoint] = 'http://localhost:11434/v1'\nconfig.embedding_config[:text][:model] = 'ollama/nomic-embed-text'      # 768 dimensions\nconfig.embedding_config[:text][:model] = 'ollama/mxbai-embed-large'     # 1024 dimensions\nconfig.embedding_config[:text][:model] = 'ollama/all-minilm'            # 384 dimensions\n</code></pre></p> <p>Benefits of Local Models: - Complete data privacy and control - No API rate limits or costs - Custom model fine-tuning capabilities - Offline operation support</p> <p>HuggingFace Models <pre><code>config.ruby_llm_config[:huggingface][:api_key] = ENV['HUGGINGFACE_API_KEY']\nconfig.embedding_config[:text][:model] = 'huggingface/sentence-transformers/all-MiniLM-L6-v2'\nconfig.embedding_config[:text][:model] = 'huggingface/sentence-transformers/all-mpnet-base-v2'\n</code></pre></p>"},{"location":"user-guide/embedding-system/#image-embeddings-planned","title":"Image Embeddings (Planned)","text":"<p>CLIP Models for Vision Understanding <pre><code># Future implementation\nconfig.embedding_config[:image][:model] = 'openai/clip-vit-large-patch14'  # 768 dimensions\nconfig.embedding_config[:image][:model] = 'openai/clip-vit-base-patch32'   # 512 dimensions\n</code></pre></p> <p>Features (Planned): - Multi-modal understanding (image + text) - Object detection and scene recognition - Visual similarity search capabilities - Integration with existing text embeddings</p> <p>Vision Transformer Models (Planned) <pre><code>config.embedding_config[:image][:model] = 'google/vit-base-patch16-224'\nconfig.embedding_config[:image][:model] = 'microsoft/resnet-50'\n</code></pre></p>"},{"location":"user-guide/embedding-system/#audio-embeddings-planned","title":"Audio Embeddings (Planned)","text":"<p>Whisper-Based Embeddings <pre><code># Future implementation\nconfig.embedding_config[:audio][:model] = 'openai/whisper-embedding-v1'   # 1024 dimensions\nconfig.embedding_config[:audio][:model] = 'openai/whisper-large-v3'       # 1280 dimensions\n</code></pre></p> <p>Audio Feature Extraction (Planned) <pre><code>config.embedding_config[:audio][:model] = 'facebook/wav2vec2-base-960h'\nconfig.embedding_config[:audio][:model] = 'microsoft/speecht5_asr'\n</code></pre></p> <p>Speech-to-Text Integration - Automatic transcription with embedding generation - Speaker identification and segmentation - Multi-language audio processing - Timestamp-aware chunk embeddings</p>"},{"location":"user-guide/embedding-system/#vector-storage","title":"Vector Storage","text":"<p>Ragdoll uses PostgreSQL with the pgvector extension for high-performance vector storage and similarity search:</p>"},{"location":"user-guide/embedding-system/#pgvector-integration-for-postgresql","title":"pgvector Integration for PostgreSQL","text":"<p>Database Schema: <pre><code>CREATE TABLE ragdoll_embeddings (\n  id BIGSERIAL PRIMARY KEY,\n  embeddable_type VARCHAR NOT NULL,\n  embeddable_id BIGINT NOT NULL,\n  chunk_index INTEGER NOT NULL,\n  content TEXT NOT NULL,\n  embedding_vector VECTOR(1536) NOT NULL,  -- Configurable dimensions\n  usage_count INTEGER DEFAULT 0,\n  returned_at TIMESTAMP,\n  created_at TIMESTAMP NOT NULL,\n  updated_at TIMESTAMP NOT NULL\n);\n</code></pre></p> <p>Polymorphic Relationships: <pre><code># Embeddings belong to content via polymorphic association\nbelongs_to :embeddable, polymorphic: true\n\n# Content types that can have embeddings:\n# - Ragdoll::TextContent\n# - Ragdoll::ImageContent  \n# - Ragdoll::AudioContent\n</code></pre></p>"},{"location":"user-guide/embedding-system/#vector-dimensionality-handling","title":"Vector Dimensionality Handling","text":"<p>Dynamic Dimension Support: <pre><code># Configure dimensions per model\nconfig.embedding_config[:text].tap do |c|\n  c[:model] = 'openai/text-embedding-3-large'\n  c[:dimensions] = 3072  # OpenAI's large model\nend\n\nconfig.embedding_config[:text].tap do |c|\n  c[:model] = 'openai/text-embedding-3-small' \n  c[:dimensions] = 1536  # OpenAI's small model\nend\n</code></pre></p> <p>Migration Support: <pre><code># Automatic schema migration for dimension changes\nclass UpdateEmbeddingDimensions &lt; ActiveRecord::Migration[7.0]\n  def up\n    # Change vector dimensions when switching models\n    execute \"ALTER TABLE ragdoll_embeddings ALTER COLUMN embedding_vector TYPE vector(3072)\"\n\n    # Rebuild indexes with new dimensions\n    execute \"DROP INDEX IF EXISTS idx_embeddings_vector_cosine\"\n    execute \"CREATE INDEX idx_embeddings_vector_cosine ON ragdoll_embeddings \n             USING ivfflat (embedding_vector vector_cosine_ops)\"\n  end\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#index-types-ivfflat-hnsw","title":"Index Types (IVFFlat, HNSW)","text":"<p>IVFFlat Index (Default) <pre><code>-- Inverted File Flat index for large datasets\nCREATE INDEX idx_embeddings_vector_cosine ON ragdoll_embeddings \nUSING ivfflat (embedding_vector vector_cosine_ops)\nWITH (lists = 100);\n\n-- L2 distance index alternative\nCREATE INDEX idx_embeddings_vector_l2 ON ragdoll_embeddings \nUSING ivfflat (embedding_vector vector_l2_ops)\nWITH (lists = 100);\n</code></pre></p> <p>HNSW Index (High Performance) <pre><code>-- Hierarchical Navigable Small World index for faster queries\nCREATE INDEX idx_embeddings_vector_hnsw ON ragdoll_embeddings \nUSING hnsw (embedding_vector vector_cosine_ops)\nWITH (m = 16, ef_construction = 64);\n</code></pre></p> <p>Index Configuration: <pre><code>config.vector_index_config.tap do |c|\n  c[:type] = 'ivfflat'  # or 'hnsw'\n  c[:lists] = 100       # IVFFlat parameter\n  c[:m] = 16           # HNSW parameter\n  c[:ef_construction] = 64  # HNSW parameter\n  c[:distance_metric] = 'cosine'  # cosine, l2, inner_product\nend\n</code></pre></p> <p>Performance Comparison: | Index Type | Build Time | Query Speed | Memory Usage | Best For | |------------|------------|-------------|--------------|----------| | IVFFlat    | Fast       | Good        | Low          | Large datasets (&gt;100K vectors) | | HNSW       | Slow       | Excellent   | High         | Real-time queries (&lt;100K vectors) |</p>"},{"location":"user-guide/embedding-system/#storage-optimization","title":"Storage Optimization","text":"<p>Compression and Quantization: <pre><code>config.vector_optimization.tap do |c|\n  c[:enable_compression] = true\n  c[:quantization_bits] = 8      # 8-bit quantization for space savings\n  c[:normalize_vectors] = true   # L2 normalization for cosine similarity\n  c[:remove_duplicates] = true   # Automatic deduplication\nend\n</code></pre></p> <p>Partitioning Strategy: <pre><code>-- Partition by embeddable_type for query optimization\nCREATE TABLE ragdoll_embeddings_text \nPARTITION OF ragdoll_embeddings \nFOR VALUES IN ('Ragdoll::TextContent');\n\nCREATE TABLE ragdoll_embeddings_image \nPARTITION OF ragdoll_embeddings \nFOR VALUES IN ('Ragdoll::ImageContent');\n</code></pre></p> <p>Storage Monitoring: <pre><code># Monitor vector storage statistics\nclass VectorStorageStats\n  def self.storage_summary\n    {\n      total_embeddings: Embedding.count,\n      by_type: Embedding.group(:embeddable_type).count,\n      by_dimensions: Embedding.joins(:embeddable).group('embedding_model').count,\n      storage_size: estimate_storage_size,\n      index_sizes: get_index_sizes,\n      compression_ratio: calculate_compression_ratio\n    }\n  end\nend\n</code></pre></p> <p>Cleanup and Maintenance: <pre><code># Automated cleanup job\nclass VectorMaintenanceJob &lt; ApplicationJob\n  def perform\n    # Remove orphaned embeddings\n    Embedding.left_joins(:embeddable).where(embeddable: { id: nil }).delete_all\n\n    # Rebuild indexes periodically\n    if should_rebuild_indexes?\n      rebuild_vector_indexes\n    end\n\n    # Update usage statistics\n    update_vector_statistics\n  end\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#similarity-search","title":"Similarity Search","text":"<p>Ragdoll implements advanced similarity search algorithms optimized for semantic retrieval:</p>"},{"location":"user-guide/embedding-system/#cosine-similarity-calculation","title":"Cosine Similarity Calculation","text":"<p>Primary Search Method: <pre><code># PostgreSQL pgvector cosine similarity\nresults = Embedding.search_similar(\n  query_embedding,\n  limit: 20,\n  threshold: 0.7,\n  filters: { document_type: 'pdf' }\n)\n\n# Ruby implementation for verification\ndef cosine_similarity(vec1, vec2)\n  dot_product = vec1.zip(vec2).sum { |a, b| a * b }\n  magnitude1 = Math.sqrt(vec1.sum { |a| a * a })\n  magnitude2 = Math.sqrt(vec2.sum { |a| a * a })\n\n  return 0.0 if magnitude1 == 0.0 || magnitude2 == 0.0\n\n  dot_product / (magnitude1 * magnitude2)\nend\n</code></pre></p> <p>Optimized PostgreSQL Query: <pre><code>-- Using pgvector's cosine distance operator\nSELECT \n  id,\n  content,\n  1 - (embedding_vector &lt;=&gt; $1::vector) AS similarity,\n  embedding_vector &lt;=&gt; $1::vector AS distance\nFROM ragdoll_embeddings\nWHERE 1 - (embedding_vector &lt;=&gt; $1::vector) &gt; $2  -- threshold\nORDER BY embedding_vector &lt;=&gt; $1::vector\nLIMIT $3;\n</code></pre></p>"},{"location":"user-guide/embedding-system/#euclidean-distance-options","title":"Euclidean Distance Options","text":"<p>L2 Distance Search: <pre><code># Configure L2 distance for geometric similarity\nconfig.search_config.tap do |c|\n  c[:distance_metric] = 'l2'  # or 'cosine', 'inner_product'\n  c[:normalize_vectors] = false  # Don't normalize for L2\nend\n\n# Search using L2 distance\nresults = Embedding.search_with_distance(\n  query_embedding,\n  distance_metric: 'l2',\n  max_distance: 0.5\n)\n</code></pre></p> <p>Distance Metrics Comparison: <pre><code>class SimilarityMetrics\n  def self.compare_metrics(query, candidates)\n    results = {}\n\n    candidates.each do |candidate|\n      results[candidate.id] = {\n        cosine: cosine_similarity(query, candidate.embedding_vector),\n        l2: l2_distance(query, candidate.embedding_vector),\n        inner_product: inner_product(query, candidate.embedding_vector)\n      }\n    end\n\n    results\n  end\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#hybrid-search-combining-multiple-metrics","title":"Hybrid Search Combining Multiple Metrics","text":"<p>Multi-Score Ranking: <pre><code>def self.hybrid_search(query_embedding, **options)\n  # Get semantic similarity results\n  semantic_results = search_similar(query_embedding, **options)\n\n  # Enhance with additional ranking factors\n  enhanced_results = semantic_results.map do |result|\n    # Usage-based scoring\n    usage_score = calculate_usage_score(result[:embedding_id])\n\n    # Recency scoring\n    recency_score = calculate_recency_score(result[:returned_at])\n\n    # Content quality scoring\n    quality_score = calculate_content_quality(result[:content])\n\n    # Combined weighted score\n    result[:combined_score] = (\n      result[:similarity] * 0.6 +      # Semantic similarity\n      usage_score * 0.2 +              # Usage frequency\n      recency_score * 0.1 +            # Recency\n      quality_score * 0.1              # Content quality\n    )\n\n    result\n  end\n\n  enhanced_results.sort_by { |r| -r[:combined_score] }\nend\n</code></pre></p> <p>Contextual Re-ranking: <pre><code># Re-rank results based on document context\ndef self.contextual_rerank(results, query_context)\n  results.each do |result|\n    # Document type preference\n    doc_type_bonus = document_type_relevance(result[:document_type], query_context)\n\n    # Content freshness\n    freshness_bonus = content_freshness_score(result[:created_at])\n\n    # Authority scoring (based on document metadata)\n    authority_bonus = document_authority_score(result[:document_id])\n\n    result[:final_score] = result[:combined_score] + \n                          doc_type_bonus + \n                          freshness_bonus + \n                          authority_bonus\n  end\n\n  results.sort_by { |r| -r[:final_score] }\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#performance-benchmarking","title":"Performance Benchmarking","text":"<p>Search Performance Metrics: <pre><code>class SearchBenchmark\n  def self.benchmark_search_performance\n    query_embedding = generate_test_embedding\n\n    # Benchmark different configurations\n    results = {}\n\n    # Test different limits\n    [10, 50, 100, 500].each do |limit|\n      time = Benchmark.realtime do\n        Embedding.search_similar(query_embedding, limit: limit)\n      end\n      results[\"limit_#{limit}\"] = time\n    end\n\n    # Test different thresholds\n    [0.5, 0.7, 0.8, 0.9].each do |threshold|\n      time = Benchmark.realtime do\n        Embedding.search_similar(query_embedding, threshold: threshold)\n      end\n      results[\"threshold_#{threshold}\"] = time\n    end\n\n    # Test index performance\n    ['ivfflat', 'hnsw'].each do |index_type|\n      time = benchmark_with_index(query_embedding, index_type)\n      results[\"index_#{index_type}\"] = time\n    end\n\n    results\n  end\nend\n</code></pre></p> <p>Performance Optimization Strategies: <pre><code># Query optimization techniques\nconfig.search_optimization.tap do |c|\n  c[:enable_query_cache] = true\n  c[:cache_similar_queries] = true\n  c[:query_cache_ttL] = 300.seconds\n\n  # Precompute popular embeddings\n  c[:precompute_popular] = true\n  c[:popularity_threshold] = 10  # usage_count &gt; 10\n\n  # Approximate search for large datasets\n  c[:enable_approximate_search] = true\n  c[:approximation_factor] = 0.95  # 95% accuracy for speed\nend\n</code></pre></p> <p>Query Performance Monitoring: <pre><code># Monitor search performance in production\nclass SearchPerformanceMonitor\n  def self.track_query(query_embedding, options = {})\n    start_time = Time.current\n\n    begin\n      results = Embedding.search_similar(query_embedding, **options)\n      duration = Time.current - start_time\n\n      # Log performance metrics\n      Rails.logger.info {\n        \"Search Performance: #{duration}s, \" +\n        \"results: #{results.length}, \" +\n        \"limit: #{options[:limit]}, \" +\n        \"threshold: #{options[:threshold]}\"\n      }\n\n      # Send to monitoring service\n      StatsD.histogram('search.duration', duration * 1000)\n      StatsD.increment('search.queries')\n      StatsD.histogram('search.results', results.length)\n\n      results\n    rescue =&gt; e\n      StatsD.increment('search.errors')\n      raise\n    end\n  end\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#chunking-strategy","title":"Chunking Strategy","text":"<p>Ragdoll implements intelligent text chunking to optimize embedding generation and search relevance:</p>"},{"location":"user-guide/embedding-system/#configurable-chunk-sizes","title":"Configurable Chunk Sizes","text":"<p>Token-Based Chunking: <pre><code>config.chunking[:text].tap do |c|\n  c[:max_tokens] = 1000        # Maximum tokens per chunk\n  c[:min_tokens] = 100         # Minimum viable chunk size\n  c[:target_tokens] = 800      # Preferred chunk size\n  c[:overlap] = 200            # Token overlap between chunks\nend\n\n# Model-specific optimizations\nconfig.chunking[:models] = {\n  'openai/text-embedding-3-large' =&gt; {\n    max_tokens: 8000,    # Large context window\n    optimal_size: 1500\n  },\n  'openai/text-embedding-3-small' =&gt; {\n    max_tokens: 8000,\n    optimal_size: 1000\n  },\n  'ollama/nomic-embed-text' =&gt; {\n    max_tokens: 2048,    # Smaller local model\n    optimal_size: 512\n  }\n}\n</code></pre></p> <p>Character-Based Fallback: <pre><code># When token counting is unavailable\nconfig.chunking[:character_fallback].tap do |c|\n  c[:max_chars] = 4000         # ~1000 tokens\n  c[:min_chars] = 400          # ~100 tokens\n  c[:overlap_chars] = 800      # ~200 tokens\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#overlap-strategies","title":"Overlap Strategies","text":"<p>Sliding Window Overlap: <pre><code>class TextChunker\n  def chunk_with_sliding_window(text, chunk_size: 1000, overlap: 200)\n    chunks = []\n    tokens = tokenize(text)\n\n    start_pos = 0\n    while start_pos &lt; tokens.length\n      end_pos = start_pos + chunk_size\n      chunk_tokens = tokens[start_pos...end_pos]\n\n      # Ensure minimum chunk size\n      break if chunk_tokens.length &lt; config.chunking[:text][:min_tokens]\n\n      chunks &lt;&lt; {\n        content: detokenize(chunk_tokens),\n        start_token: start_pos,\n        end_token: end_pos,\n        index: chunks.length\n      }\n\n      # Move window with overlap\n      start_pos += chunk_size - overlap\n    end\n\n    chunks\n  end\nend\n</code></pre></p> <p>Semantic Boundary Preservation: <pre><code># Respect sentence and paragraph boundaries\ndef chunk_with_boundaries(text, **options)\n  sentences = split_into_sentences(text)\n  paragraphs = group_into_paragraphs(sentences)\n\n  chunks = []\n  current_chunk = []\n  current_size = 0\n\n  paragraphs.each do |paragraph|\n    paragraph_size = estimate_tokens(paragraph.join(' '))\n\n    # If paragraph fits in current chunk\n    if current_size + paragraph_size &lt;= options[:max_tokens]\n      current_chunk.concat(paragraph)\n      current_size += paragraph_size\n    else\n      # Finalize current chunk if not empty\n      if current_chunk.any?\n        chunks &lt;&lt; create_chunk(current_chunk, chunks.length)\n      end\n\n      # Start new chunk with current paragraph\n      current_chunk = paragraph\n      current_size = paragraph_size\n    end\n  end\n\n  # Add final chunk\n  chunks &lt;&lt; create_chunk(current_chunk, chunks.length) if current_chunk.any?\n\n  chunks\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#content-aware-chunking","title":"Content-Aware Chunking","text":"<p>Document Structure Recognition: <pre><code>class StructuralChunker\n  def chunk_by_structure(text, document_type:)\n    case document_type\n    when 'markdown'\n      chunk_markdown_by_headers(text)\n    when 'code'\n      chunk_code_by_functions(text)\n    when 'academic'\n      chunk_academic_by_sections(text)\n    when 'legal'\n      chunk_legal_by_clauses(text)\n    else\n      chunk_generic_by_paragraphs(text)\n    end\n  end\n\n  private\n\n  def chunk_markdown_by_headers(text)\n    sections = text.split(/^#{1,6}\\s+/)\n\n    sections.map.with_index do |section, index|\n      {\n        content: section.strip,\n        type: 'markdown_section',\n        header_level: detect_header_level(section),\n        index: index\n      }\n    end\n  end\n\n  def chunk_code_by_functions(text)\n    # Language-specific function extraction\n    functions = extract_functions(text)\n\n    functions.map.with_index do |func, index|\n      {\n        content: func[:code],\n        type: 'code_function',\n        function_name: func[:name],\n        language: func[:language],\n        index: index\n      }\n    end\n  end\nend\n</code></pre></p> <p>Context Preservation: <pre><code># Maintain context across chunk boundaries\ndef enhance_chunks_with_context(chunks, text)\n  enhanced_chunks = []\n\n  chunks.each_with_index do |chunk, index|\n    enhanced_chunk = chunk.dup\n\n    # Add previous context for continuity\n    if index &gt; 0\n      prev_chunk = chunks[index - 1]\n      context_size = [prev_chunk[:content].length, 200].min\n      enhanced_chunk[:prev_context] = prev_chunk[:content][-context_size..-1]\n    end\n\n    # Add next context for completeness\n    if index &lt; chunks.length - 1\n      next_chunk = chunks[index + 1]\n      context_size = [next_chunk[:content].length, 200].min\n      enhanced_chunk[:next_context] = next_chunk[:content][0...context_size]\n    end\n\n    # Add document-level context\n    enhanced_chunk[:document_context] = extract_document_context(text)\n\n    enhanced_chunks &lt;&lt; enhanced_chunk\n  end\n\n  enhanced_chunks\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#multi-modal-content-handling","title":"Multi-Modal Content Handling","text":"<p>Image Content Chunking: <pre><code>class ImageContentChunker\n  def chunk_image_content(image_content)\n    # Image descriptions are typically single chunks\n    [{\n      content: image_content.content,  # AI-generated description\n      type: 'image_description',\n      metadata: {\n        dimensions: \"#{image_content.metadata['width']}x#{image_content.metadata['height']}\",\n        file_size: image_content.metadata['file_size'],\n        format: image_content.metadata['file_type']\n      },\n      index: 0\n    }]\n  end\nend\n</code></pre></p> <p>Audio Content Chunking (Planned): <pre><code>class AudioContentChunker\n  def chunk_audio_transcript(audio_content, chunk_duration: 30.seconds)\n    transcript = audio_content.content  # Full transcript\n    timestamps = audio_content.metadata['timestamps'] || []\n\n    chunks = []\n    current_chunk = []\n    chunk_start_time = 0\n\n    timestamps.each do |timestamp|\n      if timestamp[:time] - chunk_start_time &gt;= chunk_duration\n        if current_chunk.any?\n          chunks &lt;&lt; create_audio_chunk(\n            current_chunk, \n            chunk_start_time, \n            timestamp[:time],\n            chunks.length\n          )\n        end\n\n        current_chunk = [timestamp]\n        chunk_start_time = timestamp[:time]\n      else\n        current_chunk &lt;&lt; timestamp\n      end\n    end\n\n    # Add final chunk\n    if current_chunk.any?\n      chunks &lt;&lt; create_audio_chunk(\n        current_chunk,\n        chunk_start_time,\n        timestamps.last[:time],\n        chunks.length\n      )\n    end\n\n    chunks\n  end\nend\n</code></pre></p> <p>Cross-Modal Context: <pre><code># Maintain context across different content types in multi-modal documents\nclass MultiModalChunker\n  def chunk_mixed_content(document)\n    all_chunks = []\n\n    # Process each content type\n    document.text_contents.each do |text_content|\n      text_chunks = TextChunker.new.chunk(text_content.content)\n      text_chunks.each { |chunk| chunk[:content_type] = 'text' }\n      all_chunks.concat(text_chunks)\n    end\n\n    document.image_contents.each do |image_content|\n      image_chunks = ImageContentChunker.new.chunk_image_content(image_content)\n      image_chunks.each { |chunk| chunk[:content_type] = 'image' }\n      all_chunks.concat(image_chunks)\n    end\n\n    # Sort by creation order to maintain document flow\n    all_chunks.sort_by! { |chunk| [chunk[:content_type], chunk[:index]] }\n\n    # Add cross-modal references\n    enhance_with_cross_modal_context(all_chunks)\n  end\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#usage-analytics","title":"Usage Analytics","text":"<p>Ragdoll tracks embedding usage to optimize search results and system performance:</p>"},{"location":"user-guide/embedding-system/#usage-frequency-tracking","title":"Usage Frequency Tracking","text":"<p>Automatic Usage Recording: <pre><code># Every search result interaction is tracked\ndef self.search_similar(query_embedding, **options)\n  results = perform_vector_search(query_embedding, **options)\n\n  # Mark embeddings as used (batch update for performance)\n  embedding_ids = results.map { |r| r[:embedding_id] }\n  mark_embeddings_as_used(embedding_ids)\n\n  results\nend\n\n# Batch update for performance\ndef self.mark_embeddings_as_used(embedding_ids)\n  where(id: embedding_ids).update_all(\n    usage_count: arel_table[:usage_count] + 1,\n    returned_at: Time.current,\n    updated_at: Time.current\n  )\nend\n</code></pre></p> <p>Usage-Based Scoring: <pre><code>def self.calculate_usage_score(embedding)\n  return 0.0 unless embedding.usage_count &gt; 0\n\n  # Frequency component (logarithmic scaling)\n  frequency_score = Math.log(embedding.usage_count + 1) / Math.log(100)\n  frequency_score = [frequency_score, 1.0].min  # Cap at 1.0\n\n  # Recency component (exponential decay)\n  if embedding.returned_at\n    days_since_use = (Time.current - embedding.returned_at) / 1.day\n    recency_score = Math.exp(-days_since_use / 30)  # 30-day half-life\n  else\n    recency_score = 0.0\n  end\n\n  # Weighted combination\n  frequency_weight = 0.7\n  recency_weight = 0.3\n\n  frequency_weight * frequency_score + recency_weight * recency_score\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#recency-based-ranking","title":"Recency-Based Ranking","text":"<p>Time-Aware Search Results: <pre><code>def self.search_with_recency_boost(query_embedding, **options)\n  base_results = search_similar(query_embedding, **options)\n\n  base_results.map do |result|\n    # Calculate recency boost\n    recency_boost = calculate_recency_boost(result[:returned_at])\n\n    # Apply boost to similarity score\n    result[:boosted_similarity] = result[:similarity] + recency_boost\n    result[:recency_boost] = recency_boost\n\n    result\n  end.sort_by { |r| -r[:boosted_similarity] }\nend\n\ndef self.calculate_recency_boost(last_used_at)\n  return 0.0 unless last_used_at\n\n  hours_since_use = (Time.current - last_used_at) / 1.hour\n\n  case hours_since_use\n  when 0..1    then 0.10   # Very recent: significant boost\n  when 1..6    then 0.05   # Recent: moderate boost\n  when 6..24   then 0.02   # Same day: small boost\n  when 24..168 then 0.01   # Same week: minimal boost\n  else 0.0                 # Older: no boost\n  end\nend\n</code></pre></p> <p>Trending Content Detection: <pre><code>class TrendingAnalyzer\n  def self.detect_trending_embeddings(time_window: 24.hours)\n    recent_usage = Embedding\n      .where(returned_at: time_window.ago..Time.current)\n      .group(:id)\n      .having('COUNT(*) &gt; ?', 5)  # Minimum usage threshold\n      .order('COUNT(*) DESC')\n      .limit(100)\n\n    trending_scores = recent_usage.map do |embedding|\n      {\n        embedding_id: embedding.id,\n        recent_usage: embedding.usage_count,\n        velocity: calculate_usage_velocity(embedding),\n        trending_score: calculate_trending_score(embedding)\n      }\n    end\n\n    trending_scores.sort_by { |t| -t[:trending_score] }\n  end\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#performance-metrics","title":"Performance Metrics","text":"<p>Search Quality Metrics: <pre><code>class SearchQualityMetrics\n  def self.calculate_metrics(time_period: 7.days)\n    searches = SearchLog.where(created_at: time_period.ago..Time.current)\n\n    {\n      # Query performance\n      avg_query_time: searches.average(:duration),\n      median_query_time: searches.median(:duration),\n      p95_query_time: searches.percentile(:duration, 95),\n\n      # Result quality\n      avg_similarity_score: searches.average(:avg_similarity),\n      results_per_query: searches.average(:result_count),\n\n      # User engagement\n      click_through_rate: calculate_ctr(searches),\n      zero_result_rate: searches.where(result_count: 0).count.to_f / searches.count,\n\n      # System health\n      error_rate: searches.where.not(error: nil).count.to_f / searches.count,\n      cache_hit_rate: searches.where(cache_hit: true).count.to_f / searches.count\n    }\n  end\nend\n</code></pre></p> <p>Embedding Quality Assessment: <pre><code>class EmbeddingQualityAssessment\n  def self.assess_embedding_quality(embedding)\n    {\n      # Usage-based quality indicators\n      usage_score: embedding.usage_count &gt; 0 ? Math.log(embedding.usage_count + 1) : 0,\n      recency_score: calculate_recency_score(embedding.returned_at),\n\n      # Content-based quality indicators\n      content_length: embedding.content.length,\n      content_complexity: calculate_content_complexity(embedding.content),\n\n      # Vector quality indicators\n      vector_magnitude: calculate_vector_magnitude(embedding.embedding_vector),\n      vector_uniqueness: calculate_vector_uniqueness(embedding),\n\n      # Overall quality score\n      overall_quality: calculate_overall_quality(embedding)\n    }\n  end\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#search-analytics-integration","title":"Search Analytics Integration","text":"<p>Comprehensive Search Logging: <pre><code>class SearchAnalytics\n  def self.log_search(query, results, metadata = {})\n    SearchLog.create!(\n      query_hash: Digest::SHA256.hexdigest(query.to_s),\n      query_embedding_model: metadata[:embedding_model],\n      result_count: results.length,\n      avg_similarity: results.map { |r| r[:similarity] }.sum / results.length,\n      max_similarity: results.map { |r| r[:similarity] }.max,\n      min_similarity: results.map { |r| r[:similarity] }.min,\n      duration: metadata[:duration],\n      filters_applied: metadata[:filters]&amp;.keys || [],\n      cache_hit: metadata[:cache_hit] || false,\n      user_id: metadata[:user_id],\n      session_id: metadata[:session_id],\n      created_at: Time.current\n    )\n  end\nend\n</code></pre></p> <p>Real-time Analytics Dashboard: <pre><code>class AnalyticsDashboard\n  def self.realtime_stats\n    {\n      current_searches_per_minute: current_search_rate,\n      active_embeddings: Embedding.where(returned_at: 1.hour.ago..Time.current).count,\n      top_queries: top_queries_last_hour,\n      search_performance: {\n        avg_duration: recent_searches.average(:duration),\n        success_rate: calculate_success_rate,\n        error_rate: calculate_error_rate\n      },\n      embedding_stats: {\n        total_embeddings: Embedding.count,\n        embeddings_created_today: Embedding.where(created_at: Date.current.beginning_of_day..Time.current).count,\n        most_used_models: Embedding.joins(:embeddable).group('embedding_model').count\n      }\n    }\n  end\nend\n</code></pre></p> <p>Predictive Analytics: <pre><code>class PredictiveAnalytics\n  def self.predict_popular_content(lookahead: 7.days)\n    # Analyze usage patterns to predict trending content\n    historical_data = gather_historical_usage_data\n\n    predictions = historical_data.map do |embedding_data|\n      {\n        embedding_id: embedding_data[:id],\n        current_usage: embedding_data[:usage_count],\n        predicted_usage: predict_future_usage(embedding_data, lookahead),\n        confidence: calculate_prediction_confidence(embedding_data),\n        trending_probability: calculate_trending_probability(embedding_data)\n      }\n    end\n\n    predictions.sort_by { |p| -p[:predicted_usage] }\n  end\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#configuration","title":"Configuration","text":"<p>Ragdoll provides comprehensive configuration options for embedding generation and search:</p>"},{"location":"user-guide/embedding-system/#model-selection-per-content-type","title":"Model Selection Per Content Type","text":"<p>Content-Type Specific Models: <pre><code>Ragdoll::Core.configure do |config|\n  # Text content embeddings\n  config.embedding_config[:text].tap do |c|\n    c[:model] = 'openai/text-embedding-3-large'\n    c[:dimensions] = 3072\n    c[:batch_size] = 100\n    c[:max_tokens] = 8000\n  end\n\n  # Image content embeddings (planned)\n  config.embedding_config[:image].tap do |c|\n    c[:model] = 'openai/clip-vit-large-patch14'\n    c[:dimensions] = 768\n    c[:batch_size] = 32\n    c[:preprocessing] = true\n  end\n\n  # Audio content embeddings (planned)\n  config.embedding_config[:audio].tap do |c|\n    c[:model] = 'openai/whisper-embedding-v1'\n    c[:dimensions] = 1024\n    c[:batch_size] = 16\n    c[:chunk_duration] = 30.seconds\n  end\nend\n</code></pre></p> <p>Model Performance Profiles: <pre><code># Define performance characteristics for different models\nconfig.model_profiles = {\n  'openai/text-embedding-3-large' =&gt; {\n    quality: 'high',\n    speed: 'medium',\n    cost: 'high',\n    max_tokens: 8192,\n    optimal_chunk_size: 1500,\n    recommended_for: ['technical_docs', 'academic_papers', 'complex_content']\n  },\n  'openai/text-embedding-3-small' =&gt; {\n    quality: 'good',\n    speed: 'fast',\n    cost: 'low',\n    max_tokens: 8192,\n    optimal_chunk_size: 1000,\n    recommended_for: ['general_content', 'chat_messages', 'simple_docs']\n  },\n  'ollama/nomic-embed-text' =&gt; {\n    quality: 'medium',\n    speed: 'very_fast',\n    cost: 'free',\n    max_tokens: 2048,\n    optimal_chunk_size: 512,\n    recommended_for: ['privacy_sensitive', 'offline_processing', 'development']\n  }\n}\n</code></pre></p>"},{"location":"user-guide/embedding-system/#dimension-limits-and-optimization","title":"Dimension Limits and Optimization","text":"<p>Dynamic Dimension Handling: <pre><code>config.vector_optimization.tap do |c|\n  # Dimension limits per model\n  c[:max_dimensions] = {\n    'openai/text-embedding-3-large' =&gt; 3072,\n    'openai/text-embedding-3-small' =&gt; 1536,\n    'ollama/nomic-embed-text' =&gt; 768\n  }\n\n  # Dimension reduction options\n  c[:enable_dimension_reduction] = false\n  c[:target_dimensions] = 512  # Reduce to this if enabled\n  c[:reduction_method] = 'pca'  # 'pca', 'truncate', 'quantize'\n\n  # Vector normalization\n  c[:normalize_vectors] = true\n  c[:normalization_method] = 'l2'  # 'l2', 'unit', 'minmax'\nend\n</code></pre></p> <p>Storage Optimization: <pre><code>config.storage_optimization.tap do |c|\n  # Vector compression\n  c[:enable_compression] = false  # Experimental\n  c[:compression_ratio] = 0.8\n  c[:compression_algorithm] = 'quantization'\n\n  # Index optimization\n  c[:index_type] = 'ivfflat'  # 'ivfflat', 'hnsw'\n  c[:index_parameters] = {\n    ivfflat: { lists: 100 },\n    hnsw: { m: 16, ef_construction: 64 }\n  }\n\n  # Cleanup settings\n  c[:cleanup_orphaned_embeddings] = true\n  c[:cleanup_interval] = 1.day\n  c[:max_embedding_age] = 90.days\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#batch-processing-settings","title":"Batch Processing Settings","text":"<p>Batch Configuration: <pre><code>config.batch_processing.tap do |c|\n  # Batch sizes per provider\n  c[:batch_sizes] = {\n    openai: 100,      # OpenAI can handle large batches efficiently\n    anthropic: 50,    # Conservative batch size\n    google: 75,       # Good balance for Google models\n    ollama: 25,       # Local processing, smaller batches\n    huggingface: 32   # Variable based on model size\n  }\n\n  # Batch processing timeouts\n  c[:batch_timeout] = 300.seconds\n  c[:retry_failed_batches] = true\n  c[:max_retry_attempts] = 3\n\n  # Queue management\n  c[:max_queue_size] = 1000\n  c[:queue_priority] = 'high'  # 'high', 'medium', 'low'\n  c[:parallel_batches] = 2     # Number of concurrent batch jobs\nend\n</code></pre></p> <p>Background Job Configuration: <pre><code>config.embedding_jobs.tap do |c|\n  c[:queue_name] = 'embeddings'\n  c[:job_timeout] = 600.seconds\n  c[:retry_on_failure] = true\n\n  # Job scheduling\n  c[:immediate_processing] = false  # Set to true for real-time embedding\n  c[:batch_delay] = 30.seconds      # Wait time before processing batch\n  c[:priority_processing] = true    # Process high-priority content first\nend\n</code></pre></p>"},{"location":"user-guide/embedding-system/#caching-strategies","title":"Caching Strategies","text":"<p>Multi-Level Caching: <pre><code>config.caching.tap do |c|\n  # Query result caching\n  c[:enable_query_cache] = true\n  c[:query_cache_ttl] = 300.seconds\n  c[:query_cache_size] = 1000  # Number of cached queries\n\n  # Embedding caching\n  c[:enable_embedding_cache] = true\n  c[:embedding_cache_ttl] = 1.hour\n  c[:cache_negative_results] = false  # Don't cache failures\n\n  # Vector similarity caching\n  c[:enable_similarity_cache] = true\n  c[:similarity_cache_ttl] = 15.minutes\n  c[:cache_threshold] = 0.95  # Only cache high-similarity results\nend\n</code></pre></p> <p>Cache Implementation: <pre><code>class EmbeddingCache\n  def self.cached_search(query_embedding, **options)\n    cache_key = generate_cache_key(query_embedding, options)\n\n    # Try to get from cache first\n    cached_result = Rails.cache.read(cache_key)\n    return cached_result if cached_result\n\n    # Perform actual search\n    results = Embedding.search_similar(query_embedding, **options)\n\n    # Cache the results\n    Rails.cache.write(\n      cache_key, \n      results, \n      expires_in: Ragdoll.config.caching[:query_cache_ttl]\n    )\n\n    results\n  end\n\n  private\n\n  def self.generate_cache_key(query_embedding, options)\n    # Create a stable hash of the query and options\n    embedding_hash = Digest::SHA256.hexdigest(query_embedding.to_s)\n    options_hash = Digest::SHA256.hexdigest(options.to_s)\n\n    \"embedding_search:#{embedding_hash}:#{options_hash}\"\n  end\nend\n</code></pre></p> <p>Performance Monitoring: <pre><code>config.performance_monitoring.tap do |c|\n  c[:enable_metrics] = true\n  c[:metrics_interval] = 60.seconds\n\n  # Metrics to track\n  c[:track_query_performance] = true\n  c[:track_embedding_generation] = true\n  c[:track_cache_hit_rates] = true\n  c[:track_model_usage] = true\n\n  # Alerting thresholds\n  c[:slow_query_threshold] = 5.seconds\n  c[:low_cache_hit_threshold] = 0.3  # 30%\n  c[:high_error_rate_threshold] = 0.05  # 5%\nend\n</code></pre></p> <p>Environment-Specific Configuration: <pre><code># Development configuration\nif Rails.env.development?\n  config.embedding_config[:text][:model] = 'openai/text-embedding-3-small'  # Faster, cheaper\n  config.batch_processing[:batch_sizes][:openai] = 10  # Smaller batches\n  config.caching[:enable_query_cache] = false  # Disable caching for testing\nend\n\n# Production configuration\nif Rails.env.production?\n  config.embedding_config[:text][:model] = 'openai/text-embedding-3-large'  # Best quality\n  config.batch_processing[:parallel_batches] = 4  # More concurrent processing\n  config.caching[:enable_query_cache] = true  # Enable all caching\n  config.performance_monitoring[:enable_metrics] = true  # Full monitoring\nend\n\n# Test configuration\nif Rails.env.test?\n  config.embedding_config[:text][:model] = 'test/mock-embedding-model'\n  config.batch_processing[:immediate_processing] = true  # Synchronous for tests\n  config.caching[:enable_query_cache] = false  # Predictable test results\nend\n</code></pre></p> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"user-guide/file-uploads/","title":"File Upload System","text":"<p>Ragdoll implements a robust file upload and management system using the Shrine gem, providing secure, scalable file handling with multi-modal content support. The system handles documents, images, and audio files through a unified interface with comprehensive validation and processing capabilities.</p>"},{"location":"user-guide/file-uploads/#shrine-based-production-file-handling","title":"Shrine-based Production File Handling","text":"<p>The file upload system is built around Shrine's flexible architecture, offering:</p> <ul> <li>Multi-Modal Support: Dedicated uploaders for text documents, images, and audio files</li> <li>Storage Flexibility: Configurable backends (filesystem, S3, Google Cloud, Azure)</li> <li>Security Focus: Comprehensive file validation, MIME type checking, and size limits</li> <li>Processing Pipeline: Integration with content extraction and embedding generation</li> <li>Metadata Extraction: Automatic file property detection and storage</li> <li>Production Ready: Designed for high-volume, concurrent file processing</li> </ul>"},{"location":"user-guide/file-uploads/#shrine-integration","title":"Shrine Integration","text":"<p>Shrine provides the foundation for all file operations in Ragdoll, with specialized configurations for different content types.</p>"},{"location":"user-guide/file-uploads/#core-configuration","title":"Core Configuration","text":"<pre><code># lib/ragdoll/core/shrine_config.rb\nrequire \"shrine\"\nrequire \"shrine/storage/file_system\"\n\n# Configure Shrine with filesystem storage\nShrine.storages = {\n  cache: Shrine::Storage::FileSystem.new(\"tmp/uploads\", prefix: \"cache\"),\n  store: Shrine::Storage::FileSystem.new(\"uploads\")\n}\n\n# Essential plugins for Ragdoll functionality\nShrine.plugin :activerecord           # ActiveRecord integration\nShrine.plugin :cached_attachment_data # Handle cached file data\nShrine.plugin :restore_cached_data    # Restore cached attachments\nShrine.plugin :rack_file             # Handle Rack file uploads\nShrine.plugin :validation_helpers     # File validation utilities\nShrine.plugin :determine_mime_type    # Automatic MIME type detection\n</code></pre>"},{"location":"user-guide/file-uploads/#storage-backend-configuration","title":"Storage Backend Configuration","text":"<p>Filesystem Storage (Development/Small Scale): <pre><code># Basic filesystem configuration\nShrine.storages = {\n  cache: Shrine::Storage::FileSystem.new(\"tmp/uploads\", prefix: \"cache\"),\n  store: Shrine::Storage::FileSystem.new(\"public/uploads\")\n}\n\n# With custom directory structure\nShrine.storages = {\n  cache: Shrine::Storage::FileSystem.new(\n    \"tmp/ragdoll_cache\", \n    prefix: \"cache\",\n    permissions: 0644\n  ),\n  store: Shrine::Storage::FileSystem.new(\n    \"storage/ragdoll_files\",\n    prefix: \"files\",\n    permissions: 0644\n  )\n}\n</code></pre></p> <p>Amazon S3 Storage (Production Recommended): <pre><code># Gemfile\ngem \"aws-sdk-s3\", \"~&gt; 1.14\"\n\n# Configuration\nrequire \"shrine/storage/s3\"\n\nShrine.storages = {\n  cache: Shrine::Storage::S3.new(\n    bucket: ENV['S3_CACHE_BUCKET'],\n    region: ENV['AWS_REGION'],\n    access_key_id: ENV['AWS_ACCESS_KEY_ID'],\n    secret_access_key: ENV['AWS_SECRET_ACCESS_KEY'],\n    prefix: \"ragdoll/cache\"\n  ),\n  store: Shrine::Storage::S3.new(\n    bucket: ENV['S3_STORAGE_BUCKET'],\n    region: ENV['AWS_REGION'],\n    access_key_id: ENV['AWS_ACCESS_KEY_ID'],\n    secret_access_key: ENV['AWS_SECRET_ACCESS_KEY'],\n    prefix: \"ragdoll/files\",\n    public: false  # Private files for security\n  )\n}\n\n# S3-specific plugins\nShrine.plugin :presign_endpoint  # Generate presigned URLs\nShrine.plugin :upload_endpoint   # Direct uploads to S3\n</code></pre></p> <p>Google Cloud Storage: <pre><code># Gemfile\ngem \"google-cloud-storage\", \"~&gt; 1.11\"\n\n# Configuration\nrequire \"shrine/storage/google_cloud_storage\"\n\nShrine.storages = {\n  cache: Shrine::Storage::GoogleCloudStorage.new(\n    bucket: ENV['GCS_CACHE_BUCKET'],\n    project: ENV['GOOGLE_CLOUD_PROJECT'],\n    credentials: ENV['GOOGLE_CLOUD_KEYFILE'],\n    prefix: \"ragdoll/cache\"\n  ),\n  store: Shrine::Storage::GoogleCloudStorage.new(\n    bucket: ENV['GCS_STORAGE_BUCKET'],\n    project: ENV['GOOGLE_CLOUD_PROJECT'],\n    credentials: ENV['GOOGLE_CLOUD_KEYFILE'],\n    prefix: \"ragdoll/files\"\n  )\n}\n</code></pre></p>"},{"location":"user-guide/file-uploads/#content-specific-uploaders","title":"Content-Specific Uploaders","text":"<p>Ragdoll defines specialized uploaders for different content types:</p> <pre><code># File uploader for documents (PDF, DOCX, TXT, etc.)\nclass FileUploader &lt; Shrine\n  plugin :validation_helpers\n  plugin :determine_mime_type\n  plugin :add_metadata\n\n  # Add custom metadata extraction\n  add_metadata :custom_properties do |io|\n    {\n      original_filename: extract_filename(io),\n      file_signature: calculate_file_signature(io),\n      processing_metadata: extract_processing_hints(io)\n    }\n  end\n\n  Attacher.validate do\n    validate_max_size 50.megabytes\n    validate_mime_type %w[\n      application/pdf\n      application/vnd.openxmlformats-officedocument.wordprocessingml.document\n      text/plain\n      text/html\n      text/markdown\n      application/json\n      application/rtf\n      application/msword\n    ]\n    validate_extension %w[pdf docx txt html md json rtf doc]\n  end\n\n  private\n\n  def extract_filename(io)\n    io.respond_to?(:original_filename) ? io.original_filename : nil\n  end\n\n  def calculate_file_signature(io)\n    require 'digest'\n    Digest::SHA256.hexdigest(io.read).tap { io.rewind }\n  end\nend\n\n# Image uploader for visual content\nclass ImageUploader &lt; Shrine\n  plugin :validation_helpers\n  plugin :determine_mime_type\n  plugin :store_dimensions  # Extract image dimensions\n  plugin :add_metadata\n\n  add_metadata :image_analysis do |io|\n    {\n      color_profile: extract_color_profile(io),\n      has_transparency: detect_transparency(io),\n      estimated_quality: assess_image_quality(io)\n    }\n  end\n\n  Attacher.validate do\n    validate_max_size 10.megabytes\n    validate_mime_type %w[\n      image/jpeg\n      image/png\n      image/gif\n      image/webp\n      image/bmp\n      image/tiff\n      image/svg+xml\n    ]\n    validate_extension %w[jpg jpeg png gif webp bmp tiff svg]\n\n    # Custom validations\n    validate :acceptable_resolution\n    validate :safe_image_content\n  end\n\n  private\n\n  def acceptable_resolution\n    return unless file\n\n    width, height = file[:metadata][\"width\"], file[:metadata][\"height\"]\n    if width &amp;&amp; height\n      total_pixels = width * height\n      errors &lt;&lt; \"Image resolution too high (#{total_pixels} pixels)\" if total_pixels &gt; 50_000_000\n      errors &lt;&lt; \"Image too small (#{width}x#{height})\" if width &lt; 10 || height &lt; 10\n    end\n  end\nend\n\n# Audio uploader for audio content\nclass AudioUploader &lt; Shrine\n  plugin :validation_helpers\n  plugin :determine_mime_type\n  plugin :add_metadata\n\n  add_metadata :audio_properties do |io|\n    {\n      duration: extract_audio_duration(io),\n      sample_rate: extract_sample_rate(io),\n      channels: extract_channel_count(io),\n      bitrate: extract_bitrate(io)\n    }\n  end\n\n  Attacher.validate do\n    validate_max_size 100.megabytes\n    validate_mime_type %w[\n      audio/mpeg\n      audio/wav\n      audio/mp4\n      audio/webm\n      audio/ogg\n      audio/flac\n      audio/aac\n      audio/x-wav\n    ]\n    validate_extension %w[mp3 wav m4a webm ogg flac aac]\n\n    # Audio-specific validations\n    validate :acceptable_duration\n    validate :acceptable_quality\n  end\n\n  private\n\n  def acceptable_duration\n    return unless file\n\n    duration = file[:metadata][\"audio_properties\"]&amp;.dig(\"duration\")\n    if duration\n      errors &lt;&lt; \"Audio too long (#{duration}s)\" if duration &gt; 3600 # 1 hour max\n      errors &lt;&lt; \"Audio too short (#{duration}s)\" if duration &lt; 1   # 1 second min\n    end\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#processing-pipelines","title":"Processing Pipelines","text":"<p>Integrated processing workflows with background jobs:</p> <pre><code># Processing pipeline integration\nmodule Ragdoll\n  module Core\n    module FileProcessing\n      def self.process_uploaded_file(content_model)\n        case content_model.type\n        when 'Ragdoll::TextContent'\n          process_text_file(content_model)\n        when 'Ragdoll::ImageContent'\n          process_image_file(content_model)\n        when 'Ragdoll::AudioContent'\n          process_audio_file(content_model)\n        end\n      end\n\n      private\n\n      def self.process_text_file(text_content)\n        # Extract text content from file\n        if text_content.data.present?\n          extracted_text = extract_text_from_file(text_content.data)\n          text_content.update!(content: extracted_text)\n\n          # Queue embedding generation\n          Ragdoll::GenerateEmbeddingsJob.perform_later(\n            text_content.document_id\n          )\n        end\n      end\n\n      def self.process_image_file(image_content)\n        # Generate image description using vision AI\n        if image_content.data.present?\n          description = generate_image_description(image_content.data)\n          image_content.update!(content: description) # Store description in content field\n\n          # Queue embedding generation for the description\n          Ragdoll::GenerateEmbeddingsJob.perform_later(\n            image_content.document_id\n          )\n        end\n      end\n\n      def self.process_audio_file(audio_content)\n        # Transcribe audio to text\n        if audio_content.data.present?\n          transcript = transcribe_audio(audio_content.data)\n          audio_content.update!(content: transcript) # Store transcript in content field\n\n          # Queue embedding generation for the transcript\n          Ragdoll::GenerateEmbeddingsJob.perform_later(\n            audio_content.document_id\n          )\n        end\n      end\n    end\n  end\nend\n\n# Automatic processing triggers\nmodule Ragdoll\n  module Core\n    module Models\n      class Content &lt; ActiveRecord::Base\n        # Trigger processing after file attachment\n        after_commit :process_file_if_attached, on: [:create, :update]\n\n        private\n\n        def process_file_if_attached\n          if data_previously_changed? &amp;&amp; data.present?\n            FileProcessing.process_uploaded_file(self)\n          end\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#supported-file-types","title":"Supported File Types","text":"<p>Ragdoll provides comprehensive support for multiple file formats across text, image, and audio content types.</p>"},{"location":"user-guide/file-uploads/#text-documents","title":"Text Documents","text":"<p>Extensive document format support with intelligent content extraction:</p> <p>Primary Formats: <pre><code>SUPPORTED_TEXT_FORMATS = {\n  'application/pdf' =&gt; {\n    extensions: ['.pdf'],\n    max_size: 50.megabytes,\n    extraction_method: :pdf_reader,\n    features: [:text_extraction, :metadata_extraction, :page_analysis]\n  },\n  'application/vnd.openxmlformats-officedocument.wordprocessingml.document' =&gt; {\n    extensions: ['.docx'],\n    max_size: 25.megabytes,\n    extraction_method: :docx_parser,\n    features: [:text_extraction, :formatting_preservation, :image_extraction]\n  },\n  'text/plain' =&gt; {\n    extensions: ['.txt'],\n    max_size: 5.megabytes,\n    extraction_method: :direct_read,\n    features: [:encoding_detection, :charset_conversion]\n  },\n  'text/html' =&gt; {\n    extensions: ['.html', '.htm'],\n    max_size: 10.megabytes,\n    extraction_method: :html_parser,\n    features: [:tag_stripping, :link_extraction, :metadata_extraction]\n  },\n  'text/markdown' =&gt; {\n    extensions: ['.md', '.markdown'],\n    max_size: 5.megabytes,\n    extraction_method: :markdown_parser,\n    features: [:structure_preservation, :link_extraction, :code_block_handling]\n  },\n  'application/json' =&gt; {\n    extensions: ['.json'],\n    max_size: 10.megabytes,\n    extraction_method: :json_parser,\n    features: [:structure_analysis, :key_value_extraction]\n  }\n}\n</code></pre></p> <p>Extended Support: - RTF Documents (<code>.rtf</code>): Rich Text Format with formatting preservation - Legacy Word (<code>.doc</code>): Microsoft Word 97-2003 format - CSV Files (<code>.csv</code>): Structured data with column analysis - XML Files (<code>.xml</code>): Structured markup with schema detection - YAML Files (<code>.yml</code>, <code>.yaml</code>): Configuration and data files</p> <p>Content Extraction Examples: <pre><code># PDF text extraction with metadata\ndef extract_pdf_content(file_path)\n  require 'pdf-reader'\n\n  reader = PDF::Reader.new(file_path)\n  content = {\n    text: reader.pages.map(&amp;:text).join(\"\\n\\n\"),\n    metadata: {\n      page_count: reader.page_count,\n      pdf_version: reader.pdf_version,\n      info: reader.info,\n      encrypted: reader.encrypted?\n    }\n  }\n\n  content\nend\n\n# DOCX extraction with structure preservation\ndef extract_docx_content(file_path)\n  require 'docx'\n\n  doc = Docx::Document.open(file_path)\n  {\n    text: doc.paragraphs.map(&amp;:text).join(\"\\n\"),\n    metadata: {\n      paragraph_count: doc.paragraphs.length,\n      has_tables: doc.tables.any?,\n      has_images: doc.images.any?,\n      word_count: doc.paragraphs.map(&amp;:text).join(' ').split.length\n    }\n  }\nend\n</code></pre></p>"},{"location":"user-guide/file-uploads/#image-files","title":"Image Files","text":"<p>Comprehensive image format support with AI-powered content analysis:</p> <p>Supported Formats: <pre><code>SUPPORTED_IMAGE_FORMATS = {\n  'image/jpeg' =&gt; {\n    extensions: ['.jpg', '.jpeg'],\n    max_size: 10.megabytes,\n    features: [:exif_extraction, :quality_assessment, :color_analysis]\n  },\n  'image/png' =&gt; {\n    extensions: ['.png'],\n    max_size: 15.megabytes,\n    features: [:transparency_detection, :compression_analysis, :metadata_extraction]\n  },\n  'image/gif' =&gt; {\n    extensions: ['.gif'],\n    max_size: 20.megabytes,\n    features: [:animation_detection, :frame_extraction, :color_palette_analysis]\n  },\n  'image/webp' =&gt; {\n    extensions: ['.webp'],\n    max_size: 10.megabytes,\n    features: [:modern_compression, :quality_optimization', :animation_support]\n  },\n  'image/svg+xml' =&gt; {\n    extensions: ['.svg'],\n    max_size: 2.megabytes,\n    features: [:vector_analysis, :text_extraction, :element_parsing]\n  },\n  'image/tiff' =&gt; {\n    extensions: ['.tiff', '.tif'],\n    max_size: 50.megabytes,\n    features: [:multi_page_support, :high_quality_preservation, :metadata_rich]\n  },\n  'image/bmp' =&gt; {\n    extensions: ['.bmp'],\n    max_size: 25.megabytes,\n    features: [:uncompressed_quality, :color_depth_analysis]\n  }\n}\n</code></pre></p> <p>AI-Powered Image Analysis: <pre><code>module Ragdoll\n  module Core\n    class ImageAnalyzer\n      def self.analyze_image(image_file)\n        {\n          description: generate_description(image_file),\n          objects_detected: detect_objects(image_file),\n          text_content: extract_text_ocr(image_file),\n          visual_features: analyze_visual_features(image_file),\n          metadata: extract_technical_metadata(image_file)\n        }\n      end\n\n      private\n\n      def self.generate_description(image_file)\n        # Integration with vision AI services\n        # OpenAI GPT-4 Vision, Google Vision AI, etc.\n        \"A detailed description of the image content\"\n      end\n\n      def self.detect_objects(image_file)\n        # Object detection using AI models\n        [\n          { object: 'person', confidence: 0.95, bbox: [10, 20, 100, 200] },\n          { object: 'building', confidence: 0.87, bbox: [150, 30, 300, 250] }\n        ]\n      end\n\n      def self.extract_text_ocr(image_file)\n        # OCR text extraction for images with text\n        \"Text found in image through OCR\"\n      end\n    end\n  end\nend\n</code></pre></p>"},{"location":"user-guide/file-uploads/#audio-files","title":"Audio Files","text":"<p>Comprehensive audio format support with transcription and analysis:</p> <p>Supported Formats: <pre><code>SUPPORTED_AUDIO_FORMATS = {\n  'audio/mpeg' =&gt; {\n    extensions: ['.mp3'],\n    max_size: 100.megabytes,\n    max_duration: 3600, # 1 hour\n    features: [:id3_metadata, :variable_bitrate, :wide_compatibility]\n  },\n  'audio/wav' =&gt; {\n    extensions: ['.wav'],\n    max_size: 500.megabytes,\n    max_duration: 3600,\n    features: [:uncompressed_quality, :professional_standard, :metadata_support]\n  },\n  'audio/mp4' =&gt; {\n    extensions: ['.m4a', '.mp4'],\n    max_size: 150.megabytes,\n    max_duration: 3600,\n    features: [:aac_compression, :chapter_support, :metadata_rich]\n  },\n  'audio/webm' =&gt; {\n    extensions: ['.webm'],\n    max_size: 100.megabytes,\n    max_duration: 3600,\n    features: [:web_optimized, :open_standard, :good_compression]\n  },\n  'audio/ogg' =&gt; {\n    extensions: ['.ogg'],\n    max_size: 100.megabytes,\n    max_duration: 3600,\n    features: [:open_source, :good_quality, :vorbis_codec]\n  },\n  'audio/flac' =&gt; {\n    extensions: ['.flac'],\n    max_size: 1000.megabytes,\n    max_duration: 3600,\n    features: [:lossless_compression, :high_quality, :metadata_support]\n  },\n  'audio/aac' =&gt; {\n    extensions: ['.aac'],\n    max_size: 100.megabytes,\n    max_duration: 3600,\n    features: [:efficient_compression, :good_quality, :mobile_optimized]\n  }\n}\n</code></pre></p> <p>Audio Processing and Transcription: <pre><code>module Ragdoll\n  module Core\n    class AudioProcessor\n      def self.process_audio(audio_file)\n        {\n          transcript: transcribe_audio(audio_file),\n          metadata: extract_audio_metadata(audio_file),\n          analysis: analyze_audio_content(audio_file),\n          quality_metrics: assess_audio_quality(audio_file)\n        }\n      end\n\n      private\n\n      def self.transcribe_audio(audio_file)\n        # Integration with speech-to-text services\n        # OpenAI Whisper, Google Speech-to-Text, AWS Transcribe\n        {\n          text: \"Transcribed text from audio\",\n          confidence: 0.95,\n          language: 'en',\n          timestamps: [\n            { start: 0.0, end: 5.2, text: \"Hello, this is the beginning\" },\n            { start: 5.2, end: 10.1, text: \"of the audio transcription\" }\n          ]\n        }\n      end\n\n      def self.extract_audio_metadata(audio_file)\n        # Extract technical and ID3 metadata\n        {\n          duration: 180.5, # seconds\n          sample_rate: 44100,\n          channels: 2,\n          bitrate: 320, # kbps\n          format: 'MP3',\n          id3_tags: {\n            title: 'Audio Title',\n            artist: 'Artist Name',\n            album: 'Album Name',\n            year: 2024\n          }\n        }\n      end\n\n      def self.analyze_audio_content(audio_file)\n        # Audio content analysis (speech patterns, music detection, etc.)\n        {\n          content_type: 'speech', # speech, music, mixed, silence\n          speech_segments: 15,\n          music_segments: 0,\n          silence_percentage: 5.2,\n          dominant_language: 'english',\n          speaker_count: 2,\n          audio_quality: 'high'\n        }\n      end\n    end\n  end\nend\n</code></pre></p>"},{"location":"user-guide/file-uploads/#file-validation","title":"File Validation","text":"<p>Comprehensive file validation ensures security, quality, and system stability through multiple validation layers.</p>"},{"location":"user-guide/file-uploads/#multi-layer-validation-system","title":"Multi-Layer Validation System","text":"<pre><code>module Ragdoll\n  module Core\n    module FileValidation\n      class ValidationEngine\n        VALIDATION_LAYERS = [\n          :basic_file_checks,\n          :mime_type_validation,\n          :file_size_validation,\n          :security_scanning,\n          :content_validation,\n          :malware_detection\n        ]\n\n        def self.validate_file(file, content_type)\n          results = {\n            valid: true,\n            errors: [],\n            warnings: [],\n            metadata: {}\n          }\n\n          VALIDATION_LAYERS.each do |layer|\n            layer_result = send(layer, file, content_type)\n\n            results[:errors].concat(layer_result[:errors])\n            results[:warnings].concat(layer_result[:warnings])\n            results[:metadata].merge!(layer_result[:metadata])\n\n            # Stop validation if critical errors found\n            if layer_result[:errors].any? { |e| e[:severity] == :critical }\n              results[:valid] = false\n              break\n            end\n          end\n\n          results[:valid] = results[:errors].empty?\n          results\n        end\n\n        private\n\n        def self.basic_file_checks(file, content_type)\n          errors = []\n          warnings = []\n          metadata = {}\n\n          # File existence and readability\n          unless file.respond_to?(:read)\n            errors &lt;&lt; { message: 'File is not readable', severity: :critical }\n            return { errors: errors, warnings: warnings, metadata: metadata }\n          end\n\n          # File size basic check\n          if file.respond_to?(:size) &amp;&amp; file.size == 0\n            errors &lt;&lt; { message: 'File is empty', severity: :critical }\n          end\n\n          # Extract basic metadata\n          metadata[:original_filename] = file.original_filename if file.respond_to?(:original_filename)\n          metadata[:content_type] = file.content_type if file.respond_to?(:content_type)\n          metadata[:file_size] = file.size if file.respond_to?(:size)\n\n          { errors: errors, warnings: warnings, metadata: metadata }\n        end\n\n        def self.mime_type_validation(file, content_type)\n          errors = []\n          warnings = []\n          metadata = {}\n\n          # Determine actual MIME type\n          actual_mime_type = Marcel::MimeType.for(file)\n          declared_mime_type = file.content_type if file.respond_to?(:content_type)\n\n          metadata[:actual_mime_type] = actual_mime_type\n          metadata[:declared_mime_type] = declared_mime_type\n\n          # Get allowed MIME types for content type\n          allowed_types = get_allowed_mime_types(content_type)\n\n          unless allowed_types.include?(actual_mime_type)\n            errors &lt;&lt; {\n              message: \"Unsupported file type: #{actual_mime_type}\",\n              severity: :critical,\n              allowed_types: allowed_types\n            }\n          end\n\n          # Check for MIME type spoofing\n          if declared_mime_type &amp;&amp; declared_mime_type != actual_mime_type\n            warnings &lt;&lt; {\n              message: \"MIME type mismatch: declared #{declared_mime_type}, actual #{actual_mime_type}\",\n              severity: :medium\n            }\n          end\n\n          { errors: errors, warnings: warnings, metadata: metadata }\n        end\n\n        def self.file_size_validation(file, content_type)\n          errors = []\n          warnings = []\n          metadata = {}\n\n          file_size = file.size if file.respond_to?(:size)\n          return { errors: errors, warnings: warnings, metadata: metadata } unless file_size\n\n          size_limits = get_size_limits(content_type)\n\n          if file_size &gt; size_limits[:max_size]\n            errors &lt;&lt; {\n              message: \"File too large: #{file_size} bytes (max: #{size_limits[:max_size]})\",\n              severity: :critical\n            }\n          end\n\n          if file_size &lt; size_limits[:min_size]\n            warnings &lt;&lt; {\n              message: \"File very small: #{file_size} bytes (min recommended: #{size_limits[:min_size]})\",\n              severity: :low\n            }\n          end\n\n          metadata[:size_category] = categorize_file_size(file_size)\n\n          { errors: errors, warnings: warnings, metadata: metadata }\n        end\n\n        def self.security_scanning(file, content_type)\n          errors = []\n          warnings = []\n          metadata = {}\n\n          # File signature validation\n          file_signature = read_file_signature(file)\n          metadata[:file_signature] = file_signature\n\n          if suspicious_file_signature?(file_signature)\n            errors &lt;&lt; {\n              message: \"Suspicious file signature detected\",\n              severity: :critical\n            }\n          end\n\n          # Embedded content scanning\n          if content_type == 'document'\n            embedded_content = scan_for_embedded_content(file)\n\n            if embedded_content[:has_macros]\n              warnings &lt;&lt; {\n                message: \"Document contains macros\",\n                severity: :medium\n              }\n            end\n\n            if embedded_content[:has_external_links]\n              warnings &lt;&lt; {\n                message: \"Document contains external links\",\n                severity: :low\n              }\n            end\n\n            metadata[:embedded_content] = embedded_content\n          end\n\n          { errors: errors, warnings: warnings, metadata: metadata }\n        end\n\n        def self.content_validation(file, content_type)\n          errors = []\n          warnings = []\n          metadata = {}\n\n          case content_type\n          when 'document'\n            validate_document_content(file, errors, warnings, metadata)\n          when 'image'\n            validate_image_content(file, errors, warnings, metadata)\n          when 'audio'\n            validate_audio_content(file, errors, warnings, metadata)\n          end\n\n          { errors: errors, warnings: warnings, metadata: metadata }\n        end\n\n        def self.malware_detection(file, content_type)\n          errors = []\n          warnings = []\n          metadata = {}\n\n          # Integration with antivirus services\n          if ENV['CLAMAV_ENABLED'] == 'true'\n            scan_result = scan_with_clamav(file)\n\n            if scan_result[:infected]\n              errors &lt;&lt; {\n                message: \"Malware detected: #{scan_result[:virus_name]}\",\n                severity: :critical\n              }\n            end\n\n            metadata[:malware_scan] = scan_result\n          end\n\n          # Behavioral analysis for suspicious files\n          behavioral_analysis = analyze_file_behavior(file)\n          metadata[:behavioral_analysis] = behavioral_analysis\n\n          if behavioral_analysis[:suspicious_score] &gt; 0.7\n            warnings &lt;&lt; {\n              message: \"File shows suspicious characteristics\",\n              severity: :medium,\n              details: behavioral_analysis\n            }\n          end\n\n          { errors: errors, warnings: warnings, metadata: metadata }\n        end\n\n        # Helper methods\n        def self.get_allowed_mime_types(content_type)\n          case content_type\n          when 'document'\n            %w[\n              application/pdf\n              application/vnd.openxmlformats-officedocument.wordprocessingml.document\n              text/plain\n              text/html\n              text/markdown\n              application/json\n            ]\n          when 'image'\n            %w[\n              image/jpeg\n              image/png\n              image/gif\n              image/webp\n              image/bmp\n              image/tiff\n              image/svg+xml\n            ]\n          when 'audio'\n            %w[\n              audio/mpeg\n              audio/wav\n              audio/mp4\n              audio/webm\n              audio/ogg\n              audio/flac\n              audio/aac\n            ]\n          else\n            []\n          end\n        end\n\n        def self.get_size_limits(content_type)\n          case content_type\n          when 'document'\n            { min_size: 10, max_size: 50.megabytes }\n          when 'image'\n            { min_size: 100, max_size: 10.megabytes }\n          when 'audio'\n            { min_size: 1000, max_size: 100.megabytes }\n          else\n            { min_size: 1, max_size: 10.megabytes }\n          end\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#specialized-validation-rules","title":"Specialized Validation Rules","text":"<p>Document-Specific Validation: <pre><code>def validate_document_content(file, errors, warnings, metadata)\n  begin\n    case Marcel::MimeType.for(file)\n    when 'application/pdf'\n      pdf_validation = validate_pdf_document(file)\n      errors.concat(pdf_validation[:errors])\n      warnings.concat(pdf_validation[:warnings])\n      metadata.merge!(pdf_validation[:metadata])\n\n    when 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'\n      docx_validation = validate_docx_document(file)\n      errors.concat(docx_validation[:errors])\n      warnings.concat(docx_validation[:warnings])\n      metadata.merge!(docx_validation[:metadata])\n    end\n  rescue =&gt; e\n    errors &lt;&lt; {\n      message: \"Document validation failed: #{e.message}\",\n      severity: :high\n    }\n  end\nend\n\ndef validate_pdf_document(file)\n  errors = []\n  warnings = []\n  metadata = {}\n\n  begin\n    reader = PDF::Reader.new(file)\n\n    # Check if PDF is encrypted\n    if reader.encrypted?\n      errors &lt;&lt; {\n        message: \"PDF is password protected\",\n        severity: :critical\n      }\n    end\n\n    # Check page count\n    page_count = reader.page_count\n    metadata[:page_count] = page_count\n\n    if page_count &gt; 1000\n      warnings &lt;&lt; {\n        message: \"PDF has many pages (#{page_count}), processing may be slow\",\n        severity: :low\n      }\n    end\n\n    # Check PDF version\n    pdf_version = reader.pdf_version\n    metadata[:pdf_version] = pdf_version\n\n    if pdf_version &gt; 1.7\n      warnings &lt;&lt; {\n        message: \"PDF version #{pdf_version} may have compatibility issues\",\n        severity: :low\n      }\n    end\n\n    # Check for text content\n    has_text = reader.pages.any? { |page| page.text.strip.length &gt; 0 }\n    metadata[:has_extractable_text] = has_text\n\n    unless has_text\n      warnings &lt;&lt; {\n        message: \"PDF appears to contain no extractable text (may be image-only)\",\n        severity: :medium\n      }\n    end\n\n  rescue PDF::Reader::MalformedPDFError =&gt; e\n    errors &lt;&lt; {\n      message: \"PDF file is malformed: #{e.message}\",\n      severity: :critical\n    }\n  rescue =&gt; e\n    errors &lt;&lt; {\n      message: \"PDF validation error: #{e.message}\",\n      severity: :high\n    }\n  end\n\n  { errors: errors, warnings: warnings, metadata: metadata }\nend\n</code></pre></p> <p>Image-Specific Validation: <pre><code>def validate_image_content(file, errors, warnings, metadata)\n  begin\n    # Use ImageMagick or similar for image analysis\n    image_info = extract_image_info(file)\n    metadata.merge!(image_info)\n\n    # Check image dimensions\n    if image_info[:width] &amp;&amp; image_info[:height]\n      total_pixels = image_info[:width] * image_info[:height]\n\n      if total_pixels &gt; 50_000_000 # 50 megapixels\n        warnings &lt;&lt; {\n          message: \"Very high resolution image (#{image_info[:width]}x#{image_info[:height]})\",\n          severity: :medium\n        }\n      end\n\n      if image_info[:width] &lt; 50 || image_info[:height] &lt; 50\n        warnings &lt;&lt; {\n          message: \"Very small image (#{image_info[:width]}x#{image_info[:height]})\",\n          severity: :low\n        }\n      end\n    end\n\n    # Check color depth\n    if image_info[:bit_depth] &amp;&amp; image_info[:bit_depth] &gt; 16\n      warnings &lt;&lt; {\n        message: \"High bit depth (#{image_info[:bit_depth]}) may not be preserved\",\n        severity: :low\n      }\n    end\n\n    # Check for transparency\n    if image_info[:has_alpha]\n      metadata[:supports_transparency] = true\n    end\n\n  rescue =&gt; e\n    errors &lt;&lt; {\n      message: \"Image validation error: #{e.message}\",\n      severity: :high\n    }\n  end\nend\n</code></pre></p>"},{"location":"user-guide/file-uploads/#security-integration","title":"Security Integration","text":"<p>ClamAV Integration: <pre><code>def scan_with_clamav(file)\n  return { scanned: false, reason: 'ClamAV not available' } unless clamav_available?\n\n  begin\n    # Create temporary file for scanning\n    temp_file = Tempfile.new('ragdoll_scan')\n    temp_file.binmode\n    temp_file.write(file.read)\n    temp_file.close\n    file.rewind\n\n    # Run ClamAV scan\n    result = `clamscan --no-summary --infected #{temp_file.path}`\n    exit_code = $?.exitstatus\n\n    scan_result = {\n      scanned: true,\n      infected: exit_code == 1,\n      virus_name: nil,\n      scan_output: result.strip\n    }\n\n    if scan_result[:infected]\n      # Extract virus name from output\n      if match = result.match(/: (.+) FOUND$/)\n        scan_result[:virus_name] = match[1]\n      end\n    end\n\n    scan_result\n\n  rescue =&gt; e\n    { scanned: false, error: e.message }\n  ensure\n    temp_file&amp;.unlink\n  end\nend\n\ndef clamav_available?\n  `which clamscan` &amp;&amp; $?.success?\nrescue\n  false\nend\n</code></pre></p> <p>Behavioral Analysis: <pre><code>def analyze_file_behavior(file)\n  suspicious_score = 0.0\n  indicators = []\n\n  # Check file extension vs content mismatch\n  if extension_content_mismatch?(file)\n    suspicious_score += 0.3\n    indicators &lt;&lt; 'extension_mismatch'\n  end\n\n  # Check for unusual file patterns\n  if unusual_file_pattern?(file)\n    suspicious_score += 0.2\n    indicators &lt;&lt; 'unusual_pattern'\n  end\n\n  # Check for embedded executables\n  if contains_embedded_executable?(file)\n    suspicious_score += 0.5\n    indicators &lt;&lt; 'embedded_executable'\n  end\n\n  {\n    suspicious_score: suspicious_score,\n    indicators: indicators,\n    risk_level: categorize_risk_level(suspicious_score)\n  }\nend\n\ndef categorize_risk_level(score)\n  case score\n  when 0...0.3 then 'low'\n  when 0.3...0.6 then 'medium'\n  when 0.6...0.8 then 'high'\n  else 'critical'\n  end\nend\n</code></pre></p>"},{"location":"user-guide/file-uploads/#storage-configuration","title":"Storage Configuration","text":"<p>Ragdoll supports multiple storage backends through Shrine's flexible architecture, enabling deployment across various infrastructure scenarios.</p>"},{"location":"user-guide/file-uploads/#local-filesystem-storage","title":"Local Filesystem Storage","text":"<p>Ideal for development, testing, and small-scale deployments:</p> <pre><code># Basic filesystem configuration\nShrine.storages = {\n  cache: Shrine::Storage::FileSystem.new(\"tmp/uploads\", prefix: \"cache\"),\n  store: Shrine::Storage::FileSystem.new(\"public/uploads\")\n}\n\n# Advanced filesystem configuration\nShrine.storages = {\n  cache: Shrine::Storage::FileSystem.new(\n    \"tmp/ragdoll_cache\",\n    prefix: \"cache\",\n    permissions: 0644,\n    directory_permissions: 0755,\n    clean: true  # Clean up empty directories\n  ),\n  store: Shrine::Storage::FileSystem.new(\n    \"storage/ragdoll_files\",\n    prefix: \"#{Rails.env}/files\",  # Environment-specific paths\n    permissions: 0644,\n    directory_permissions: 0755,\n    host: \"https://example.com\"   # For generating URLs\n  )\n}\n\n# Custom directory structure\nclass CustomFileSystem &lt; Shrine::Storage::FileSystem\n  def generate_location(io, **options)\n    # Organize files by date and content type\n    date_path = Date.current.strftime(\"%Y/%m/%d\")\n    content_type = options[:metadata]&amp;.dig(\"mime_type\") || \"unknown\"\n    type_dir = content_type.split(\"/\").first # e.g., \"image\", \"audio\", \"application\"\n\n    \"#{type_dir}/#{date_path}/#{super}\"\n  end\nend\n\nShrine.storages = {\n  cache: CustomFileSystem.new(\"tmp/uploads\"),\n  store: CustomFileSystem.new(\"storage/files\")\n}\n</code></pre> <p>Filesystem Storage Benefits: - Simple setup and configuration - No external dependencies - Full control over file organization - Easy backup and migration - No API rate limits or costs</p> <p>Considerations: - Limited scalability - Single point of failure - No built-in CDN capabilities - Requires local disk space management</p>"},{"location":"user-guide/file-uploads/#amazon-s3-integration","title":"Amazon S3 Integration","text":"<p>Recommended for production deployments with scalability requirements:</p> <pre><code># Gemfile\ngem \"aws-sdk-s3\", \"~&gt; 1.14\"\n\n# S3 configuration\nrequire \"shrine/storage/s3\"\n\n# Basic S3 setup\nShrine.storages = {\n  cache: Shrine::Storage::S3.new(\n    bucket: ENV['S3_CACHE_BUCKET'],\n    region: ENV['AWS_REGION'],\n    access_key_id: ENV['AWS_ACCESS_KEY_ID'],\n    secret_access_key: ENV['AWS_SECRET_ACCESS_KEY']\n  ),\n  store: Shrine::Storage::S3.new(\n    bucket: ENV['S3_STORAGE_BUCKET'],\n    region: ENV['AWS_REGION'],\n    access_key_id: ENV['AWS_ACCESS_KEY_ID'],\n    secret_access_key: ENV['AWS_SECRET_ACCESS_KEY']\n  )\n}\n\n# Advanced S3 configuration\nShrine.storages = {\n  cache: Shrine::Storage::S3.new(\n    bucket: ENV['S3_CACHE_BUCKET'],\n    region: ENV['AWS_REGION'],\n    access_key_id: ENV['AWS_ACCESS_KEY_ID'],\n    secret_access_key: ENV['AWS_SECRET_ACCESS_KEY'],\n    prefix: \"ragdoll/cache/#{Rails.env}\",\n    public: false,\n    upload: {\n      server_side_encryption: \"AES256\",\n      metadata_directive: \"REPLACE\"\n    },\n    signer: { expires_in: 1.hour } # Presigned URL expiration\n  ),\n  store: Shrine::Storage::S3.new(\n    bucket: ENV['S3_STORAGE_BUCKET'],\n    region: ENV['AWS_REGION'],\n    access_key_id: ENV['AWS_ACCESS_KEY_ID'],\n    secret_access_key: ENV['AWS_SECRET_ACCESS_KEY'],\n    prefix: \"ragdoll/files/#{Rails.env}\",\n    public: false,  # Private files for security\n    upload: {\n      server_side_encryption: \"AES256\",\n      storage_class: \"STANDARD_IA\", # Infrequent Access for cost optimization\n      metadata_directive: \"REPLACE\"\n    },\n    signer: { expires_in: 24.hours }\n  )\n}\n\n# S3-specific plugins\nShrine.plugin :presign_endpoint, presign_options: -&gt; (request) {\n  {\n    content_length_range: 0..50.megabytes,\n    content_type: request.params[\"content_type\"],\n    expires_in: 1.hour\n  }\n}\n\nShrine.plugin :upload_endpoint, max_size: 50.megabytes\n</code></pre> <p>S3 Lifecycle Management: <pre><code># S3 lifecycle configuration for cost optimization\nclass S3LifecycleManager\n  def self.configure_lifecycle_rules\n    s3_client = Aws::S3::Client.new(\n      region: ENV['AWS_REGION'],\n      access_key_id: ENV['AWS_ACCESS_KEY_ID'],\n      secret_access_key: ENV['AWS_SECRET_ACCESS_KEY']\n    )\n\n    # Configure lifecycle rules\n    lifecycle_configuration = {\n      rules: [\n        {\n          id: 'ragdoll-cache-cleanup',\n          status: 'Enabled',\n          filter: { prefix: 'ragdoll/cache/' },\n          expiration: { days: 1 }, # Delete cache files after 1 day\n        },\n        {\n          id: 'ragdoll-files-archive',\n          status: 'Enabled',\n          filter: { prefix: 'ragdoll/files/' },\n          transitions: [\n            {\n              days: 30,\n              storage_class: 'STANDARD_IA' # Move to Infrequent Access after 30 days\n            },\n            {\n              days: 90,\n              storage_class: 'GLACIER' # Archive to Glacier after 90 days\n            }\n          ]\n        }\n      ]\n    }\n\n    s3_client.put_bucket_lifecycle_configuration(\n      bucket: ENV['S3_STORAGE_BUCKET'],\n      lifecycle_configuration: lifecycle_configuration\n    )\n  end\nend\n</code></pre></p>"},{"location":"user-guide/file-uploads/#google-cloud-storage","title":"Google Cloud Storage","text":"<p>Excellent alternative to S3 with similar features:</p> <pre><code># Gemfile\ngem \"google-cloud-storage\", \"~&gt; 1.11\"\n\n# GCS configuration\nrequire \"shrine/storage/google_cloud_storage\"\n\nShrine.storages = {\n  cache: Shrine::Storage::GoogleCloudStorage.new(\n    bucket: ENV['GCS_CACHE_BUCKET'],\n    project: ENV['GOOGLE_CLOUD_PROJECT'],\n    credentials: ENV['GOOGLE_CLOUD_KEYFILE'],\n    prefix: \"ragdoll/cache/#{Rails.env}\",\n    object_options: {\n      cache_control: \"public, max-age=3600\",\n      metadata: {\n        environment: Rails.env,\n        application: \"ragdoll\"\n      }\n    }\n  ),\n  store: Shrine::Storage::GoogleCloudStorage.new(\n    bucket: ENV['GCS_STORAGE_BUCKET'],\n    project: ENV['GOOGLE_CLOUD_PROJECT'],\n    credentials: ENV['GOOGLE_CLOUD_KEYFILE'],\n    prefix: \"ragdoll/files/#{Rails.env}\",\n    default_acl: \"private\", # Private files for security\n    object_options: {\n      storage_class: \"NEARLINE\", # Cost-effective for infrequent access\n      metadata: {\n        environment: Rails.env,\n        application: \"ragdoll\",\n        retention_policy: \"365days\"\n      }\n    }\n  )\n}\n\n# GCS-specific features\nclass GCSManager\n  def self.setup_bucket_policies\n    storage = Google::Cloud::Storage.new(\n      project_id: ENV['GOOGLE_CLOUD_PROJECT'],\n      credentials: ENV['GOOGLE_CLOUD_KEYFILE']\n    )\n\n    bucket = storage.bucket(ENV['GCS_STORAGE_BUCKET'])\n\n    # Set up lifecycle management\n    bucket.lifecycle do |l|\n      l.add_rule {\n        l.action :delete\n        l.condition age: 365 # Delete files older than 1 year\n        l.condition matches_prefix: \"ragdoll/cache/\"\n      }\n\n      l.add_rule {\n        l.action :set_storage_class, storage_class: \"COLDLINE\"\n        l.condition age: 90\n        l.condition matches_prefix: \"ragdoll/files/\"\n      }\n    end\n\n    # Enable uniform bucket-level access\n    bucket.uniform_bucket_level_access = true\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#azure-blob-storage","title":"Azure Blob Storage","text":"<p>Integration with Microsoft Azure ecosystem:</p> <pre><code># Gemfile\ngem \"azure-storage-blob\", \"~&gt; 2.0\"\n\n# Custom Azure storage implementation\nclass AzureBlobStorage\n  def initialize(container:, account_name:, account_key:, **options)\n    @container = container\n    @client = Azure::Storage::Blob::BlobService.create(\n      storage_account_name: account_name,\n      storage_access_key: account_key\n    )\n    @prefix = options[:prefix]\n    @public = options[:public] != false\n  end\n\n  def upload(io, id, shrine_metadata: {}, **options)\n    content_type = shrine_metadata[\"mime_type\"]\n\n    @client.create_block_blob(\n      @container,\n      path(id),\n      io.read,\n      content_type: content_type,\n      metadata: {\n        \"shrine_metadata\" =&gt; JSON.generate(shrine_metadata),\n        \"ragdoll_version\" =&gt; Ragdoll::Core::VERSION\n      }\n    )\n  end\n\n  def download(id)\n    blob, content = @client.get_blob(@container, path(id))\n    StringIO.new(content)\n  end\n\n  def exists?(id)\n    @client.get_blob_properties(@container, path(id))\n    true\n  rescue Azure::Core::Http::HTTPError =&gt; e\n    raise unless e.status_code == 404\n    false\n  end\n\n  def delete(id)\n    @client.delete_blob(@container, path(id))\n  end\n\n  def url(id, **options)\n    if @public\n      @client.generate_uri(path(id), @container)\n    else\n      # Generate SAS token for private access\n      @client.generate_uri(path(id), @container, {\n        permissions: \"r\",\n        expiry: (Time.now + 1.hour).utc.iso8601\n      })\n    end\n  end\n\n  private\n\n  def path(id)\n    [@prefix, id].compact.join(\"/\")\n  end\nend\n\n# Azure configuration\nShrine.storages = {\n  cache: AzureBlobStorage.new(\n    container: ENV['AZURE_CACHE_CONTAINER'],\n    account_name: ENV['AZURE_STORAGE_ACCOUNT'],\n    account_key: ENV['AZURE_STORAGE_KEY'],\n    prefix: \"ragdoll/cache/#{Rails.env}\",\n    public: false\n  ),\n  store: AzureBlobStorage.new(\n    container: ENV['AZURE_STORAGE_CONTAINER'],\n    account_name: ENV['AZURE_STORAGE_ACCOUNT'],\n    account_key: ENV['AZURE_STORAGE_KEY'],\n    prefix: \"ragdoll/files/#{Rails.env}\",\n    public: false\n  )\n}\n</code></pre>"},{"location":"user-guide/file-uploads/#cdn-integration","title":"CDN Integration","text":"<p>Optimize file delivery with Content Delivery Networks:</p> <p>CloudFront (AWS) Integration: <pre><code># CloudFront configuration for S3\nclass CloudFrontIntegration\n  def self.configure_distribution\n    cloudfront = Aws::CloudFront::Client.new(\n      region: 'us-east-1', # CloudFront is global but API is in us-east-1\n      access_key_id: ENV['AWS_ACCESS_KEY_ID'],\n      secret_access_key: ENV['AWS_SECRET_ACCESS_KEY']\n    )\n\n    distribution_config = {\n      caller_reference: \"ragdoll-#{Time.current.to_i}\",\n      aliases: {\n        quantity: 1,\n        items: [ENV['CDN_DOMAIN']] # e.g., cdn.example.com\n      },\n      default_root_object: \"index.html\",\n      origins: {\n        quantity: 1,\n        items: [{\n          id: \"ragdoll-s3-origin\",\n          domain_name: \"#{ENV['S3_STORAGE_BUCKET']}.s3.#{ENV['AWS_REGION']}.amazonaws.com\",\n          s3_origin_config: {\n            origin_access_identity: \"origin-access-identity/cloudfront/#{ENV['OAI_ID']}\"\n          }\n        }]\n      },\n      default_cache_behavior: {\n        target_origin_id: \"ragdoll-s3-origin\",\n        viewer_protocol_policy: \"redirect-to-https\",\n        trusted_signers: {\n          enabled: false,\n          quantity: 0\n        },\n        forwarded_values: {\n          query_string: false,\n          cookies: {\n            forward: \"none\"\n          }\n        },\n        min_ttl: 0,\n        default_ttl: 86400, # 24 hours\n        max_ttl: 31536000   # 1 year\n      },\n      comment: \"Ragdoll file distribution\",\n      enabled: true,\n      price_class: \"PriceClass_100\" # Use only North America and Europe edge locations\n    }\n\n    response = cloudfront.create_distribution(distribution_config: distribution_config)\n    response.distribution.domain_name\n  end\nend\n\n# URL generation with CDN\nclass CDNFileUploader &lt; Shrine\n  def self.cdn_url(file)\n    if Rails.env.production? &amp;&amp; ENV['CDN_DOMAIN']\n      file.url.gsub(\n        \"#{ENV['S3_STORAGE_BUCKET']}.s3.#{ENV['AWS_REGION']}.amazonaws.com\",\n        ENV['CDN_DOMAIN']\n      )\n    else\n      file.url\n    end\n  end\nend\n</code></pre></p> <p>Multi-CDN Strategy: <pre><code>class MultiCDNManager\n  CDN_PROVIDERS = {\n    cloudflare: {\n      domain: ENV['CLOUDFLARE_CDN_DOMAIN'],\n      priority: 1\n    },\n    cloudfront: {\n      domain: ENV['CLOUDFRONT_CDN_DOMAIN'],\n      priority: 2\n    },\n    fastly: {\n      domain: ENV['FASTLY_CDN_DOMAIN'],\n      priority: 3\n    }\n  }\n\n  def self.get_optimized_url(file, user_location: nil)\n    # Select best CDN based on user location and CDN health\n    best_cdn = select_optimal_cdn(user_location)\n\n    if best_cdn &amp;&amp; best_cdn[:domain]\n      # Replace origin domain with CDN domain\n      original_url = file.url\n      cdn_url = original_url.gsub(\n        /https:\\/\\/[^.]+\\.s3\\.[^.]+\\.amazonaws\\.com/,\n        \"https://#{best_cdn[:domain]}\"\n      )\n\n      # Add cache busting and optimization parameters\n      add_optimization_params(cdn_url, file)\n    else\n      file.url # Fallback to original URL\n    end\n  end\n\n  private\n\n  def self.select_optimal_cdn(user_location)\n    # Health check CDNs and select best performing\n    available_cdns = CDN_PROVIDERS.select { |name, config| \n      cdn_healthy?(config[:domain]) \n    }\n\n    # Select based on priority and user location\n    available_cdns.min_by { |name, config| config[:priority] }&amp;.last\n  end\n\n  def self.cdn_healthy?(domain)\n    # Simple health check\n    begin\n      response = Net::HTTP.get_response(URI(\"https://#{domain}/health\"))\n      response.code == '200'\n    rescue\n      false\n    end\n  end\n\n  def self.add_optimization_params(url, file)\n    # Add image optimization parameters for supported CDNs\n    if file.metadata['mime_type']&amp;.start_with?('image/')\n      \"#{url}?auto=compress,format&amp;w=1200&amp;q=85\"\n    else\n      url\n    end\n  end\nend\n</code></pre></p>"},{"location":"user-guide/file-uploads/#hybrid-storage-strategy","title":"Hybrid Storage Strategy","text":"<p>Combine multiple storage backends for optimal performance and cost:</p> <pre><code>class HybridStorageManager\n  def self.configure_hybrid_storage\n    # Hot storage: Frequently accessed files (S3 Standard)\n    hot_storage = Shrine::Storage::S3.new(\n      bucket: ENV['S3_HOT_BUCKET'],\n      region: ENV['AWS_REGION'],\n      access_key_id: ENV['AWS_ACCESS_KEY_ID'],\n      secret_access_key: ENV['AWS_SECRET_ACCESS_KEY'],\n      prefix: \"ragdoll/hot\",\n      upload: { storage_class: \"STANDARD\" }\n    )\n\n    # Warm storage: Occasionally accessed files (S3 Standard-IA)\n    warm_storage = Shrine::Storage::S3.new(\n      bucket: ENV['S3_WARM_BUCKET'],\n      region: ENV['AWS_REGION'],\n      access_key_id: ENV['AWS_ACCESS_KEY_ID'],\n      secret_access_key: ENV['AWS_SECRET_ACCESS_KEY'],\n      prefix: \"ragdoll/warm\",\n      upload: { storage_class: \"STANDARD_IA\" }\n    )\n\n    # Cold storage: Rarely accessed files (S3 Glacier)\n    cold_storage = Shrine::Storage::S3.new(\n      bucket: ENV['S3_COLD_BUCKET'],\n      region: ENV['AWS_REGION'],\n      access_key_id: ENV['AWS_ACCESS_KEY_ID'],\n      secret_access_key: ENV['AWS_SECRET_ACCESS_KEY'],\n      prefix: \"ragdoll/cold\",\n      upload: { storage_class: \"GLACIER\" }\n    )\n\n    {\n      cache: Shrine::Storage::FileSystem.new(\"tmp/uploads\"),\n      hot: hot_storage,\n      warm: warm_storage,\n      cold: cold_storage\n    }\n  end\n\n  def self.migrate_files_by_usage\n    # Move files between storage tiers based on access patterns\n\n    # Move frequently accessed files to hot storage\n    frequently_accessed = Ragdoll::Embedding\n      .where('usage_count &gt; ? AND returned_at &gt; ?', 10, 30.days.ago)\n      .includes(:embeddable)\n\n    frequently_accessed.each do |embedding|\n      content = embedding.embeddable\n      if content.data.storage_key != :hot\n        migrate_file_to_storage(content, :hot)\n      end\n    end\n\n    # Move rarely accessed files to cold storage\n    rarely_accessed = Ragdoll::Embedding\n      .where('usage_count &lt; ? AND (returned_at IS NULL OR returned_at &lt; ?)', 2, 90.days.ago)\n      .includes(:embeddable)\n\n    rarely_accessed.each do |embedding|\n      content = embedding.embeddable\n      if content.data.storage_key == :hot\n        migrate_file_to_storage(content, :cold)\n      end\n    end\n  end\n\n  private\n\n  def self.migrate_file_to_storage(content, target_storage)\n    # Download from current storage\n    current_file = content.data\n\n    # Upload to target storage\n    new_file = content.data_attacher.upload(\n      current_file.download,\n      storage: target_storage\n    )\n\n    # Update content record\n    content.update!(data: new_file)\n\n    # Delete from old storage\n    current_file.delete\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#file-processing-pipeline","title":"File Processing Pipeline","text":"<p>Ragdoll implements a comprehensive file processing pipeline that ensures secure, efficient handling of uploaded files through multiple stages.</p>"},{"location":"user-guide/file-uploads/#processing-workflow-overview","title":"Processing Workflow Overview","text":"<pre><code>flowchart TD\n    A[File Upload] --&gt; B[Temporary Storage]\n    B --&gt; C[File Validation]\n    C --&gt; D{Validation Passed?}\n    D --&gt;|No| E[Reject Upload]\n    D --&gt;|Yes| F[Metadata Extraction]\n    F --&gt; G[Security Scanning]\n    G --&gt; H{Security Check?}\n    H --&gt;|Failed| I[Quarantine File]\n    H --&gt;|Passed| J[Content Processing]\n    J --&gt; K[Permanent Storage]\n    K --&gt; L[Database Record Creation]\n    L --&gt; M[Background Job Queuing]\n    M --&gt; N[Embedding Generation]\n    N --&gt; O[Content Analysis]\n    O --&gt; P[Processing Complete]</code></pre>"},{"location":"user-guide/file-uploads/#stage-1-file-upload-and-temporary-storage","title":"Stage 1: File Upload and Temporary Storage","text":"<p>Initial file reception with immediate temporary storage:</p> <pre><code>module Ragdoll\n  module Core\n    class FileUploadHandler\n      def self.handle_upload(uploaded_file, document_id, content_type)\n        upload_result = {\n          success: false,\n          file_id: nil,\n          metadata: {},\n          errors: [],\n          processing_stages: []\n        }\n\n        begin\n          # Stage 1: Temporary storage\n          upload_result[:processing_stages] &lt;&lt; {\n            stage: 'temporary_storage',\n            started_at: Time.current,\n            status: 'started'\n          }\n\n          # Create temporary file with unique identifier\n          temp_file_id = generate_temp_file_id\n          temp_file = store_temporarily(uploaded_file, temp_file_id)\n\n          upload_result[:file_id] = temp_file_id\n          upload_result[:metadata][:temp_location] = temp_file.id\n          upload_result[:metadata][:original_filename] = uploaded_file.original_filename\n          upload_result[:metadata][:content_type] = uploaded_file.content_type\n          upload_result[:metadata][:file_size] = uploaded_file.size\n\n          upload_result[:processing_stages].last[:status] = 'completed'\n          upload_result[:processing_stages].last[:completed_at] = Time.current\n\n          # Proceed to validation\n          validation_result = FileValidation::ValidationEngine.validate_file(\n            temp_file, content_type\n          )\n\n          if validation_result[:valid]\n            upload_result.merge!(process_validated_file(\n              temp_file, document_id, content_type, upload_result\n            ))\n          else\n            upload_result[:errors] = validation_result[:errors]\n            cleanup_temp_file(temp_file)\n          end\n\n        rescue =&gt; e\n          upload_result[:errors] &lt;&lt; {\n            stage: 'upload_handling',\n            message: e.message,\n            backtrace: e.backtrace&amp;.first(5)\n          }\n\n          cleanup_temp_file(temp_file) if temp_file\n        end\n\n        upload_result\n      end\n\n      private\n\n      def self.generate_temp_file_id\n        \"temp_#{SecureRandom.uuid}_#{Time.current.to_i}\"\n      end\n\n      def self.store_temporarily(uploaded_file, temp_id)\n        # Use cache storage for temporary files\n        cache_storage = Shrine.storages[:cache]\n\n        # Store with metadata\n        cache_storage.upload(\n          uploaded_file,\n          temp_id,\n          shrine_metadata: {\n            'filename' =&gt; uploaded_file.original_filename,\n            'mime_type' =&gt; uploaded_file.content_type,\n            'size' =&gt; uploaded_file.size\n          }\n        )\n\n        # Return Shrine uploaded file object\n        Shrine::UploadedFile.new(\n          id: temp_id,\n          storage: :cache,\n          metadata: {\n            'filename' =&gt; uploaded_file.original_filename,\n            'mime_type' =&gt; uploaded_file.content_type,\n            'size' =&gt; uploaded_file.size\n          }\n        )\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#stage-2-validation-and-security-checks","title":"Stage 2: Validation and Security Checks","text":"<p>Comprehensive validation using the validation engine:</p> <pre><code>def process_validated_file(temp_file, document_id, content_type, upload_result)\n  # Stage 2: Validation and Security\n  upload_result[:processing_stages] &lt;&lt; {\n    stage: 'validation_security',\n    started_at: Time.current,\n    status: 'started'\n  }\n\n  begin\n    validation_result = FileValidation::ValidationEngine.validate_file(\n      temp_file, content_type\n    )\n\n    upload_result[:metadata].merge!(validation_result[:metadata])\n\n    if validation_result[:valid]\n      upload_result[:processing_stages].last[:status] = 'completed'\n      upload_result[:processing_stages].last[:completed_at] = Time.current\n      upload_result[:processing_stages].last[:validation_warnings] = validation_result[:warnings]\n\n      # Proceed to metadata extraction\n      metadata_result = extract_comprehensive_metadata(temp_file, content_type)\n      upload_result[:metadata].merge!(metadata_result)\n\n      # Proceed to permanent storage\n      storage_result = move_to_permanent_storage(\n        temp_file, document_id, content_type, upload_result[:metadata]\n      )\n\n      upload_result.merge!(storage_result)\n\n    else\n      upload_result[:errors] = validation_result[:errors]\n      upload_result[:processing_stages].last[:status] = 'failed'\n      upload_result[:processing_stages].last[:completed_at] = Time.current\n    end\n\n  rescue =&gt; e\n    upload_result[:errors] &lt;&lt; {\n      stage: 'validation',\n      message: e.message\n    }\n    upload_result[:processing_stages].last[:status] = 'error'\n    upload_result[:processing_stages].last[:completed_at] = Time.current\n  end\n\n  upload_result\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#stage-3-metadata-extraction","title":"Stage 3: Metadata Extraction","text":"<p>Extract comprehensive metadata from different file types:</p> <pre><code>def extract_comprehensive_metadata(file, content_type)\n  metadata = {\n    extraction_timestamp: Time.current.iso8601,\n    file_signature: calculate_file_signature(file),\n    content_type_specific: {}\n  }\n\n  case content_type\n  when 'document'\n    metadata[:content_type_specific] = extract_document_metadata(file)\n  when 'image'\n    metadata[:content_type_specific] = extract_image_metadata(file)\n  when 'audio'\n    metadata[:content_type_specific] = extract_audio_metadata(file)\n  end\n\n  # Add processing hints for downstream jobs\n  metadata[:processing_hints] = generate_processing_hints(file, content_type, metadata)\n\n  metadata\nend\n\ndef extract_document_metadata(file)\n  mime_type = Marcel::MimeType.for(file)\n\n  case mime_type\n  when 'application/pdf'\n    extract_pdf_metadata(file)\n  when 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'\n    extract_docx_metadata(file)\n  when 'text/plain'\n    extract_text_metadata(file)\n  else\n    { type: mime_type, basic_analysis: true }\n  end\nend\n\ndef extract_pdf_metadata(file)\n  begin\n    reader = PDF::Reader.new(file)\n\n    {\n      format: 'pdf',\n      page_count: reader.page_count,\n      pdf_version: reader.pdf_version,\n      encrypted: reader.encrypted?,\n      info: reader.info.to_h,\n      estimated_word_count: estimate_pdf_word_count(reader),\n      has_images: detect_pdf_images(reader),\n      has_forms: detect_pdf_forms(reader),\n      text_extractable: assess_pdf_text_extraction(reader)\n    }\n  rescue =&gt; e\n    {\n      format: 'pdf',\n      error: \"Metadata extraction failed: #{e.message}\",\n      fallback_analysis: true\n    }\n  end\nend\n\ndef extract_image_metadata(file)\n  begin\n    # Use ImageMagick or similar for comprehensive image analysis\n    image_info = identify_image(file)\n\n    {\n      format: image_info[:format],\n      dimensions: {\n        width: image_info[:width],\n        height: image_info[:height],\n        aspect_ratio: calculate_aspect_ratio(image_info[:width], image_info[:height])\n      },\n      color_info: {\n        bit_depth: image_info[:bit_depth],\n        color_space: image_info[:color_space],\n        has_transparency: image_info[:has_alpha],\n        color_count: image_info[:colors]\n      },\n      technical: {\n        compression: image_info[:compression],\n        quality: image_info[:quality],\n        file_size_efficiency: calculate_size_efficiency(image_info)\n      },\n      exif_data: extract_exif_data(file),\n      processing_recommendations: generate_image_processing_recommendations(image_info)\n    }\n  rescue =&gt; e\n    {\n      format: 'unknown_image',\n      error: \"Image analysis failed: #{e.message}\",\n      basic_analysis_only: true\n    }\n  end\nend\n\ndef extract_audio_metadata(file)\n  begin\n    # Use audio analysis libraries\n    audio_info = analyze_audio_file(file)\n\n    {\n      format: audio_info[:format],\n      duration: audio_info[:duration],\n      audio_properties: {\n        sample_rate: audio_info[:sample_rate],\n        channels: audio_info[:channels],\n        bit_rate: audio_info[:bit_rate],\n        bit_depth: audio_info[:bit_depth]\n      },\n      content_analysis: {\n        has_speech: detect_speech_content(file),\n        has_music: detect_music_content(file),\n        silence_percentage: calculate_silence_percentage(file),\n        volume_levels: analyze_volume_levels(file)\n      },\n      transcription_readiness: assess_transcription_feasibility(audio_info),\n      id3_tags: extract_id3_tags(file)\n    }\n  rescue =&gt; e\n    {\n      format: 'unknown_audio',\n      error: \"Audio analysis failed: #{e.message}\",\n      basic_analysis_only: true\n    }\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#stage-4-content-processing-and-analysis","title":"Stage 4: Content Processing and Analysis","text":"<p>Initial content extraction and preparation:</p> <pre><code>def perform_initial_content_processing(file, content_type, metadata)\n  processing_result = {\n    content_extracted: false,\n    content_preview: nil,\n    processing_jobs_queued: [],\n    analysis_metadata: {}\n  }\n\n  case content_type\n  when 'document'\n    processing_result.merge!(process_document_content(file, metadata))\n  when 'image'\n    processing_result.merge!(process_image_content(file, metadata))\n  when 'audio'\n    processing_result.merge!(process_audio_content(file, metadata))\n  end\n\n  processing_result\nend\n\ndef process_document_content(file, metadata)\n  begin\n    # Extract text content immediately for preview\n    extracted_text = extract_text_content(file, metadata[:content_type_specific][:format])\n\n    {\n      content_extracted: true,\n      content_preview: extracted_text[0..500], # First 500 characters\n      full_content_length: extracted_text.length,\n      word_count: extracted_text.split.length,\n      language_detected: detect_language(extracted_text),\n      processing_jobs_queued: [\n        'GenerateEmbeddings',\n        'ExtractKeywords',\n        'GenerateSummary'\n      ]\n    }\n  rescue =&gt; e\n    {\n      content_extracted: false,\n      extraction_error: e.message,\n      processing_jobs_queued: ['ExtractText'] # Fallback to background extraction\n    }\n  end\nend\n\ndef process_image_content(file, metadata)\n  begin\n    # Generate quick image description\n    quick_description = generate_quick_image_description(file)\n\n    {\n      content_extracted: true,\n      content_preview: quick_description,\n      requires_ai_analysis: true,\n      processing_jobs_queued: [\n        'GenerateImageDescription',\n        'ExtractImageText', # OCR if text detected\n        'GenerateEmbeddings'\n      ],\n      analysis_metadata: {\n        optimal_ai_model: recommend_vision_model(metadata),\n        processing_priority: calculate_processing_priority(metadata)\n      }\n    }\n  rescue =&gt; e\n    {\n      content_extracted: false,\n      analysis_error: e.message,\n      processing_jobs_queued: ['ProcessImageContent']\n    }\n  end\nend\n\ndef process_audio_content(file, metadata)\n  begin\n    # Quick audio analysis\n    audio_analysis = perform_quick_audio_analysis(file)\n\n    {\n      content_extracted: false, # Audio requires transcription\n      content_preview: \"Audio file: #{metadata[:content_type_specific][:duration]}s duration\",\n      transcription_required: true,\n      processing_jobs_queued: [\n        'TranscribeAudio',\n        'GenerateEmbeddings' # Will be queued after transcription\n      ],\n      analysis_metadata: {\n        transcription_service: recommend_transcription_service(metadata),\n        estimated_processing_time: estimate_transcription_time(metadata),\n        language_hint: audio_analysis[:detected_language]\n      }\n    }\n  rescue =&gt; e\n    {\n      content_extracted: false,\n      analysis_error: e.message,\n      processing_jobs_queued: ['ProcessAudioContent']\n    }\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#stage-5-permanent-storage","title":"Stage 5: Permanent Storage","text":"<p>Move validated files to permanent storage:</p> <pre><code>def move_to_permanent_storage(temp_file, document_id, content_type, metadata)\n  storage_result = {\n    success: false,\n    permanent_file_id: nil,\n    storage_location: nil,\n    storage_metadata: {}\n  }\n\n  begin\n    # Determine optimal storage based on file characteristics\n    target_storage = determine_optimal_storage(metadata, content_type)\n\n    # Generate permanent file ID with organized structure\n    permanent_id = generate_permanent_file_id(document_id, content_type, metadata)\n\n    # Move file to permanent storage\n    permanent_file = move_file_to_storage(temp_file, target_storage, permanent_id)\n\n    storage_result.merge!({\n      success: true,\n      permanent_file_id: permanent_file.id,\n      storage_location: target_storage,\n      storage_metadata: {\n        stored_at: Time.current.iso8601,\n        storage_class: determine_storage_class(metadata),\n        replication_status: 'pending',\n        backup_scheduled: true\n      }\n    })\n\n    # Schedule backup and replication\n    schedule_file_backup(permanent_file, metadata)\n\n    # Clean up temporary file\n    cleanup_temp_file(temp_file)\n\n  rescue =&gt; e\n    storage_result[:error] = \"Storage failed: #{e.message}\"\n  end\n\n  storage_result\nend\n\ndef determine_optimal_storage(metadata, content_type)\n  file_size = metadata[:file_size]\n  access_pattern = predict_access_pattern(metadata, content_type)\n\n  case access_pattern\n  when :frequent\n    :hot_storage  # Fast, expensive storage\n  when :occasional\n    :warm_storage # Balanced cost/performance\n  when :archive\n    :cold_storage # Cheap, slower access\n  else\n    :standard_storage # Default\n  end\nend\n\ndef generate_permanent_file_id(document_id, content_type, metadata)\n  # Organized file structure: type/year/month/document_id/hash\n  date_path = Date.current.strftime(\"%Y/%m\")\n  file_hash = metadata[:file_signature][0..8] # First 8 chars of hash\n  extension = extract_file_extension(metadata[:original_filename])\n\n  \"#{content_type}/#{date_path}/#{document_id}/#{file_hash}#{extension}\"\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#stage-6-database-record-creation","title":"Stage 6: Database Record Creation","text":"<p>Create database records with comprehensive metadata:</p> <pre><code>def create_database_records(document_id, content_type, file_info, metadata, processing_info)\n  ActiveRecord::Base.transaction do\n    # Find or create document\n    document = Ragdoll::Document.find(document_id)\n\n    # Create content record based on type\n    content_record = create_content_record(\n      document, content_type, file_info, metadata, processing_info\n    )\n\n    # Update document status and metadata\n    update_document_with_file_info(document, content_record, metadata)\n\n    # Queue background processing jobs\n    queue_processing_jobs(content_record, processing_info[:processing_jobs_queued])\n\n    {\n      success: true,\n      document_id: document.id,\n      content_id: content_record.id,\n      jobs_queued: processing_info[:processing_jobs_queued].length\n    }\n  end\nrescue =&gt; e\n  {\n    success: false,\n    error: \"Database record creation failed: #{e.message}\"\n  }\nend\n\ndef create_content_record(document, content_type, file_info, metadata, processing_info)\n  content_class = case content_type\n                  when 'document' then Ragdoll::TextContent\n                  when 'image' then Ragdoll::ImageContent\n                  when 'audio' then Ragdoll::AudioContent\n                  else Ragdoll::Content\n                  end\n\n  content_attributes = {\n    document: document,\n    data: build_shrine_file_object(file_info),\n    embedding_model: determine_embedding_model(content_type, metadata),\n    metadata: build_content_metadata(metadata, processing_info)\n  }\n\n  # Add content-specific attributes\n  case content_type\n  when 'document'\n    content_attributes[:content] = processing_info[:content_preview] if processing_info[:content_extracted]\n  when 'image'\n    content_attributes[:content] = processing_info[:content_preview] # Description\n  when 'audio'\n    # Content (transcript) will be added by background job\n  end\n\n  content_class.create!(content_attributes)\nend\n\ndef build_shrine_file_object(file_info)\n  {\n    'id' =&gt; file_info[:permanent_file_id],\n    'storage' =&gt; file_info[:storage_location].to_s,\n    'metadata' =&gt; file_info[:storage_metadata].merge({\n      'filename' =&gt; file_info[:original_filename],\n      'mime_type' =&gt; file_info[:actual_mime_type],\n      'size' =&gt; file_info[:file_size]\n    })\n  }\nend\n\ndef queue_processing_jobs(content_record, job_names)\n  job_names.each do |job_name|\n    case job_name\n    when 'GenerateEmbeddings'\n      Ragdoll::GenerateEmbeddingsJob.perform_later(content_record.document_id)\n    when 'ExtractKeywords'\n      Ragdoll::ExtractKeywordsJob.perform_later(content_record.document_id)\n    when 'GenerateSummary'\n      Ragdoll::GenerateSummaryJob.perform_later(content_record.document_id)\n    when 'TranscribeAudio'\n      # Custom job for audio transcription\n      TranscribeAudioJob.perform_later(content_record.id)\n    when 'GenerateImageDescription'\n      # Custom job for image description\n      GenerateImageDescriptionJob.perform_later(content_record.id)\n    end\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#pipeline-monitoring-and-error-recovery","title":"Pipeline Monitoring and Error Recovery","text":"<pre><code>module Ragdoll\n  module Core\n    class FileProcessingMonitor\n      def self.monitor_processing_pipeline\n        {\n          active_uploads: count_active_uploads,\n          processing_stages: analyze_processing_stages,\n          error_rates: calculate_error_rates,\n          throughput_metrics: calculate_throughput,\n          storage_utilization: analyze_storage_usage\n        }\n      end\n\n      def self.recover_failed_processing(document_id)\n        document = Ragdoll::Document.find(document_id)\n\n        # Analyze what went wrong\n        failure_analysis = analyze_processing_failure(document)\n\n        # Attempt recovery based on failure type\n        case failure_analysis[:failure_type]\n        when :validation_failure\n          # Re-validate with updated rules\n          retry_file_validation(document)\n        when :storage_failure\n          # Retry storage operation\n          retry_file_storage(document)\n        when :processing_timeout\n          # Retry with extended timeout\n          retry_processing_jobs(document, timeout: :extended)\n        else\n          # Manual intervention required\n          flag_for_manual_review(document, failure_analysis)\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#security-considerations","title":"Security Considerations","text":"<p>Ragdoll implements comprehensive security measures to protect against malicious files, unauthorized access, and data breaches throughout the file handling lifecycle.</p>"},{"location":"user-guide/file-uploads/#multi-layered-security-architecture","title":"Multi-Layered Security Architecture","text":"<pre><code>flowchart TD\n    A[File Upload] --&gt; B[Input Validation]\n    B --&gt; C[MIME Type Verification]\n    C --&gt; D[Malware Scanning]\n    D --&gt; E[Content Analysis]\n    E --&gt; F[Access Control]\n    F --&gt; G[Encryption at Rest]\n    G --&gt; H[Secure URL Generation]\n    H --&gt; I[Audit Logging]</code></pre>"},{"location":"user-guide/file-uploads/#file-type-validation","title":"File Type Validation","text":"<p>Strict validation prevents malicious file uploads:</p> <pre><code>module Ragdoll\n  module Core\n    module Security\n      class FileTypeValidator\n        # Whitelist approach - only allow explicitly permitted file types\n        ALLOWED_SIGNATURES = {\n          # PDF files\n          'application/pdf' =&gt; [\n            ['%PDF-1.', 0], # PDF signature at start\n          ],\n          # DOCX files\n          'application/vnd.openxmlformats-officedocument.wordprocessingml.document' =&gt; [\n            ['PK', 0], # ZIP signature (DOCX is ZIP-based)\n            ['[Content_Types].xml', 30..100] # DOCX-specific content\n          ],\n          # Image files\n          'image/jpeg' =&gt; [\n            ['\\xFF\\xD8\\xFF', 0], # JPEG signature\n          ],\n          'image/png' =&gt; [\n            ['\\x89PNG\\r\\n\\x1a\\n', 0], # PNG signature\n          ],\n          # Audio files\n          'audio/mpeg' =&gt; [\n            ['ID3', 0], # MP3 with ID3 tag\n            ['\\xFF\\xFB', 0], # MP3 without ID3\n            ['\\xFF\\xF3', 0], # MP3 alternative\n          ]\n        }\n\n        def self.validate_file_type(file)\n          validation_result = {\n            valid: false,\n            detected_type: nil,\n            security_issues: []\n          }\n\n          # Read file signature\n          file_signature = read_file_signature(file, 256) # First 256 bytes\n\n          # Detect actual file type by signature\n          detected_type = detect_type_by_signature(file_signature)\n          validation_result[:detected_type] = detected_type\n\n          # Check against declared MIME type\n          declared_type = file.content_type if file.respond_to?(:content_type)\n\n          if declared_type &amp;&amp; declared_type != detected_type\n            validation_result[:security_issues] &lt;&lt; {\n              type: :mime_type_spoofing,\n              severity: :high,\n              message: \"Declared type #{declared_type} doesn't match detected type #{detected_type}\"\n            }\n          end\n\n          # Validate against whitelist\n          if ALLOWED_SIGNATURES.key?(detected_type)\n            validation_result[:valid] = verify_file_signature(file_signature, detected_type)\n          else\n            validation_result[:security_issues] &lt;&lt; {\n              type: :unsupported_file_type,\n              severity: :critical,\n              message: \"File type #{detected_type} is not allowed\"\n            }\n          end\n\n          # Additional security checks\n          validation_result[:security_issues].concat(perform_security_analysis(file))\n\n          validation_result\n        end\n\n        private\n\n        def self.read_file_signature(file, bytes = 256)\n          file.rewind\n          signature = file.read(bytes)\n          file.rewind\n          signature\n        end\n\n        def self.detect_type_by_signature(signature)\n          ALLOWED_SIGNATURES.each do |mime_type, signatures|\n            signatures.each do |sig_pattern, position|\n              if position.is_a?(Range)\n                # Check if pattern exists within range\n                range_content = signature[position]\n                return mime_type if range_content&amp;.include?(sig_pattern)\n              else\n                # Check exact position\n                return mime_type if signature[position, sig_pattern.length] == sig_pattern\n              end\n            end\n          end\n\n          'application/octet-stream' # Unknown type\n        end\n\n        def self.perform_security_analysis(file)\n          security_issues = []\n\n          # Check for embedded executables\n          if contains_embedded_executable?(file)\n            security_issues &lt;&lt; {\n              type: :embedded_executable,\n              severity: :critical,\n              message: \"File contains embedded executable content\"\n            }\n          end\n\n          # Check for suspicious file patterns\n          if suspicious_file_patterns?(file)\n            security_issues &lt;&lt; {\n              type: :suspicious_patterns,\n              severity: :medium,\n              message: \"File contains suspicious patterns\"\n            }\n          end\n\n          # Check file size anomalies\n          if unusual_file_size?(file)\n            security_issues &lt;&lt; {\n              type: :size_anomaly,\n              severity: :low,\n              message: \"File size is unusual for declared type\"\n            }\n          end\n\n          security_issues\n        end\n\n        def self.contains_embedded_executable?(file)\n          # Check for common executable signatures within the file\n          executable_signatures = [\n            'MZ',      # Windows PE\n            '\\x7fELF',  # Linux ELF\n            '\\xCE\\xFA\\xED\\xFE', # Mach-O\n            '\\xFE\\xED\\xFA\\xCE'  # Mach-O (reverse)\n          ]\n\n          file.rewind\n          content = file.read\n          file.rewind\n\n          executable_signatures.any? { |sig| content.include?(sig) }\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#malware-and-content-scanning","title":"Malware and Content Scanning","text":"<p>Integrated malware detection and content analysis:</p> <pre><code>module Ragdoll\n  module Core\n    module Security\n      class MalwareScanner\n        def self.scan_file(file)\n          scan_results = {\n            scanned: false,\n            clean: false,\n            threats_detected: [],\n            scan_engines: []\n          }\n\n          # Multi-engine scanning approach\n          scan_engines = configure_scan_engines\n\n          scan_engines.each do |engine|\n            begin\n              engine_result = engine[:scanner].call(file)\n              scan_results[:scan_engines] &lt;&lt; {\n                name: engine[:name],\n                result: engine_result,\n                scanned_at: Time.current\n              }\n\n              if engine_result[:threats_detected]&amp;.any?\n                scan_results[:threats_detected].concat(engine_result[:threats_detected])\n              end\n\n            rescue =&gt; e\n              scan_results[:scan_engines] &lt;&lt; {\n                name: engine[:name],\n                error: e.message,\n                scanned_at: Time.current\n              }\n            end\n          end\n\n          scan_results[:scanned] = scan_results[:scan_engines].any? { |e| !e[:error] }\n          scan_results[:clean] = scan_results[:threats_detected].empty?\n\n          # Quarantine file if threats detected\n          if !scan_results[:clean]\n            quarantine_file(file, scan_results[:threats_detected])\n          end\n\n          scan_results\n        end\n\n        private\n\n        def self.configure_scan_engines\n          engines = []\n\n          # ClamAV integration\n          if clamav_available?\n            engines &lt;&lt; {\n              name: 'ClamAV',\n              priority: 1,\n              scanner: method(:scan_with_clamav)\n            }\n          end\n\n          # Custom pattern matching\n          engines &lt;&lt; {\n            name: 'PatternMatcher',\n            priority: 2,\n            scanner: method(:scan_with_patterns)\n          }\n\n          # Behavioral analysis\n          engines &lt;&lt; {\n            name: 'BehavioralAnalysis',\n            priority: 3,\n            scanner: method(:behavioral_scan)\n          }\n\n          engines\n        end\n\n        def self.scan_with_clamav(file)\n          temp_file = create_temp_file_for_scanning(file)\n\n          begin\n            result = `clamscan --no-summary --infected #{temp_file.path} 2&gt;&amp;1`\n            exit_code = $?.exitstatus\n\n            scan_result = {\n              engine: 'clamav',\n              threats_detected: [],\n              scan_output: result.strip\n            }\n\n            if exit_code == 1 # Virus found\n              if match = result.match(/: (.+) FOUND$/)\n                scan_result[:threats_detected] &lt;&lt; {\n                  name: match[1],\n                  type: 'virus',\n                  severity: 'critical'\n                }\n              end\n            elsif exit_code &gt; 1 # Scan error\n              scan_result[:error] = \"ClamAV scan failed: #{result}\"\n            end\n\n            scan_result\n\n          ensure\n            temp_file&amp;.unlink\n          end\n        end\n\n        def self.scan_with_patterns(file)\n          # Custom malware signature patterns\n          malware_patterns = [\n            {\n              pattern: /eval\\s*\\(\\s*base64_decode/i,\n              name: 'PHP Base64 Eval',\n              type: 'webshell',\n              severity: 'high'\n            },\n            {\n              pattern: /&lt;script[^&gt;]*&gt;.*?(document\\.write|eval|unescape).*?&lt;\\/script&gt;/mi,\n              name: 'Malicious JavaScript',\n              type: 'script_injection',\n              severity: 'high'\n            },\n            {\n              pattern: /cmd\\.exe|powershell\\.exe|sh\\s+-c/i,\n              name: 'Command Execution',\n              type: 'command_injection',\n              severity: 'critical'\n            }\n          ]\n\n          file.rewind\n          content = file.read(10.megabytes) # Scan first 10MB\n          file.rewind\n\n          threats_detected = []\n\n          malware_patterns.each do |pattern_info|\n            if content.match?(pattern_info[:pattern])\n              threats_detected &lt;&lt; {\n                name: pattern_info[:name],\n                type: pattern_info[:type],\n                severity: pattern_info[:severity],\n                detection_method: 'pattern_matching'\n              }\n            end\n          end\n\n          {\n            engine: 'pattern_matcher',\n            threats_detected: threats_detected\n          }\n        end\n\n        def self.behavioral_scan(file)\n          suspicious_behaviors = []\n\n          # Analyze file entropy (high entropy might indicate encryption/packing)\n          entropy = calculate_file_entropy(file)\n          if entropy &gt; 7.5 # High entropy threshold\n            suspicious_behaviors &lt;&lt; {\n              name: 'High Entropy Content',\n              type: 'suspicious_structure',\n              severity: 'medium',\n              details: { entropy: entropy }\n            }\n          end\n\n          # Check for unusual file structure\n          if unusual_file_structure?(file)\n            suspicious_behaviors &lt;&lt; {\n              name: 'Unusual File Structure',\n              type: 'structural_anomaly',\n              severity: 'low'\n            }\n          end\n\n          {\n            engine: 'behavioral_analysis',\n            threats_detected: suspicious_behaviors.select { |b| b[:severity] == 'critical' },\n            suspicious_behaviors: suspicious_behaviors\n          }\n        end\n\n        def self.quarantine_file(file, threats)\n          # Move file to quarantine location\n          quarantine_id = SecureRandom.uuid\n          quarantine_path = \"quarantine/#{Date.current.strftime('%Y/%m/%d')}/#{quarantine_id}\"\n\n          # Store quarantine information\n          quarantine_info = {\n            id: quarantine_id,\n            original_filename: file.original_filename,\n            quarantined_at: Time.current,\n            threats: threats,\n            file_signature: calculate_file_hash(file)\n          }\n\n          # Log security incident\n          log_security_incident('file_quarantined', quarantine_info)\n\n          # Notify security team\n          notify_security_team(quarantine_info)\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#access-control-and-authorization","title":"Access Control and Authorization","text":"<p>Robust access control for file operations:</p> <pre><code>module Ragdoll\n  module Core\n    module Security\n      class FileAccessControl\n        def self.authorize_file_access(user, file, operation)\n          authorization_result = {\n            authorized: false,\n            reason: nil,\n            restrictions: []\n          }\n\n          # Check basic permissions\n          base_permission = check_base_permission(user, operation)\n          unless base_permission[:allowed]\n            authorization_result[:reason] = base_permission[:reason]\n            return authorization_result\n          end\n\n          # Check file-specific permissions\n          file_permission = check_file_permission(user, file, operation)\n          unless file_permission[:allowed]\n            authorization_result[:reason] = file_permission[:reason]\n            return authorization_result\n          end\n\n          # Check rate limits\n          rate_limit_check = check_rate_limits(user, operation)\n          if rate_limit_check[:exceeded]\n            authorization_result[:reason] = 'Rate limit exceeded'\n            authorization_result[:restrictions] &lt;&lt; rate_limit_check\n            return authorization_result\n          end\n\n          # Check file access patterns\n          access_pattern_check = analyze_access_pattern(user, file, operation)\n          if access_pattern_check[:suspicious]\n            # Log suspicious activity but allow access with monitoring\n            log_suspicious_access(user, file, operation, access_pattern_check)\n            authorization_result[:restrictions] &lt;&lt; {\n              type: 'enhanced_monitoring',\n              reason: access_pattern_check[:reason]\n            }\n          end\n\n          authorization_result[:authorized] = true\n          authorization_result\n        end\n\n        def self.generate_secure_file_url(file, user, options = {})\n          # Generate time-limited, signed URLs for file access\n          expires_at = options[:expires_at] || 1.hour.from_now\n\n          # Create signature\n          signature_data = {\n            file_id: file.id,\n            user_id: user&amp;.id,\n            expires_at: expires_at.to_i,\n            operation: options[:operation] || 'read'\n          }\n\n          signature = generate_url_signature(signature_data)\n\n          # Build secure URL\n          base_url = file.url\n          query_params = {\n            token: signature,\n            expires: expires_at.to_i,\n            user: user&amp;.id\n          }.compact\n\n          \"#{base_url}?#{query_params.to_query}\"\n        end\n\n        private\n\n        def self.check_base_permission(user, operation)\n          # Define operation permissions\n          operation_permissions = {\n            'read' =&gt; ['viewer', 'editor', 'admin'],\n            'write' =&gt; ['editor', 'admin'],\n            'delete' =&gt; ['admin'],\n            'upload' =&gt; ['editor', 'admin']\n          }\n\n          required_roles = operation_permissions[operation] || []\n\n          if user.nil?\n            return { allowed: false, reason: 'Authentication required' }\n          end\n\n          unless required_roles.include?(user.role)\n            return { \n              allowed: false, \n              reason: \"Insufficient permissions. Required: #{required_roles.join(', ')}\" \n            }\n          end\n\n          { allowed: true }\n        end\n\n        def self.check_file_permission(user, file, operation)\n          # Check file-specific access rules\n\n          # Personal files - only owner can access\n          if file.metadata&amp;.dig('access_control', 'owner_only')\n            file_owner_id = file.metadata.dig('access_control', 'owner_id')\n            unless file_owner_id == user.id\n              return { \n                allowed: false, \n                reason: 'File is private to owner only' \n              }\n            end\n          end\n\n          # Restricted files - check explicit permissions\n          if file.metadata&amp;.dig('access_control', 'restricted')\n            allowed_users = file.metadata.dig('access_control', 'allowed_users') || []\n            unless allowed_users.include?(user.id)\n              return { \n                allowed: false, \n                reason: 'User not in file access list' \n              }\n            end\n          end\n\n          # Quarantined files - admin only\n          if file.metadata&amp;.dig('security', 'quarantined')\n            unless user.role == 'admin'\n              return { \n                allowed: false, \n                reason: 'File is quarantined - admin access only' \n              }\n            end\n          end\n\n          { allowed: true }\n        end\n\n        def self.generate_url_signature(data)\n          # Generate HMAC signature for URL security\n          secret_key = ENV['RAGDOLL_URL_SIGNING_KEY'] || Rails.application.secret_key_base\n          payload = data.to_json\n\n          OpenSSL::HMAC.hexdigest('SHA256', secret_key, payload)\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#encryption-at-rest","title":"Encryption at Rest","text":"<p>Comprehensive encryption for stored files:</p> <pre><code>module Ragdoll\n  module Core\n    module Security\n      class FileEncryption\n        def self.configure_encryption\n          # S3 server-side encryption configuration\n          if Shrine.storages[:store].is_a?(Shrine::Storage::S3)\n            configure_s3_encryption\n          end\n\n          # Application-level encryption for sensitive files\n          configure_application_encryption\n        end\n\n        def self.encrypt_sensitive_file(file)\n          # Identify sensitive files that need application-level encryption\n          return file unless requires_encryption?(file)\n\n          # Generate file-specific encryption key\n          encryption_key = generate_file_encryption_key\n\n          # Encrypt file content\n          encrypted_content = encrypt_content(file.read, encryption_key)\n\n          # Store encryption metadata\n          encryption_metadata = {\n            encrypted: true,\n            encryption_algorithm: 'AES-256-GCM',\n            key_id: store_encryption_key(encryption_key),\n            encrypted_at: Time.current.iso8601\n          }\n\n          # Create encrypted file object\n          encrypted_file = StringIO.new(encrypted_content)\n          encrypted_file.define_singleton_method(:metadata) do\n            file.metadata.merge(encryption: encryption_metadata)\n          end\n\n          encrypted_file\n        end\n\n        def self.decrypt_file(encrypted_file)\n          encryption_metadata = encrypted_file.metadata['encryption']\n          return encrypted_file unless encryption_metadata&amp;.dig('encrypted')\n\n          # Retrieve encryption key\n          encryption_key = retrieve_encryption_key(encryption_metadata['key_id'])\n\n          # Decrypt content\n          decrypted_content = decrypt_content(\n            encrypted_file.read, \n            encryption_key, \n            encryption_metadata['encryption_algorithm']\n          )\n\n          StringIO.new(decrypted_content)\n        end\n\n        private\n\n        def self.configure_s3_encryption\n          # Configure S3 server-side encryption\n          s3_config = {\n            server_side_encryption: 'AES256',\n            # Or use KMS for additional key management\n            # server_side_encryption: 'aws:kms',\n            # ssekms_key_id: ENV['AWS_KMS_KEY_ID']\n          }\n\n          # Apply to storage configuration\n          Shrine.storages[:store].instance_variable_get(:@s3).client.config.update(\n            server_side_encryption: s3_config[:server_side_encryption]\n          )\n        end\n\n        def self.requires_encryption?(file)\n          # Determine if file needs application-level encryption\n          sensitive_patterns = [\n            /password/i,\n            /confidential/i,\n            /private/i,\n            /secret/i,\n            /ssn/i,\n            /social.security/i\n          ]\n\n          filename = file.original_filename || ''\n          content_sample = file.read(1024) # First 1KB\n          file.rewind\n\n          # Check filename\n          return true if sensitive_patterns.any? { |pattern| filename.match?(pattern) }\n\n          # Check content\n          return true if sensitive_patterns.any? { |pattern| content_sample.match?(pattern) }\n\n          false\n        end\n\n        def self.encrypt_content(content, key)\n          cipher = OpenSSL::Cipher.new('AES-256-GCM')\n          cipher.encrypt\n          cipher.key = key\n\n          iv = cipher.random_iv\n          encrypted = cipher.update(content) + cipher.final\n          tag = cipher.auth_tag\n\n          # Combine IV, tag, and encrypted content\n          [iv, tag, encrypted].map(&amp;:unpack1).join\n        end\n\n        def self.decrypt_content(encrypted_data, key, algorithm)\n          # Split combined data\n          iv_length = 12 # GCM IV length\n          tag_length = 16 # GCM tag length\n\n          iv = encrypted_data[0, iv_length]\n          tag = encrypted_data[iv_length, tag_length]\n          encrypted = encrypted_data[iv_length + tag_length..-1]\n\n          # Decrypt\n          decipher = OpenSSL::Cipher.new(algorithm)\n          decipher.decrypt\n          decipher.key = key\n          decipher.iv = iv\n          decipher.auth_tag = tag\n\n          decipher.update(encrypted) + decipher.final\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#security-audit-and-logging","title":"Security Audit and Logging","text":"<p>Comprehensive security monitoring and audit trails:</p> <pre><code>module Ragdoll\n  module Core\n    module Security\n      class SecurityAuditor\n        def self.log_file_operation(operation, file, user, result)\n          audit_entry = {\n            timestamp: Time.current.iso8601,\n            operation: operation,\n            file_id: file.id,\n            file_path: file.metadata['filename'],\n            user_id: user&amp;.id,\n            user_ip: get_user_ip,\n            result: result,\n            metadata: {\n              file_size: file.metadata['size'],\n              mime_type: file.metadata['mime_type'],\n              security_scan_result: file.metadata.dig('security', 'scan_result')\n            }\n          }\n\n          # Log to security audit log\n          write_security_log(audit_entry)\n\n          # Send to SIEM if configured\n          send_to_siem(audit_entry) if siem_configured?\n        end\n\n        def self.detect_anomalous_activity\n          # Analyze recent file operations for suspicious patterns\n\n          # Check for unusual upload volumes\n          recent_uploads = count_recent_uploads(1.hour)\n          if recent_uploads &gt; 100 # Threshold\n            create_security_alert('high_upload_volume', {\n              count: recent_uploads,\n              threshold: 100,\n              time_window: '1 hour'\n            })\n          end\n\n          # Check for failed security scans\n          failed_scans = count_failed_security_scans(24.hours)\n          if failed_scans &gt; 5\n            create_security_alert('multiple_security_failures', {\n              count: failed_scans,\n              time_window: '24 hours'\n            })\n          end\n\n          # Check for unusual access patterns\n          detect_unusual_access_patterns\n        end\n\n        private\n\n        def self.write_security_log(entry)\n          # Write to dedicated security log\n          security_logger = Logger.new(\n            ENV['SECURITY_LOG_PATH'] || 'log/security.log',\n            formatter: proc { |severity, datetime, progname, msg|\n              \"#{datetime.iso8601} #{severity} #{msg}\\n\"\n            }\n          )\n\n          security_logger.info(entry.to_json)\n        end\n\n        def self.create_security_alert(alert_type, details)\n          alert = {\n            id: SecureRandom.uuid,\n            type: alert_type,\n            severity: determine_alert_severity(alert_type),\n            details: details,\n            created_at: Time.current.iso8601,\n            investigated: false\n          }\n\n          # Store alert\n          store_security_alert(alert)\n\n          # Notify security team\n          notify_security_team(alert)\n        end\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"user-guide/file-uploads/#summary","title":"Summary","text":"<p>Ragdoll's file upload system provides enterprise-grade file handling through Shrine's flexible architecture. The system offers:</p> <p>Core Capabilities: - Multi-Modal Support: Specialized handling for documents, images, and audio files - Flexible Storage: Support for filesystem, S3, Google Cloud, and Azure backends - Comprehensive Validation: Multi-layer security with MIME type verification and malware scanning - Processing Pipeline: Automated content extraction and AI-powered analysis - Security First: Encryption at rest, access control, and audit logging</p> <p>Supported File Types: - Documents: PDF, DOCX, TXT, HTML, Markdown, JSON, RTF - Images: JPEG, PNG, GIF, WebP, BMP, TIFF, SVG with AI description generation - Audio: MP3, WAV, M4A, FLAC, OGG with automatic transcription</p> <p>Security Features: - File signature validation and MIME type verification - Integrated malware scanning with ClamAV and custom patterns - Role-based access control with secure URL generation - Application-level encryption for sensitive content - Comprehensive audit logging and anomaly detection</p> <p>Production Ready: - CDN integration for optimized delivery - Hybrid storage strategies for cost optimization - Automatic backup and replication - Error recovery and processing monitoring - Horizontal scaling with load balancing</p> <p>The system seamlessly integrates with Ragdoll's document processing pipeline, ensuring secure, efficient file handling from upload through AI-powered content analysis and embedding generation.</p> <p>This document is part of the Ragdoll documentation suite. For immediate help, see the Quick Start Guide or API Reference.</p>"},{"location":"user-guide/multi-modal/","title":"Multi-Modal Architecture","text":"<p>Ragdoll's multi-modal architecture is one of its most sophisticated features, designed from the ground up to handle text, image, and audio content as first-class citizens through a polymorphic database design.</p>"},{"location":"user-guide/multi-modal/#overview","title":"Overview","text":"<p>Unlike most RAG systems that retrofit multi-modal support, Ragdoll implements a native multi-modal architecture where different content types are unified through polymorphic associations while maintaining specialized handling for each media type.</p>"},{"location":"user-guide/multi-modal/#architecture-design","title":"Architecture Design","text":""},{"location":"user-guide/multi-modal/#schema-optimization","title":"Schema Optimization","text":"<p>Ragdoll uses an optimized polymorphic schema that eliminates field duplication while maintaining full functionality:</p> <ul> <li>Embedding model information is stored in the content-specific tables (text_contents, image_contents, audio_contents)</li> <li>Embeddings table contains only embedding-specific data (vector, content chunk, metadata)</li> <li>Polymorphic relationships provide seamless access to embedding model information</li> <li>Zero data duplication across the schema while preserving all functionality</li> </ul> <p>This design provides better data normalization, reduced storage requirements, and maintains referential integrity across all content types.</p>"},{"location":"user-guide/multi-modal/#polymorphic-content-system","title":"Polymorphic Content System","text":"<pre><code># Unified embedding storage across all content types\nEmbedding.where(embeddable_type: 'TextContent')\nEmbedding.where(embeddable_type: 'ImageContent')\nEmbedding.where(embeddable_type: 'AudioContent')\n\n# Access embedding model through polymorphic relationship\nembedding = Embedding.find(123)\nembedding.embedding_model  # Returns content-specific embedding model\n# e.g., 'text-embedding-3-small', 'clip-vit-large-patch14', 'whisper-embedding-v1'\n\n# Cross-modal search\nresults = SearchEngine.search_similar_content(\"machine learning diagram\")\n# Can return text, image, and audio results in a single query\n</code></pre>"},{"location":"user-guide/multi-modal/#database-schema","title":"Database Schema","text":"<pre><code>-- Central document entity\nragdoll_documents\n\u251c\u2500\u2500 file_data (Shrine attachment)\n\u251c\u2500\u2500 metadata (LLM-generated content analysis)\n\u251c\u2500\u2500 file_metadata (system file properties)\n\u2514\u2500\u2500 document_type (text/image/audio/pdf/mixed)\n\n-- Content-specific tables (each stores embedding_model)\nragdoll_text_contents\n\u251c\u2500\u2500 document_id (foreign key)\n\u251c\u2500\u2500 content (extracted text)\n\u251c\u2500\u2500 chunk_size, chunk_overlap (processing parameters)\n\u251c\u2500\u2500 embedding_model (model used for this content type)\n\u2514\u2500\u2500 language_detected\n\nragdoll_image_contents\n\u251c\u2500\u2500 document_id (foreign key)\n\u251c\u2500\u2500 file_data (Shrine image attachment)\n\u251c\u2500\u2500 description (AI-generated description)\n\u251c\u2500\u2500 embedding_model (model used for this content type)\n\u251c\u2500\u2500 width, height (image dimensions)\n\u2514\u2500\u2500 alt_text\n\nragdoll_audio_contents\n\u251c\u2500\u2500 document_id (foreign key)\n\u251c\u2500\u2500 file_data (Shrine audio attachment)\n\u251c\u2500\u2500 transcript (speech-to-text result)\n\u251c\u2500\u2500 embedding_model (model used for this content type)\n\u251c\u2500\u2500 duration_seconds\n\u2514\u2500\u2500 language_detected\n\n-- Polymorphic embeddings (normalized schema - no duplicated fields)\nragdoll_embeddings\n\u251c\u2500\u2500 embeddable_type (TextContent/ImageContent/AudioContent)\n\u251c\u2500\u2500 embeddable_id (references content table)\n\u251c\u2500\u2500 embedding_vector (pgvector)\n\u251c\u2500\u2500 content (original text/description/transcript chunk)\n\u251c\u2500\u2500 chunk_index (position within embeddable content)\n\u2514\u2500\u2500 metadata (embedding-specific data only)\n</code></pre>"},{"location":"user-guide/multi-modal/#content-type-support","title":"Content Type Support","text":""},{"location":"user-guide/multi-modal/#text-content","title":"Text Content","text":"<p>Fully Implemented with comprehensive processing:</p> <pre><code># Supported formats\ntext_content = TextContent.create!(\n  document: document,\n  content: extracted_text,\n  chunk_size: 1000,\n  chunk_overlap: 200,\n  embedding_model: 'text-embedding-3-small',\n  language_detected: 'en'\n)\n\n# Automatic chunking and embedding generation\nRagdoll::GenerateEmbeddingsJob.perform_later(text_content)\n</code></pre> <p>Features: - \u2705 Intelligent text chunking with boundary detection - \u2705 Language detection and encoding handling - \u2705 Configurable chunk size and overlap - \u2705 Sentence/paragraph boundary preservation - \u2705 Code-aware chunking for technical documents</p>"},{"location":"user-guide/multi-modal/#image-content","title":"Image Content","text":"<p>Fully Implemented with AI-powered analysis:</p> <pre><code># Image processing with Shrine\nimage_content = ImageContent.create!(\n  document: document,\n  file: uploaded_file,  # Shrine attachment\n  description: ai_generated_description,\n  embedding_model: 'clip-vit-large-patch14',\n  width: 1920,\n  height: 1080,\n  alt_text: \"Machine learning workflow diagram\"\n)\n\n# AI description generation\nMetadataGeneratorService.generate_image_description(image_content)\n</code></pre> <p>Features: - \u2705 Shrine file attachment with validation - \u2705 AI-powered image description generation - \u2705 Metadata extraction (dimensions, format, size) - \u2705 Embedding generation from descriptions - \u2705 Cross-modal search (find images from text queries)</p>"},{"location":"user-guide/multi-modal/#audio-content","title":"Audio Content","text":"<p>Fully Implemented with transcription support:</p> <pre><code># Audio processing with transcription\naudio_content = AudioContent.create!(\n  document: document,\n  file: uploaded_file,  # Shrine attachment\n  transcript: speech_to_text_result,\n  embedding_model: 'whisper-embedding-v1',\n  duration_seconds: 125.5,\n  language_detected: 'en'\n)\n\n# Transcription and embedding\nRagdoll::ExtractTextJob.perform_later(audio_content)\nRagdoll::GenerateEmbeddingsJob.perform_later(audio_content)\n</code></pre> <p>Features: - \u2705 Shrine file attachment with audio validation - \u2705 Speech-to-text transcription integration - \u2705 Duration and metadata extraction - \u2705 Language detection - \u2705 Embedding generation from transcripts - \u2705 Searchable by spoken content</p>"},{"location":"user-guide/multi-modal/#cross-modal-search","title":"Cross-Modal Search","text":"<p>The unified embedding system enables sophisticated cross-modal search capabilities:</p>"},{"location":"user-guide/multi-modal/#search-across-all-content-types","title":"Search Across All Content Types","text":"<pre><code># Single query searches text, image descriptions, and audio transcripts\nresults = Ragdoll::Core.search(\n  query: \"neural network architecture\",\n  content_types: ['text', 'image', 'audio']  # optional filter\n)\n\n# Results include:\n# - Text documents mentioning neural networks\n# - Images with AI-generated descriptions about neural networks\n# - Audio files with transcripts discussing neural networks\n</code></pre>"},{"location":"user-guide/multi-modal/#content-type-specific-search","title":"Content-Type Specific Search","text":"<pre><code># Search only images\nimage_results = Ragdoll::Core.search(\n  query: \"machine learning diagram\",\n  content_type: 'image'\n)\n\n# Search only audio transcripts\naudio_results = Ragdoll::Core.search(\n  query: \"podcast about AI\",\n  content_type: 'audio'\n)\n</code></pre>"},{"location":"user-guide/multi-modal/#advanced-multi-modal-queries","title":"Advanced Multi-Modal Queries","text":"<pre><code># Complex cross-modal search with metadata filters\nresults = Ragdoll::Core.search(\n  query: \"deep learning\",\n  content_types: ['text', 'image'],\n  metadata_filters: {\n    classification: 'technical',\n    topics: ['artificial intelligence']\n  },\n  similarity_threshold: 0.8\n)\n</code></pre>"},{"location":"user-guide/multi-modal/#file-upload-and-processing","title":"File Upload and Processing","text":""},{"location":"user-guide/multi-modal/#shrine-integration","title":"Shrine Integration","text":"<p>Each content type uses Shrine for production-grade file handling:</p> <pre><code># Configuration in shrine_config.rb\nShrine.configure do |config|\n  config.storages = {\n    cache: Shrine::Storage::FileSystem.new(\"tmp\", prefix: \"uploads/cache\"),\n    store: Shrine::Storage::FileSystem.new(\"storage\", prefix: \"uploads\")\n  }\nend\n\n# Automatic file validation by content type\nclass ImageUploader &lt; Shrine\n  plugin :validation_helpers\n\n  Attacher.validate do\n    validate_max_size 10.megabytes\n    validate_mime_type %w[image/jpeg image/png image/gif image/webp]\n  end\nend\n</code></pre>"},{"location":"user-guide/multi-modal/#processing-pipeline","title":"Processing Pipeline","text":"<pre><code># Multi-modal document processing\ndocument = Document.create!(\n  location: file_path,\n  document_type: 'mixed'  # Can contain multiple content types\n)\n\n# Automatic content type detection and processing\nDocumentProcessor.process(document) do |processor|\n  case processor.detected_type\n  when 'image'\n    processor.create_image_content_with_description\n  when 'audio'\n    processor.create_audio_content_with_transcription\n  when 'text'\n    processor.create_text_content_with_chunking\n  end\nend\n</code></pre>"},{"location":"user-guide/multi-modal/#background-processing","title":"Background Processing","text":"<p>All multi-modal operations are designed for background processing:</p> <pre><code># Jobs for each content type\nRagdoll::GenerateEmbeddingsJob.perform_later(text_content)\nRagdoll::GenerateEmbeddingsJob.perform_later(image_content)  # From description\nRagdoll::GenerateEmbeddingsJob.perform_later(audio_content)  # From transcript\n\n# Content analysis jobs\nRagdoll::ExtractTextJob.perform_later(audio_content)         # Speech-to-text\nRagdoll::GenerateSummaryJob.perform_later(text_content)      # Summarization\nRagdoll::ExtractKeywordsJob.perform_later(image_content)     # Image analysis\n</code></pre>"},{"location":"user-guide/multi-modal/#usage-analytics","title":"Usage Analytics","text":"<p>Multi-modal search includes sophisticated analytics:</p> <pre><code># Usage tracking across content types\nembedding.update!(\n  usage_count: embedding.usage_count + 1,\n  returned_at: Time.current\n)\n\n# Analytics by content type\nDocument.joins(:embeddings)\n        .where(embeddings: { embeddable_type: 'ImageContent' })\n        .group(:document_type)\n        .count\n\n# Cross-modal performance metrics\nSearchEngine.analytics_for_query(\"machine learning\")\n# Returns usage stats across text, image, and audio results\n</code></pre>"},{"location":"user-guide/multi-modal/#api-examples","title":"API Examples","text":""},{"location":"user-guide/multi-modal/#adding-multi-modal-content","title":"Adding Multi-Modal Content","text":"<pre><code># Mixed document with multiple content types\nresult = Ragdoll::Core.add_document(path: 'presentation.pptx')\n# Automatically extracts:\n# - Text from slides \u2192 TextContent\n# - Images from slides \u2192 ImageContent\n# - Speaker notes \u2192 TextContent\n\n# Manual content addition\ndoc_id = Ragdoll::Core.add_document(path: 'research_paper.pdf')[:document_id]\n\n# Add supplementary image\nRagdoll::Core.add_image(\n  document_id: doc_id,\n  image_path: 'diagram.png',\n  description: 'Neural network architecture diagram'\n)\n\n# Add supplementary audio\nRagdoll::Core.add_audio(\n  document_id: doc_id,\n  audio_path: 'presentation.mp3',\n  transcript: 'Today we discuss neural network architectures...'\n)\n</code></pre>"},{"location":"user-guide/multi-modal/#searching-multi-modal-content","title":"Searching Multi-Modal Content","text":"<pre><code># Unified search across all content\nresults = Ragdoll::Core.search(query: 'convolutional neural networks')\n\nresults.each do |result|\n  case result[:content_type]\n  when 'text'\n    puts \"Text: #{result[:content]}\"\n  when 'image'\n    puts \"Image: #{result[:description]} (#{result[:file_url]})\"\n  when 'audio'\n    puts \"Audio: #{result[:transcript]} (#{result[:duration]}s)\"\n  end\nend\n</code></pre>"},{"location":"user-guide/multi-modal/#performance-considerations","title":"Performance Considerations","text":""},{"location":"user-guide/multi-modal/#vector-storage-optimization","title":"Vector Storage Optimization","text":"<pre><code>-- Polymorphic embeddings with optimized indexing\nCREATE INDEX idx_embeddings_polymorphic_search\nON ragdoll_embeddings (embeddable_type, embedding_vector)\nUSING ivfflat (embedding_vector vector_cosine_ops);\n\n-- Content-type specific indexes\nCREATE INDEX idx_embeddings_text_usage\nON ragdoll_embeddings (embeddable_type, usage_count DESC)\nWHERE embeddable_type = 'TextContent';\n</code></pre>"},{"location":"user-guide/multi-modal/#batch-processing","title":"Batch Processing","text":"<pre><code># Efficient batch embedding generation\nTextContent.where(embeddings_count: 0)\n           .find_in_batches(batch_size: 100) do |batch|\n  Ragdoll::GenerateEmbeddingsJob.perform_later(batch.map(&amp;:id))\nend\n\n# Cross-modal batch search\nqueries = ['AI research', 'neural networks', 'deep learning']\nresults = SearchEngine.batch_search(queries, content_types: ['text', 'image'])\n</code></pre>"},{"location":"user-guide/multi-modal/#extending-multi-modal-support","title":"Extending Multi-Modal Support","text":""},{"location":"user-guide/multi-modal/#adding-new-content-types","title":"Adding New Content Types","text":"<pre><code># 1. Create new content model\nclass VideoContent &lt; ApplicationRecord\n  belongs_to :document\n  has_many :embeddings, as: :embeddable\n\n  # Shrine attachment for video files\n  include VideoUploader::Attachment(:file)\nend\n\n# 2. Add processing logic\nclass DocumentProcessor\n  def process_video(video_file)\n    # Extract frames, transcribe audio, analyze content\n    VideoContent.create!(\n      document: @document,\n      file: video_file,\n      transcript: extract_audio_transcript(video_file),\n      frame_descriptions: extract_key_frames(video_file),\n      duration_seconds: get_video_duration(video_file)\n    )\n  end\nend\n\n# 3. Add embedding generation\nclass Ragdoll::GenerateEmbeddingsJob\n  def perform_for_video(video_content)\n    # Generate embeddings from transcript + frame descriptions\n    combined_text = \"#{video_content.transcript} #{video_content.frame_descriptions.join(' ')}\"\n    vector = EmbeddingService.generate_embedding(combined_text)\n\n    video_content.embeddings.create!(\n      embedding_vector: vector,\n      content: combined_text,\n      embedding_model: current_model\n    )\n  end\nend\n</code></pre>"},{"location":"user-guide/multi-modal/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/multi-modal/#1-content-type-strategy","title":"1. Content Type Strategy","text":"<ul> <li>Use appropriate content types for your data</li> <li>Consider mixed documents for complex files (PDFs, presentations)</li> <li>Leverage cross-modal search for comprehensive results</li> </ul>"},{"location":"user-guide/multi-modal/#2-performance-optimization","title":"2. Performance Optimization","text":"<ul> <li>Process large files in background jobs</li> <li>Use batch operations for multiple files</li> <li>Implement appropriate caching for frequently accessed content</li> </ul>"},{"location":"user-guide/multi-modal/#3-search-strategy","title":"3. Search Strategy","text":"<ul> <li>Start with broad cross-modal searches</li> <li>Use content-type filters to narrow results</li> <li>Combine with metadata filters for precision</li> </ul>"},{"location":"user-guide/multi-modal/#4-storage-management","title":"4. Storage Management","text":"<ul> <li>Configure appropriate Shrine storage backends</li> <li>Implement file validation and size limits</li> <li>Plan for storage scaling in production</li> </ul> <p>The multi-modal architecture in Ragdoll provides a powerful foundation for building sophisticated document intelligence applications that can understand and search across different types of content seamlessly.</p>"},{"location":"user-guide/search-analytics/","title":"Search &amp; Analytics","text":"<p>Ragdoll implements a sophisticated search and analytics system that goes far beyond basic semantic similarity. The system combines vector search with usage analytics, full-text search capabilities, and intelligent ranking algorithms to provide enterprise-grade search functionality.</p>"},{"location":"user-guide/search-analytics/#overview","title":"Overview","text":"<p>The search system consists of several integrated components:</p> <ul> <li>Semantic Search: pgvector-powered similarity search with cosine distance</li> <li>Usage Analytics: Frequency and recency-based ranking algorithms</li> <li>Full-Text Search: PostgreSQL GIN indexes with tsquery support</li> <li>Cross-Modal Search: Unified search across text, image, and audio content</li> <li>Hybrid Search: Combining semantic and full-text search with keyword extraction</li> <li>Keyword Extraction: Intelligent keyword extraction for enhanced search relevance</li> <li>Smart Ranking: Usage patterns, similarity scores, and metadata weighting</li> </ul>"},{"location":"user-guide/search-analytics/#architecture","title":"Architecture","text":""},{"location":"user-guide/search-analytics/#searchengine-core","title":"SearchEngine Core","text":"<pre><code>class SearchEngine\n  def search_similar_content(query, options = {})\n    # 1. Generate query embedding\n    query_vector = EmbeddingService.generate_embedding(query)\n\n    # 2. Perform vector similarity search\n    similar_embeddings = find_similar_embeddings(\n      query_vector, \n      options[:similarity_threshold] || Configuration.search_similarity_threshold,\n      options[:limit] || Configuration.max_search_results\n    )\n\n    # 3. Apply usage-based ranking\n    ranked_results = apply_usage_ranking(similar_embeddings, options)\n\n    # 4. Enhance with metadata and formatting\n    format_search_results(ranked_results, options)\n  end\nend\n</code></pre>"},{"location":"user-guide/search-analytics/#database-optimization","title":"Database Optimization","text":"<pre><code>-- Optimized pgvector indexes for fast similarity search\nCREATE INDEX idx_embeddings_vector_cosine \nON ragdoll_embeddings \nUSING ivfflat (embedding_vector vector_cosine_ops) \nWITH (lists = 100);\n\n-- Usage analytics indexes\nCREATE INDEX idx_embeddings_usage_analytics \nON ragdoll_embeddings (usage_count DESC, returned_at DESC);\n\n-- Polymorphic content type indexes\nCREATE INDEX idx_embeddings_content_type \nON ragdoll_embeddings (embeddable_type, embeddable_id);\n\n-- Full-text search indexes\nCREATE INDEX idx_embeddings_content_fulltext \nON ragdoll_embeddings \nUSING GIN (to_tsvector('english', content));\n</code></pre>"},{"location":"user-guide/search-analytics/#semantic-search","title":"Semantic Search","text":""},{"location":"user-guide/search-analytics/#vector-similarity-search","title":"Vector Similarity Search","text":"<pre><code># Core similarity search implementation\ndef find_similar_embeddings(query_vector, threshold, limit)\n  Embedding.select(\n    \"*, \" \\\n    \"embedding_vector &lt;=&gt; '#{query_vector}' as distance, \" \\\n    \"1 - (embedding_vector &lt;=&gt; '#{query_vector}') as similarity\"\n  )\n  .where(\"1 - (embedding_vector &lt;=&gt; '#{query_vector}') &gt;= ?\", threshold)\n  .order(\"embedding_vector &lt;=&gt; '#{query_vector}'\")\n  .limit(limit)\nend\n\n# Usage example\nresults = SearchEngine.search_similar_content(\n  \"machine learning algorithms\",\n  similarity_threshold: 0.8,\n  limit: 20\n)\n</code></pre>"},{"location":"user-guide/search-analytics/#multi-modal-search","title":"Multi-Modal Search","text":"<pre><code># Search across all content types\ndef cross_modal_search(query, content_types: nil)\n  base_query = Embedding.joins(:embeddable)\n\n  if content_types.present?\n    base_query = base_query.where(embeddable_type: content_types.map(&amp;:classify))\n  end\n\n  # Generate embeddings and search\n  query_vector = EmbeddingService.generate_embedding(query)\n\n  base_query.select(\n    \"ragdoll_embeddings.*, \" \\\n    \"embedding_vector &lt;=&gt; '#{query_vector}' as distance, \" \\\n    \"embeddable_type as content_type\"\n  )\n  .where(\"1 - (embedding_vector &lt;=&gt; '#{query_vector}') &gt;= ?\", similarity_threshold)\n  .order(\"embedding_vector &lt;=&gt; '#{query_vector}'\")\nend\n\n# Search specific content types\ntext_results = SearchEngine.search_similar_content(\n  \"neural networks\",\n  content_types: ['text']\n)\n\nimage_results = SearchEngine.search_similar_content(\n  \"architecture diagram\", \n  content_types: ['image']\n)\n\n# Combined cross-modal search\nall_results = SearchEngine.search_similar_content(\n  \"deep learning concepts\",\n  content_types: ['text', 'image', 'audio']\n)\n</code></pre>"},{"location":"user-guide/search-analytics/#hybrid-search","title":"Hybrid Search","text":"<p>Ragdoll's hybrid search combines semantic vector search with keyword-based full-text search to provide more comprehensive and relevant results.</p>"},{"location":"user-guide/search-analytics/#architecture_1","title":"Architecture","text":"<pre><code># High-level hybrid search interface\nresults = Ragdoll::Core.hybrid_search(\n  query: \"neural networks and deep learning\",\n  semantic_weight: 0.7,    # Weight for vector similarity (0.0 - 1.0)\n  text_weight: 0.3,        # Weight for full-text search (0.0 - 1.0)\n  limit: 20\n)\n</code></pre>"},{"location":"user-guide/search-analytics/#how-hybrid-search-works","title":"How Hybrid Search Works","text":"<ol> <li>Query Processing: The input query is processed through both semantic and lexical pipelines</li> <li>Keyword Extraction: Keywords are extracted from the query using <code>extract_keywords</code></li> <li>Semantic Search: Vector embeddings are generated and similarity search is performed</li> <li>Full-Text Search: PostgreSQL full-text search using GIN indexes</li> <li>Result Combination: Results are merged with configurable weighting</li> <li>Ranking: Combined scores determine final result ordering</li> </ol>"},{"location":"user-guide/search-analytics/#implementation-details","title":"Implementation Details","text":"<pre><code>class Document &lt; ActiveRecord::Base\n  # Extract keywords from query string (words &gt; 4 characters)\n  def self.extract_keywords(query:)\n    return [] if query.nil? || query.strip.empty?\n\n    query.split(/\\s+/)\n         .map(&amp;:strip)\n         .reject(&amp;:empty?)\n         .select { |word| word.length &gt; 4 }\n  end\n\n  # Hybrid search combining semantic and full-text approaches\n  def self.hybrid_search(query, query_embedding: nil, **options)\n    limit = options[:limit] || 20\n    semantic_weight = options[:semantic_weight] || 0.7\n    text_weight = options[:text_weight] || 0.3\n\n    results = []\n\n    # Get semantic search results if embedding provided\n    if query_embedding\n      semantic_results = embeddings_search(query_embedding, limit: limit)\n      results.concat(semantic_results.map do |result|\n        result.merge(\n          search_type: 'semantic',\n          weighted_score: result[:combined_score] * semantic_weight\n        )\n      end)\n    end\n\n    # Get PostgreSQL full-text search results\n    text_results = search_content(query, limit: limit)\n    text_results.each_with_index do |doc, index|\n      score = (limit - index).to_f / limit * text_weight\n      results &lt;&lt; {\n        document_id: doc.id.to_s,\n        document_title: doc.title,\n        content: doc.content[0..500],\n        search_type: 'full_text',\n        weighted_score: score,\n        document: doc\n      }\n    end\n\n    # Combine and deduplicate by document_id\n    combined = results.group_by { |r| r[:document_id] }\n                      .map do |_doc_id, doc_results|\n      best_result = doc_results.max_by { |r| r[:weighted_score] }\n      total_score = doc_results.sum { |r| r[:weighted_score] }\n      search_types = doc_results.map { |r| r[:search_type] }.uniq\n\n      best_result.merge(\n        combined_score: total_score,\n        search_types: search_types\n      )\n    end\n\n    combined.sort_by { |r| -r[:combined_score] }.take(limit)\n  end\nend\n</code></pre>"},{"location":"user-guide/search-analytics/#usage-examples","title":"Usage Examples","text":"<pre><code># Basic hybrid search\nresults = client.hybrid_search(\n  query: \"machine learning algorithms for text processing\"\n)\n\n# Emphasize semantic similarity\nresults = client.hybrid_search(\n  query: \"neural network architectures\",\n  semantic_weight: 0.8,\n  text_weight: 0.2\n)\n\n# Emphasize keyword matching\nresults = client.hybrid_search(\n  query: \"specific technical term\",\n  semantic_weight: 0.3,\n  text_weight: 0.7\n)\n\n# Large result set for comprehensive analysis\nresults = client.hybrid_search(\n  query: \"artificial intelligence research\",\n  limit: 50,\n  semantic_weight: 0.6,\n  text_weight: 0.4\n)\n</code></pre>"},{"location":"user-guide/search-analytics/#keyword-extraction","title":"Keyword Extraction","text":"<p>The <code>extract_keywords</code> method intelligently identifies significant terms from search queries:</p> <pre><code># Extract meaningful keywords from queries\nkeywords = Ragdoll::Document.extract_keywords(\n  query: \"machine learning algorithms neural networks\"\n)\n# Returns: [\"machine\", \"learning\", \"algorithms\", \"neural\", \"networks\"]\n\n# Handles punctuation and formatting\nkeywords = Ragdoll::Document.extract_keywords(\n  query: \"deep-learning, artificial-intelligence!\"\n)  \n# Returns: [\"deep-learning,\", \"artificial-intelligence!\"]\n\n# Filters short words automatically\nkeywords = Ragdoll::Document.extract_keywords(\n  query: \"AI and ML for big data analysis\"\n)\n# Returns: [\"analysis\"] # Only words &gt; 4 characters\n</code></pre>"},{"location":"user-guide/search-analytics/#best-practices","title":"Best Practices","text":"<p>Query Optimization: - Use descriptive queries with both specific terms and conceptual language - For technical searches, use higher text weights to prioritize exact matches - For conceptual searches, use higher semantic weights for broader relevance</p> <p>Weight Configuration: - Semantic-Heavy (0.8/0.2): Conceptual searches, exploratory research - Balanced (0.6/0.4): General purpose searches, mixed content - Text-Heavy (0.3/0.7): Technical documentation, specific term searches</p> <p>Performance Considerations: - Hybrid search is more computationally expensive than single-mode search - Consider caching frequently used query patterns - Use appropriate limits to balance comprehensiveness with performance <pre><code>## Usage Analytics\n\n### Analytics Data Collection\n\n```ruby\nclass Embedding &lt; ApplicationRecord\n  # Automatic usage tracking\n  after_find :track_usage, if: :search_context?\n\n  def track_usage\n    increment!(:usage_count)\n    update_column(:returned_at, Time.current)\n\n    # Record detailed analytics\n    SearchAnalytics.record_access(\n      embedding: self,\n      query: current_search_query,\n      user_id: current_user_id,\n      session_id: current_session_id,\n      timestamp: Time.current\n    )\n  end\n\n  # Usage-based scoring\n  def usage_score\n    recency_score = calculate_recency_score\n    frequency_score = calculate_frequency_score\n\n    (recency_score * 0.3) + (frequency_score * 0.7)\n  end\n\n  private\n\n  def calculate_recency_score\n    return 0 unless returned_at\n\n    days_since = (Time.current - returned_at) / 1.day\n    Math.exp(-days_since / 30.0)  # Exponential decay over 30 days\n  end\n\n  def calculate_frequency_score\n    return 0 if usage_count.zero?\n\n    # Logarithmic scaling for usage frequency\n    Math.log(usage_count + 1) / Math.log(100)  # Max score at 100 uses\n  end\nend\n</code></pre></p>"},{"location":"user-guide/search-analytics/#smart-ranking-algorithm","title":"Smart Ranking Algorithm","text":"<pre><code>class SearchRanker\n  def rank_results(embeddings, options = {})\n    embeddings.map do |embedding|\n      {\n        embedding: embedding,\n        similarity_score: embedding.similarity,\n        usage_score: embedding.usage_score,\n        metadata_score: calculate_metadata_score(embedding, options),\n        composite_score: calculate_composite_score(embedding, options)\n      }\n    end.sort_by { |result| -result[:composite_score] }\n  end\n\n  private\n\n  def calculate_composite_score(embedding, options)\n    weights = options[:ranking_weights] || default_weights\n\n    similarity_weight = weights[:similarity] || 0.6\n    usage_weight = weights[:usage] || 0.3\n    metadata_weight = weights[:metadata] || 0.1\n\n    (embedding.similarity * similarity_weight) +\n    (embedding.usage_score * usage_weight) +\n    (calculate_metadata_score(embedding, options) * metadata_weight)\n  end\n\n  def calculate_metadata_score(embedding, options)\n    document = embedding.embeddable.document\n    score = 0.0\n\n    # Boost recent documents\n    if document.created_at &gt; 30.days.ago\n      score += 0.2\n    end\n\n    # Boost documents with rich metadata\n    if document.metadata&amp;.key?('classification')\n      score += 0.1\n    end\n\n    # Classification matching\n    if options[:preferred_classification] &amp;&amp; \n       document.metadata&amp;.dig('classification') == options[:preferred_classification]\n      score += 0.3\n    end\n\n    # Topic relevance scoring\n    if options[:topic_filters].present? &amp;&amp; document.metadata&amp;.key?('topics')\n      overlap = (options[:topic_filters] &amp; document.metadata['topics']).size\n      score += (overlap.to_f / options[:topic_filters].size) * 0.4\n    end\n\n    [score, 1.0].min  # Cap at 1.0\n  end\nend\n</code></pre>"},{"location":"user-guide/search-analytics/#full-text-search-integration","title":"Full-Text Search Integration","text":""},{"location":"user-guide/search-analytics/#postgresql-full-text-search","title":"PostgreSQL Full-Text Search","text":"<pre><code>class HybridSearchEngine\n  def hybrid_search(query, semantic_weight: 0.7, text_weight: 0.3, options: {})\n    # 1. Semantic search results\n    semantic_results = semantic_search(query, options)\n\n    # 2. Full-text search results\n    fulltext_results = fulltext_search(query, options)\n\n    # 3. Combine and weight results\n    combine_search_results(\n      semantic_results, \n      fulltext_results,\n      semantic_weight: semantic_weight,\n      text_weight: text_weight\n    )\n  end\n\n  private\n\n  def fulltext_search(query, options)\n    # PostgreSQL full-text search with ranking\n    Embedding.select(\n      \"*, \" \\\n      \"ts_rank(to_tsvector('english', content), plainto_tsquery('english', ?)) as text_rank\"\n    )\n    .where(\"to_tsvector('english', content) @@ plainto_tsquery('english', ?)\", query)\n    .where(\"ts_rank(to_tsvector('english', content), plainto_tsquery('english', ?)) &gt; 0.1\", query)\n    .order(\"text_rank DESC\")\n    .limit(options[:limit] || 50)\n  end\n\n  def combine_search_results(semantic_results, fulltext_results, weights)\n    all_results = {}\n\n    # Add semantic results with weighting\n    semantic_results.each do |result|\n      all_results[result.id] = {\n        embedding: result,\n        semantic_score: result.similarity * weights[:semantic_weight],\n        text_score: 0,\n        composite_score: result.similarity * weights[:semantic_weight]\n      }\n    end\n\n    # Add/enhance with full-text results\n    fulltext_results.each do |result|\n      if all_results[result.id]\n        # Enhance existing result\n        all_results[result.id][:text_score] = result.text_rank * weights[:text_weight]\n        all_results[result.id][:composite_score] += result.text_rank * weights[:text_weight]\n      else\n        # Add new result\n        all_results[result.id] = {\n          embedding: result,\n          semantic_score: 0,\n          text_score: result.text_rank * weights[:text_weight],\n          composite_score: result.text_rank * weights[:text_weight]\n        }\n      end\n    end\n\n    # Sort by composite score and apply usage ranking\n    ranked_results = all_results.values\n                               .sort_by { |r| -r[:composite_score] }\n                               .first(weights[:limit] || 20)\n\n    apply_usage_ranking(ranked_results)\n  end\nend\n</code></pre>"},{"location":"user-guide/search-analytics/#advanced-search-features","title":"Advanced Search Features","text":""},{"location":"user-guide/search-analytics/#faceted-search","title":"Faceted Search","text":"<pre><code>class FacetedSearch\n  def search_with_facets(query, facets: {})\n    base_results = SearchEngine.search_similar_content(query)\n\n    # Apply facet filters\n    filtered_results = apply_facet_filters(base_results, facets)\n\n    # Calculate facet counts for UI\n    facet_counts = calculate_facet_counts(base_results, facets.keys)\n\n    {\n      results: filtered_results,\n      facets: facet_counts,\n      total_count: filtered_results.size\n    }\n  end\n\n  private\n\n  def apply_facet_filters(results, facets)\n    filtered = results.includes(embeddable: :document)\n\n    facets.each do |facet_type, values|\n      case facet_type\n      when :content_type\n        filtered = filtered.where(embeddable_type: values.map(&amp;:classify))\n      when :classification\n        filtered = filtered.joins(embeddable: :document)\n                          .where(\"ragdoll_documents.metadata -&gt;&gt; 'classification' IN (?)\", values)\n      when :topics\n        filtered = filtered.joins(embeddable: :document)\n                          .where(\"ragdoll_documents.metadata -&gt; 'topics' ?| array[?]\", values)\n      when :date_range\n        start_date, end_date = values\n        filtered = filtered.joins(embeddable: :document)\n                          .where(ragdoll_documents: { created_at: start_date..end_date })\n      end\n    end\n\n    filtered\n  end\nend\n\n# Usage example\nresults = FacetedSearch.search_with_facets(\n  \"machine learning\",\n  facets: {\n    content_type: ['text', 'image'],\n    classification: ['research', 'technical'],\n    topics: ['artificial intelligence', 'neural networks'],\n    date_range: [1.year.ago, Time.current]\n  }\n)\n</code></pre>"},{"location":"user-guide/search-analytics/#search-suggestions-and-autocomplete","title":"Search Suggestions and Autocomplete","text":"<pre><code>class SearchSuggestions\n  def suggest_queries(partial_query, limit: 10)\n    # 1. Historical query suggestions\n    historical = suggest_from_history(partial_query, limit: limit / 2)\n\n    # 2. Content-based suggestions  \n    content_based = suggest_from_content(partial_query, limit: limit / 2)\n\n    # 3. Combine and rank suggestions\n    combine_suggestions(historical, content_based).first(limit)\n  end\n\n  private\n\n  def suggest_from_history(partial_query, limit:)\n    SearchAnalytics.where(\"query ILIKE ?\", \"#{partial_query}%\")\n                   .group(:query)\n                   .order(\"COUNT(*) DESC\")\n                   .limit(limit)\n                   .pluck(:query)\n  end\n\n  def suggest_from_content(partial_query, limit:)\n    # Extract common phrases from content\n    Embedding.where(\"content ILIKE ?\", \"%#{partial_query}%\")\n             .select(\"regexp_split_to_table(content, E'\\\\\\\\s+') as word\")\n             .where(\"word ILIKE ?\", \"#{partial_query}%\")\n             .group(\"word\")\n             .order(\"COUNT(*) DESC\")\n             .limit(limit)\n             .pluck(\"word\")\n  end\nend\n</code></pre>"},{"location":"user-guide/search-analytics/#search-tracking-system","title":"Search Tracking System","text":"<p>Ragdoll includes a comprehensive search tracking system that automatically records all searches performed through the platform. This provides valuable insights into user behavior, search patterns, and system performance.</p>"},{"location":"user-guide/search-analytics/#database-schema","title":"Database Schema","text":"<p>The search tracking system uses two main tables:</p> <ol> <li><code>ragdoll_searches</code> - Stores search queries and metadata</li> <li><code>ragdoll_search_results</code> - Junction table linking searches to returned embeddings</li> </ol> <pre><code># Search record with vector support for similarity analysis\nclass Ragdoll::Search &lt; ActiveRecord::Base\n  has_many :search_results, dependent: :destroy\n  has_many :embeddings, through: :search_results\n\n  # Vector similarity support for finding similar searches\n  has_neighbors :query_embedding\n\n  # Automatic tracking is enabled by default\n  def self.record_search(query:, query_embedding:, results:, **options)\n    search = create!(\n      query: query,\n      query_embedding: query_embedding,\n      search_type: options[:search_type] || 'semantic',\n      results_count: results.size,\n      search_filters: options[:filters],\n      search_options: options[:options],\n      execution_time_ms: options[:execution_time_ms],\n      session_id: options[:session_id],\n      user_id: options[:user_id]\n    )\n\n    # Record individual results with similarity scores\n    results.each_with_index do |result, index|\n      search.search_results.create!(\n        embedding_id: result[:embedding_id],\n        similarity_score: result[:similarity],\n        result_rank: index + 1\n      )\n    end\n\n    # Calculate and store similarity statistics\n    search.calculate_similarity_stats!\n    search\n  end\nend\n</code></pre>"},{"location":"user-guide/search-analytics/#automatic-search-tracking","title":"Automatic Search Tracking","text":"<p>All searches are automatically tracked unless explicitly disabled:</p> <pre><code># Search with automatic tracking (default)\nresults = client.search(\n  query: \"machine learning algorithms\",\n  session_id: \"user_session_123\",\n  user_id: \"user_456\"\n)\n\n# Disable tracking for specific searches\nresults = client.search(\n  query: \"private query\",\n  track_search: false  # Disables tracking for this search\n)\n\n# Hybrid search with tracking\nresults = client.hybrid_search(\n  query: \"neural networks\",\n  session_id: \"session_789\",\n  user_id: \"user_456\"\n)\n</code></pre>"},{"location":"user-guide/search-analytics/#search-result-tracking","title":"Search Result Tracking","text":"<p>Each search result is tracked with detailed metrics:</p> <pre><code>class Ragdoll::SearchResult &lt; ActiveRecord::Base\n  belongs_to :search\n  belongs_to :embedding\n\n  # Track user interactions\n  def mark_as_clicked!\n    update!(clicked: true, clicked_at: Time.current)\n  end\n\n  # Analytics methods\n  scope :clicked, -&gt; { where(clicked: true) }\n  scope :unclicked, -&gt; { where(clicked: false) }\n  scope :high_similarity, -&gt;(threshold) { where('similarity_score &gt;= ?', threshold) }\n  scope :by_rank, -&gt; { order(result_rank: :asc) }\nend\n</code></pre>"},{"location":"user-guide/search-analytics/#finding-similar-searches","title":"Finding Similar Searches","text":"<p>The system can identify similar searches using vector similarity:</p> <pre><code># Find searches similar to a given query\nsimilar_searches = Ragdoll::Search.find_similar(\n  query_embedding,\n  limit: 10,\n  threshold: 0.8\n)\n\n# Find searches similar to an existing search\nsearch = Ragdoll::Search.first\nsimilar = search.nearest_neighbors(:query_embedding, distance: :cosine).limit(5)\n\n# Analyze search patterns\nsimilar.each do |similar_search|\n  puts \"Query: #{similar_search.query}\"\n  puts \"Similarity: #{similar_search.neighbor_distance}\"\n  puts \"Results: #{similar_search.results_count}\"\n  puts \"CTR: #{similar_search.click_through_rate}%\"\nend\n</code></pre>"},{"location":"user-guide/search-analytics/#search-performance-metrics","title":"Search Performance Metrics","text":"<p>Track execution time and performance characteristics:</p> <pre><code># Search records include performance metrics\nsearch = Ragdoll::Search.last\nputs \"Query: #{search.query}\"\nputs \"Execution time: #{search.execution_time_ms}ms\"\nputs \"Results returned: #{search.results_count}\"\nputs \"Avg similarity: #{search.avg_similarity_score}\"\nputs \"Search type: #{search.search_type}\"\n\n# Analyze slow searches\nslow_searches = Ragdoll::Search.slow_searches(threshold_ms: 1000)\nslow_searches.each do |search|\n  puts \"Slow query: #{search.query} (#{search.execution_time_ms}ms)\"\nend\n</code></pre>"},{"location":"user-guide/search-analytics/#click-through-rate-ctr-analysis","title":"Click-Through Rate (CTR) Analysis","text":"<p>Monitor which searches lead to user engagement:</p> <pre><code># Calculate CTR for individual searches\nsearch = Ragdoll::Search.find(id)\nctr = search.click_through_rate\nputs \"CTR: #{ctr}%\"\n\n# Analyze CTR by result rank\nrank_analysis = Ragdoll::SearchResult.rank_click_analysis\nrank_analysis.each do |rank, stats|\n  puts \"Rank #{rank}: #{stats[:ctr]}% CTR (#{stats[:clicked]}/#{stats[:total]})\"\nend\n\n# Find top performing embeddings\ntop_embeddings = Ragdoll::SearchResult.top_performing_embeddings(limit: 10)\ntop_embeddings.each do |embedding_stats|\n  puts \"Embedding #{embedding_stats.embedding_id}:\"\n  puts \"  Appearances: #{embedding_stats.appearance_count}\"\n  puts \"  Clicks: #{embedding_stats.click_count}\"\n  puts \"  CTR: #{embedding_stats.ctr}%\"\n  puts \"  Avg Similarity: #{embedding_stats.avg_similarity}\"\nend\n</code></pre>"},{"location":"user-guide/search-analytics/#cleanup-and-maintenance","title":"Cleanup and Maintenance","text":"<p>The system includes automatic cleanup capabilities:</p> <pre><code># Remove orphaned searches (searches with no results)\norphaned_count = Ragdoll::Search.cleanup_orphaned_searches\nputs \"Cleaned up #{orphaned_count} orphaned searches\"\n\n# Remove old unused searches\ncleaned_count = Ragdoll::Search.cleanup_old_unused_searches(days: 30)\nputs \"Cleaned up #{cleaned_count} old unused searches\"\n\n# Automatic cascade deletion\n# When documents are deleted, associated search results are automatically cleaned up\n# Empty searches (with no results) are automatically removed\n</code></pre>"},{"location":"user-guide/search-analytics/#analytics-and-reporting","title":"Analytics and Reporting","text":""},{"location":"user-guide/search-analytics/#search-analytics-dashboard","title":"Search Analytics Dashboard","text":"<p>With the comprehensive search tracking system, you can generate detailed analytics:</p> <pre><code># Generate analytics from tracked searches\nanalytics = Ragdoll::Search.search_analytics(days: 30)\n\nputs \"Search Analytics (Last 30 Days):\"\nputs \"  Total searches: #{analytics[:total_searches]}\"\nputs \"  Unique queries: #{analytics[:unique_queries]}\"\nputs \"  Avg results per search: #{analytics[:avg_results_per_search]}\"\nputs \"  Avg execution time: #{analytics[:avg_execution_time]}ms\"\nputs \"  Search types: #{analytics[:search_types]}\"\nputs \"  Searches with results: #{analytics[:searches_with_results]}\"\n\n# Detailed search result analytics\nresult_analytics = Ragdoll::SearchResult.analytics(days: 30)\n\nputs \"Search Result Analytics:\"\nputs \"  Total results: #{result_analytics[:total_results]}\"\nputs \"  Clicked results: #{result_analytics[:clicked_results]}\"\nputs \"  CTR: #{result_analytics[:click_through_rate]}%\"\nputs \"  Avg similarity: #{result_analytics[:avg_similarity_score]}\"\nputs \"  High similarity results: #{result_analytics[:high_similarity_results]}\"\nputs \"  Low similarity results: #{result_analytics[:low_similarity_results]}\"\n\n# Get popular searches\npopular = Ragdoll::Search.popular.limit(10)\npopular.each do |search|\n  puts \"#{search.query} - #{search.results_count} results, #{search.click_through_rate}% CTR\"\nend\n</code></pre>"},{"location":"user-guide/search-analytics/#enhanced-analytics-methods","title":"Enhanced Analytics Methods","text":"<pre><code>class Ragdoll::Search\n  # Comprehensive search analytics\n  def self.search_analytics(days: 30)\n    searches = where('created_at &gt;= ?', days.days.ago)\n\n    {\n      total_searches: searches.count,\n      unique_queries: searches.distinct.count(:query),\n      avg_results_per_search: searches.average(:results_count) || 0.0,\n      avg_execution_time: searches.average(:execution_time_ms) || 0.0,\n      search_types: searches.group(:search_type).count,\n      searches_with_results: searches.with_results.count,\n      searches_without_results: searches.without_results.count,\n      avg_similarity_scores: {\n        max: searches.average(:max_similarity_score) || 0.0,\n        min: searches.average(:min_similarity_score) || 0.0,\n        avg: searches.average(:avg_similarity_score) || 0.0\n      },\n      session_count: searches.distinct.count(:session_id),\n      user_count: searches.distinct.count(:user_id)\n    }\n  end\n\n  def self.search_trends(date_range)\n    group_by_day(date_range)\n      .group(:query)\n      .count\n      .group_by { |k, v| k[0] }  # Group by date\n      .transform_values { |queries| queries.sum { |_, count| count } }\n  end\n\n  def self.content_type_distribution(date_range)\n    joins(embedding: :embeddable)\n      .where(created_at: date_range)\n      .group(\"embeddings.embeddable_type\")\n      .count\n  end\n\n  def self.user_engagement_metrics(date_range)\n    {\n      click_through_rate: calculate_ctr(date_range),\n      average_session_searches: calculate_avg_session_searches(date_range),\n      result_relevance_score: calculate_relevance_score(date_range),\n      abandonment_rate: calculate_abandonment_rate(date_range)\n    }\n  end\nend\n</code></pre>"},{"location":"user-guide/search-analytics/#query-performance-monitoring","title":"Query Performance Monitoring","text":"<pre><code>class SearchPerformanceMonitor\n  def self.monitor_search_performance\n    {\n      embedding_generation_time: benchmark_embedding_generation,\n      vector_search_time: benchmark_vector_search,\n      fulltext_search_time: benchmark_fulltext_search,\n      ranking_time: benchmark_ranking,\n      total_search_time: benchmark_total_search,\n      index_sizes: calculate_index_sizes,\n      queue_depths: check_background_job_queues\n    }\n  end\n\n  def self.benchmark_embedding_generation\n    sample_queries = [\"machine learning\", \"neural networks\", \"data science\"]\n\n    times = sample_queries.map do |query|\n      Benchmark.realtime { EmbeddingService.generate_embedding(query) }\n    end\n\n    {\n      average: times.sum / times.size,\n      min: times.min,\n      max: times.max,\n      samples: times.size\n    }\n  end\n\n  def self.benchmark_vector_search\n    sample_vector = EmbeddingService.generate_embedding(\"test query\")\n\n    time = Benchmark.realtime do\n      Embedding.select(\"embedding_vector &lt;=&gt; '#{sample_vector}' as distance\")\n               .order(\"embedding_vector &lt;=&gt; '#{sample_vector}'\")\n               .limit(20)\n               .to_a\n    end\n\n    { duration: time, query_type: \"vector_similarity\" }\n  end\nend\n</code></pre>"},{"location":"user-guide/search-analytics/#api-integration","title":"API Integration","text":""},{"location":"user-guide/search-analytics/#search-api-endpoints","title":"Search API Endpoints","text":"<pre><code># High-level search API\nclass SearchAPI\n  def search(params)\n    query = params[:query]\n    options = extract_search_options(params)\n\n    case params[:search_type]\n    when 'semantic'\n      SearchEngine.search_similar_content(query, options)\n    when 'fulltext'\n      FullTextSearchEngine.search(query, options)\n    when 'hybrid'\n      HybridSearchEngine.hybrid_search(query, options)\n    when 'faceted'\n      FacetedSearch.search_with_facets(query, facets: params[:facets])\n    else\n      # Default to hybrid search\n      HybridSearchEngine.hybrid_search(query, options)\n    end\n  end\n\n  def suggest(params)\n    SearchSuggestions.suggest_queries(\n      params[:partial_query],\n      limit: params[:limit] || 10\n    )\n  end\n\n  def analytics(params)\n    date_range = parse_date_range(params[:date_range])\n    SearchAnalytics.dashboard_metrics(date_range)\n  end\nend\n\n# Usage examples\nPOST /api/search\n{\n  \"query\": \"machine learning algorithms\",\n  \"search_type\": \"hybrid\",\n  \"semantic_weight\": 0.7,\n  \"text_weight\": 0.3,\n  \"content_types\": [\"text\", \"image\"],\n  \"similarity_threshold\": 0.75,\n  \"limit\": 20,\n  \"facets\": {\n    \"classification\": [\"research\", \"technical\"],\n    \"topics\": [\"AI\", \"neural networks\"]\n  }\n}\n\nGET /api/search/suggest?partial_query=machine&amp;limit=10\n{\n  \"suggestions\": [\n    \"machine learning\",\n    \"machine learning algorithms\", \n    \"machine learning models\",\n    \"machine vision\",\n    \"machine translation\"\n  ]\n}\n</code></pre>"},{"location":"user-guide/search-analytics/#configuration","title":"Configuration","text":""},{"location":"user-guide/search-analytics/#search-configuration","title":"Search Configuration","text":"<pre><code>Ragdoll::Core.configure do |config|\n  # Basic search settings\n  config.search_similarity_threshold = 0.7\n  config.max_search_results = 20\n  config.enable_usage_analytics = true\n\n  # Ranking weights\n  config.ranking_weights = {\n    similarity: 0.6,\n    usage: 0.3,\n    metadata: 0.1\n  }\n\n  # Hybrid search settings\n  config.enable_fulltext_search = true\n  config.fulltext_search_weight = 0.3\n  config.semantic_search_weight = 0.7\n\n  # Performance settings\n  config.search_cache_ttl = 300.seconds\n  config.embedding_cache_size = 1000\n  config.analytics_batch_size = 100\n\n  # Index configuration\n  config.vector_index_lists = 100  # IVFFlat lists parameter\n  config.enable_search_suggestions = true\n  config.suggestion_cache_ttl = 3600.seconds\nend\n</code></pre>"},{"location":"user-guide/search-analytics/#best-practices_1","title":"Best Practices","text":""},{"location":"user-guide/search-analytics/#1-search-optimization","title":"1. Search Optimization","text":"<ul> <li>Use appropriate similarity thresholds based on your content</li> <li>Implement result caching for common queries</li> <li>Monitor search performance and optimize indexes</li> <li>Balance semantic and full-text search weights for your use case</li> </ul>"},{"location":"user-guide/search-analytics/#2-analytics-implementation","title":"2. Analytics Implementation","text":"<ul> <li>Track user interactions with search results</li> <li>Monitor query performance and adjust configurations</li> <li>Analyze search patterns to improve content organization</li> <li>Use analytics to guide feature development</li> </ul>"},{"location":"user-guide/search-analytics/#3-ranking-strategy","title":"3. Ranking Strategy","text":"<ul> <li>Tune ranking weights based on user feedback</li> <li>Consider domain-specific ranking factors</li> <li>Implement A/B testing for ranking algorithms</li> <li>Monitor ranking effectiveness through engagement metrics</li> </ul>"},{"location":"user-guide/search-analytics/#4-performance-scaling","title":"4. Performance Scaling","text":"<ul> <li>Implement search result caching</li> <li>Use connection pooling for database access</li> <li>Consider search result pre-computation for common queries</li> <li>Monitor and scale background job processing</li> </ul> <p>The search and analytics system in Ragdoll provides enterprise-grade search capabilities that combine the power of semantic search with practical usage analytics and full-text search, enabling sophisticated document intelligence applications with excellent user experience.</p>"}]}